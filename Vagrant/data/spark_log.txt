Starting Spark at Tue Jun 25 10:04:20 PM UTC 2024
24/06/25 22:04:26 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:04:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-5b079899-988c-410f-9e19-0c5f7b1e5034;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 967ms :: artifacts dl 44ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-5b079899-988c-410f-9e19-0c5f7b1e5034
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/21ms)
24/06/25 22:04:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:04:30 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:04:30 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:04:30 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:04:30 INFO ResourceUtils: ==============================================================
24/06/25 22:04:30 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:04:30 INFO ResourceUtils: ==============================================================
24/06/25 22:04:30 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:04:30 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:04:30 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:04:30 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:04:30 INFO SecurityManager: Changing view acls to: root
24/06/25 22:04:30 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:04:30 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:04:30 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:04:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:04:31 INFO Utils: Successfully started service 'sparkDriver' on port 36581.
24/06/25 22:04:31 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:04:31 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:04:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:04:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:04:31 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:04:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60b5c0b4-842e-42b9-8395-ca97350eaf3b
24/06/25 22:04:31 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:04:31 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:04:32 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:04:32 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:36581/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:36581/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:36581/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:36581/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:36581/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:36581/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:36581/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:36581/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:36581/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:36581/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:36581/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:04:32 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353070475
24/06/25 22:04:32 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:04:32 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:04:32 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:04:32 INFO Executor: Java version 11.0.20.1
24/06/25 22:04:32 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:04:32 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4542939b for default.
24/06/25 22:04:32 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:04:33 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:04:33 INFO Executor: Fetching spark://10.0.2.15:36581/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:36581 after 41 ms (0 ms spent in bootstraps)
24/06/25 22:04:33 INFO Utils: Fetching spark://10.0.2.15:36581/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp12713887102969271664.tmp
24/06/25 22:04:33 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp12713887102969271664.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:04:33 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:04:33 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp12843145487146961437.tmp
24/06/25 22:04:33 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp12843145487146961437.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:04:33 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:04:33 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp15181263603285055517.tmp
24/06/25 22:04:33 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp15181263603285055517.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:04:33 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:04:33 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp8729019882951293145.tmp
24/06/25 22:04:33 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp8729019882951293145.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:04:33 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:04:33 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp17429406278889514957.tmp
24/06/25 22:04:33 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp17429406278889514957.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:04:33 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:04:33 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353070475
24/06/25 22:04:33 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp5056297831484332539.tmp
24/06/25 22:04:34 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp5056297831484332539.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:04:34 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:04:34 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353070475
24/06/25 22:04:34 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp2237532778113730829.tmp
24/06/25 22:04:34 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp2237532778113730829.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:04:34 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:04:34 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353070475
24/06/25 22:04:34 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp7286668783804350475.tmp
24/06/25 22:04:34 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp7286668783804350475.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:04:34 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:04:34 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353070475
24/06/25 22:04:34 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp3269364019402174621.tmp
24/06/25 22:04:34 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp3269364019402174621.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:04:34 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:04:34 INFO Executor: Fetching spark://10.0.2.15:36581/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353070475
24/06/25 22:04:34 INFO Utils: Fetching spark://10.0.2.15:36581/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp17969812292946352708.tmp
24/06/25 22:04:34 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp17969812292946352708.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:04:34 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:04:34 INFO Executor: Fetching spark://10.0.2.15:36581/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353070475
24/06/25 22:04:34 INFO Utils: Fetching spark://10.0.2.15:36581/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp17687919904537750492.tmp
24/06/25 22:04:34 INFO Utils: /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/fetchFileTemp17687919904537750492.tmp has been previously copied to /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:04:34 INFO Executor: Adding file:/tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/userFiles-db9a3193-2608-4a9b-9925-37653590a9e0/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:04:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42755.
24/06/25 22:04:34 INFO NettyBlockTransferService: Server created on 10.0.2.15:42755
24/06/25 22:04:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:04:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 42755, None)
24/06/25 22:04:34 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:42755 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 42755, None)
24/06/25 22:04:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 42755, None)
24/06/25 22:04:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 42755, None)
24/06/25 22:04:35 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:04:35 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/25 22:04:38 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:04:38 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:04:38 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437 resolved to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437.
24/06/25 22:04:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:04:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/metadata using temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/.metadata.463b40bd-e1fe-4788-aaca-ee0269bdd278.tmp
24/06/25 22:04:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/.metadata.463b40bd-e1fe-4788-aaca-ee0269bdd278.tmp to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/metadata
24/06/25 22:04:39 INFO MicroBatchExecution: Starting [id = 90c2b8ea-fdb1-4da8-b06a-f5882df36fbd, runId = a6a9356e-be20-499a-816a-805cfbad7578]. Use file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437 to store the query checkpoint.
24/06/25 22:04:39 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1ab77694] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@53d4d2c3]
24/06/25 22:04:39 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:04:39 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:04:39 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:04:39 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:04:39 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:04:39 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:04:39 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:04:39 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:04:39 INFO AppInfoParser: Kafka startTimeMs: 1719353079819
24/06/25 22:04:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/sources/0/0 using temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/sources/0/.0.7b77062d-2a9e-4f6d-9e56-90b3c012ba50.tmp
24/06/25 22:04:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/sources/0/.0.7b77062d-2a9e-4f6d-9e56-90b3c012ba50.tmp to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/sources/0/0
24/06/25 22:04:40 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":171}}
24/06/25 22:04:40 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/offsets/0 using temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/offsets/.0.b5ad167a-53a8-4b34-9133-5bfe1d291a97.tmp
24/06/25 22:04:40 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/offsets/.0.b5ad167a-53a8-4b34-9133-5bfe1d291a97.tmp to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/offsets/0
24/06/25 22:04:40 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719353080439,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:04:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:04:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:04:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:04:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:04:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:04:41 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:04:41 INFO CodeGenerator: Code generated in 342.011523 ms
24/06/25 22:04:41 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:04:41 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:04:41 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:04:41 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:04:41 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:04:41 INFO DAGScheduler: Missing parents: List()
24/06/25 22:04:41 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[3] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:04:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/25 22:04:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/25 22:04:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:42755 (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:04:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:04:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[3] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:04:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:04:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:04:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:04:42 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:04:42 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:04:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/25 22:04:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 190 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:04:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:04:42 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.476 s
24/06/25 22:04:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:04:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:04:42 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.543593 s
24/06/25 22:04:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
+-----+
|value|
+-----+
+-----+

24/06/25 22:04:42 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:04:42 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/commits/0 using temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/commits/.0.86ae7d64-c51e-4827-b528-7b8cad992ebe.tmp
24/06/25 22:04:42 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/commits/.0.86ae7d64-c51e-4827-b528-7b8cad992ebe.tmp to file:/tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437/commits/0
24/06/25 22:04:42 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "90c2b8ea-fdb1-4da8-b06a-f5882df36fbd",
  "runId" : "a6a9356e-be20-499a-816a-805cfbad7578",
  "name" : null,
  "timestamp" : "2024-06-25T22:04:39.177Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1369,
    "commitOffsets" : 66,
    "getBatch" : 46,
    "latestOffset" : 1227,
    "queryPlanning" : 651,
    "triggerExecution" : 3497,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 171
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 171
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@c11499e",
    "numOutputRows" : 0
  }
}
24/06/25 22:04:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:42755 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:04:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:05:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:05:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:05:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:05:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:05:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:05:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:06:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:06:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:06:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:06:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:06:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:06:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:07:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:07:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:07:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:07:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:07:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:07:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:08:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:08:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:08:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:08:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:08:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o17.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
24/06/25 22:08:50 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:08:50 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:08:50 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:08:50 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:08:50 INFO MemoryStore: MemoryStore cleared
24/06/25 22:08:50 INFO BlockManager: BlockManager stopped
Traceback (most recent call last):
  File "/vagrant_data/spark_kafka_consumer.py", line 26, in <module>
24/06/25 22:08:50 INFO BlockManagerMaster: BlockManagerMaster stopped
    query.awaitTermination()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o40.awaitTermination
24/06/25 22:08:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:08:51 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:08:51 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:08:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-27f74f3d-ecb3-4d60-94af-582c9306e6f6
24/06/25 22:08:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314
24/06/25 22:08:51 INFO ShutdownHookManager: Deleting directory /tmp/temporary-d8bca1a3-5859-4722-a354-e81c969a1437
24/06/25 22:08:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-f4d95138-cb59-42cb-9f0b-b6b8cb91f314/pyspark-5ef21ede-17f9-466b-bb48-ef31b586e8fc
Starting Spark at Tue Jun 25 10:09:09 PM UTC 2024
24/06/25 22:09:11 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:09:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-8e0cffaa-388e-407b-b717-f294b8bda8ce;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 562ms :: artifacts dl 12ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-8e0cffaa-388e-407b-b717-f294b8bda8ce
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/14ms)
24/06/25 22:09:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:09:13 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:09:13 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:09:13 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:09:13 INFO ResourceUtils: ==============================================================
24/06/25 22:09:13 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:09:13 INFO ResourceUtils: ==============================================================
24/06/25 22:09:13 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:09:13 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:09:14 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:09:14 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:09:14 INFO SecurityManager: Changing view acls to: root
24/06/25 22:09:14 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:09:14 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:09:14 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:09:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:09:14 INFO Utils: Successfully started service 'sparkDriver' on port 36097.
24/06/25 22:09:14 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:09:14 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:09:14 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:09:14 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:09:14 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:09:14 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0f5d7b5a-2c6d-4af0-b4d4-97c9814e9049
24/06/25 22:09:14 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:09:14 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:09:14 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:09:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:36097/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:36097/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:36097/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:36097/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:36097/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:36097/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:36097/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:36097/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:36097/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:36097/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:36097/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:09:14 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:09:14 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:09:14 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353353920
24/06/25 22:09:14 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:09:15 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:09:15 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:09:15 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:09:15 INFO Executor: Java version 11.0.20.1
24/06/25 22:09:15 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:09:15 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7f07f990 for default.
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:09:15 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:09:15 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:36097 after 38 ms (0 ms spent in bootstraps)
24/06/25 22:09:15 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp10261138081844032750.tmp
24/06/25 22:09:15 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp10261138081844032750.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:09:15 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:09:15 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp9897447443645848312.tmp
24/06/25 22:09:15 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp9897447443645848312.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:09:15 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:09:15 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp8874580527586615602.tmp
24/06/25 22:09:15 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp8874580527586615602.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:09:15 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:09:15 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp4273033555815083350.tmp
24/06/25 22:09:15 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp4273033555815083350.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:09:15 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:09:15 INFO Executor: Fetching spark://10.0.2.15:36097/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353353920
24/06/25 22:09:15 INFO Utils: Fetching spark://10.0.2.15:36097/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp16044541521864628571.tmp
24/06/25 22:09:15 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp16044541521864628571.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:09:15 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:09:15 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353353920
24/06/25 22:09:16 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp16036996927080409000.tmp
24/06/25 22:09:16 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp16036996927080409000.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:09:16 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:09:16 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353353920
24/06/25 22:09:16 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp2233036225781361510.tmp
24/06/25 22:09:16 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp2233036225781361510.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:09:16 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:09:16 INFO Executor: Fetching spark://10.0.2.15:36097/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353353920
24/06/25 22:09:16 INFO Utils: Fetching spark://10.0.2.15:36097/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp16289512104319003728.tmp
24/06/25 22:09:16 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp16289512104319003728.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:09:16 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:09:16 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353353920
24/06/25 22:09:16 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp2705543174784413392.tmp
24/06/25 22:09:16 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp2705543174784413392.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:09:16 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:09:16 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353353920
24/06/25 22:09:16 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp2895917787139392996.tmp
24/06/25 22:09:16 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp2895917787139392996.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:09:16 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:09:16 INFO Executor: Fetching spark://10.0.2.15:36097/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353353920
24/06/25 22:09:16 INFO Utils: Fetching spark://10.0.2.15:36097/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp17087417129277542464.tmp
24/06/25 22:09:16 INFO Utils: /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/fetchFileTemp17087417129277542464.tmp has been previously copied to /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:09:16 INFO Executor: Adding file:/tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/userFiles-af5a7949-5d9b-4e5e-96a8-ec2ab0ff2018/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:09:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39109.
24/06/25 22:09:16 INFO NettyBlockTransferService: Server created on 10.0.2.15:39109
24/06/25 22:09:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:09:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39109, None)
24/06/25 22:09:16 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:39109 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 39109, None)
24/06/25 22:09:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39109, None)
24/06/25 22:09:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39109, None)
24/06/25 22:09:17 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:09:17 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/25 22:09:19 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:09:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:09:19 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f resolved to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f.
24/06/25 22:09:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:09:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/metadata using temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/.metadata.e125f914-7dd7-4c07-8fca-22357989a05f.tmp
24/06/25 22:09:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/.metadata.e125f914-7dd7-4c07-8fca-22357989a05f.tmp to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/metadata
24/06/25 22:09:19 INFO MicroBatchExecution: Starting [id = 465ade72-96ca-442e-87b4-ea1d353c8b1f, runId = b929f636-1b6a-412c-bfcb-b46be259f5be]. Use file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f to store the query checkpoint.
24/06/25 22:09:19 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5482c9e6] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@58b3f798]
24/06/25 22:09:19 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:09:19 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:09:19 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:09:19 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:09:20 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:09:20 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:09:20 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:09:20 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:09:20 INFO AppInfoParser: Kafka startTimeMs: 1719353360407
24/06/25 22:09:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/sources/0/0 using temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/sources/0/.0.42c60d9c-6da8-4397-b24b-bfef6e52a4ef.tmp
24/06/25 22:09:20 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/sources/0/.0.42c60d9c-6da8-4397-b24b-bfef6e52a4ef.tmp to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/sources/0/0
24/06/25 22:09:20 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":171}}
24/06/25 22:09:20 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/0 using temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/.0.af6106a7-be78-49f4-8924-7d265033de21.tmp
24/06/25 22:09:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/.0.af6106a7-be78-49f4-8924-7d265033de21.tmp to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/0
24/06/25 22:09:21 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719353360975,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:09:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:09:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:09:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:09:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:09:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:09:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:09:22 INFO CodeGenerator: Code generated in 288.340739 ms
24/06/25 22:09:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:09:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:09:22 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:09:22 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:09:22 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:09:22 INFO DAGScheduler: Missing parents: List()
24/06/25 22:09:22 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[3] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:09:22 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/25 22:09:22 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/25 22:09:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:39109 (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:09:22 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:09:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[3] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:09:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:09:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:09:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:09:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:09:22 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:09:22 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/25 22:09:22 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 219 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:09:22 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:09:22 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.435 s
24/06/25 22:09:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:09:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:09:22 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.500008 s
24/06/25 22:09:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
+-----+
|value|
+-----+
+-----+

24/06/25 22:09:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:09:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/0 using temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/.0.f4f9730e-31d4-4f61-b487-cfed286f6567.tmp
24/06/25 22:09:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/.0.f4f9730e-31d4-4f61-b487-cfed286f6567.tmp to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/0
24/06/25 22:09:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "465ade72-96ca-442e-87b4-ea1d353c8b1f",
  "runId" : "b929f636-1b6a-412c-bfcb-b46be259f5be",
  "name" : null,
  "timestamp" : "2024-06-25T22:09:19.787Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1284,
    "commitOffsets" : 68,
    "getBatch" : 36,
    "latestOffset" : 1157,
    "queryPlanning" : 548,
    "triggerExecution" : 3230,
    "walCommit" : 97
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 171
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 171
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@247ac234",
    "numOutputRows" : 0
  }
}
24/06/25 22:09:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:09:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:09:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:39109 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:09:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:10:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:10:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:10:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:10:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:10:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:10:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:11:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:11:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:11:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/1 using temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/.1.56025edc-d3b8-495a-87f0-42bb425635ba.tmp
24/06/25 22:11:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/.1.56025edc-d3b8-495a-87f0-42bb425635ba.tmp to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/offsets/1
24/06/25 22:11:14 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719353474823,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:11:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:11:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:11:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:11:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:11:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:11:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:11:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:11:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:11:15 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:11:15 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:11:15 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:11:15 INFO DAGScheduler: Missing parents: List()
24/06/25 22:11:15 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:11:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.2 KiB, free 434.4 MiB)
24/06/25 22:11:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/25 22:11:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:39109 (size: 4.5 KiB, free: 434.4 MiB)
24/06/25 22:11:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/25 22:11:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:11:15 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/25 22:11:15 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:11:15 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/25 22:11:15 INFO CodeGenerator: Code generated in 26.819448 ms
24/06/25 22:11:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=171 untilOffset=174, for query queryId=465ade72-96ca-442e-87b4-ea1d353c8b1f batchId=1 taskId=1 partitionId=0
24/06/25 22:11:15 INFO CodeGenerator: Code generated in 30.429272 ms
24/06/25 22:11:15 INFO CodeGenerator: Code generated in 34.901333 ms
24/06/25 22:11:15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:11:15 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:11:15 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:11:15 INFO AppInfoParser: Kafka startTimeMs: 1719353475425
24/06/25 22:11:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Assigned to partition(s): logs-0
24/06/25 22:11:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Seeking to offset 171 for partition logs-0
24/06/25 22:11:15 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:11:15 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:11:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:11:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:11:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Seeking to latest offset of partition logs-0
24/06/25 22:11:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=174, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:11:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:11:16 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/25 22:11:16 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor read 3 records through 1 polls (polled  out 3 records), taking 596696623 nanos, during time span of 697978099 nanos.
24/06/25 22:11:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2449 bytes result sent to driver
24/06/25 22:11:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1074 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:11:16 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 1.103 s
24/06/25 22:11:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:11:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/25 22:11:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/25 22:11:16 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 1.127991 s
24/06/25 22:11:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/25 22:11:16 INFO CodeGenerator: Code generated in 13.512316 ms
24/06/25 22:11:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:39109 in memory (size: 4.5 KiB, free: 434.4 MiB)
24/06/25 22:11:17 INFO CodeGenerator: Code generated in 13.823653 ms
+--------------------+
|               value|
+--------------------+
|{"timestamp": "20...|
|{"timestamp": "20...|
|{"timestamp": "20...|
+--------------------+

24/06/25 22:11:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:11:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/1 using temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/.1.f2a81621-bb8d-43db-85f5-1f159f2fca49.tmp
24/06/25 22:11:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/.1.f2a81621-bb8d-43db-85f5-1f159f2fca49.tmp to file:/tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f/commits/1
24/06/25 22:11:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "465ade72-96ca-442e-87b4-ea1d353c8b1f",
  "runId" : "b929f636-1b6a-412c-bfcb-b46be259f5be",
  "name" : null,
  "timestamp" : "2024-06-25T22:11:14.812Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 130.43478260869566,
  "processedRowsPerSecond" : 1.1307953260459858,
  "durationMs" : {
    "addBatch" : 2466,
    "commitOffsets" : 54,
    "getBatch" : 0,
    "latestOffset" : 11,
    "queryPlanning" : 35,
    "triggerExecution" : 2652,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 171
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 174
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 174
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 130.43478260869566,
    "processedRowsPerSecond" : 1.1307953260459858,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@247ac234",
    "numOutputRows" : 3
  }
}
24/06/25 22:11:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:11:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:11:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:11:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:12:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:12:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:12:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:12:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:12:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:12:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:13:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:13:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:13:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:13:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o17.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
24/06/25 22:13:38 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:13:38 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1, groupId=spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:13:38 INFO Metrics: Metrics scheduler closed
24/06/25 22:13:38 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:13:38 INFO Metrics: Metrics reporters closed
24/06/25 22:13:38 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-4f568ebd-4f75-4f09-a920-470543adc0c7-2007400652-executor-1 unregistered
24/06/25 22:13:38 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:13:38 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:13:38 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:13:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:13:38 INFO MemoryStore: MemoryStore cleared
24/06/25 22:13:38 INFO BlockManager: BlockManager stopped
24/06/25 22:13:38 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/25 22:13:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:13:38 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:13:38 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:13:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7
24/06/25 22:13:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-63730401-99dd-410e-b4ac-9734be4fc82d
24/06/25 22:13:38 INFO ShutdownHookManager: Deleting directory /tmp/spark-9086bf0a-84bf-4c54-8fa3-43055ac728d7/pyspark-42018535-3af8-44b7-b061-9630c527b736
24/06/25 22:13:38 INFO ShutdownHookManager: Deleting directory /tmp/temporary-40936725-59a8-454f-8b5c-029d8836cf6f
Starting Spark at Tue Jun 25 10:13:51 PM UTC 2024
24/06/25 22:13:53 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:13:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-cbb7ce6d-8228-4953-9372-6f0ea26368c4;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 664ms :: artifacts dl 17ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-cbb7ce6d-8228-4953-9372-6f0ea26368c4
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/14ms)
24/06/25 22:13:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:13:56 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:13:56 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:13:56 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:13:56 INFO ResourceUtils: ==============================================================
24/06/25 22:13:56 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:13:56 INFO ResourceUtils: ==============================================================
24/06/25 22:13:56 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:13:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:13:56 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:13:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:13:56 INFO SecurityManager: Changing view acls to: root
24/06/25 22:13:56 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:13:56 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:13:56 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:13:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:13:56 INFO Utils: Successfully started service 'sparkDriver' on port 45627.
24/06/25 22:13:56 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:13:56 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:13:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:13:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:13:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:13:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-97ceb01d-0aed-4da5-a73f-552f64bfce57
24/06/25 22:13:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:13:56 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:13:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:13:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:45627/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:45627/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:45627/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:45627/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:45627/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:45627/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:45627/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:45627/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:45627/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:45627/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:45627/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:13:57 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:13:57 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:13:57 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:13:57 INFO Executor: Java version 11.0.20.1
24/06/25 22:13:57 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:13:57 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@59af5155 for default.
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:13:57 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:13:57 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:45627 after 36 ms (0 ms spent in bootstraps)
24/06/25 22:13:57 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp14807602441292779782.tmp
24/06/25 22:13:57 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp14807602441292779782.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:13:57 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:13:57 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp4269304295788926509.tmp
24/06/25 22:13:57 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp4269304295788926509.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:13:57 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:13:57 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp3044868856643234398.tmp
24/06/25 22:13:57 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp3044868856643234398.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:13:57 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:13:57 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:57 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp11356867917226664767.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp11356867917226664767.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp3877700570206393034.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp3877700570206393034.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp4495396426013523978.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp4495396426013523978.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp5131842683881224140.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp5131842683881224140.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp18224667914532502419.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp18224667914532502419.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp17131607550915953009.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp17131607550915953009.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp17749087973224168017.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp17749087973224168017.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:13:58 INFO Executor: Fetching spark://10.0.2.15:45627/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719353636248
24/06/25 22:13:58 INFO Utils: Fetching spark://10.0.2.15:45627/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp358459747737965057.tmp
24/06/25 22:13:58 INFO Utils: /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/fetchFileTemp358459747737965057.tmp has been previously copied to /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:13:58 INFO Executor: Adding file:/tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/userFiles-76e8b304-bfab-4e9a-b370-d62784fa57ae/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:13:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39135.
24/06/25 22:13:58 INFO NettyBlockTransferService: Server created on 10.0.2.15:39135
24/06/25 22:13:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:13:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39135, None)
24/06/25 22:13:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:39135 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 39135, None)
24/06/25 22:13:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39135, None)
24/06/25 22:13:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39135, None)
24/06/25 22:13:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:13:59 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/25 22:14:02 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:14:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:14:02 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd resolved to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd.
24/06/25 22:14:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:14:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/metadata using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/.metadata.87b2a919-155e-44bb-b47e-0ea436ab10c9.tmp
24/06/25 22:14:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/.metadata.87b2a919-155e-44bb-b47e-0ea436ab10c9.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/metadata
24/06/25 22:14:02 INFO MicroBatchExecution: Starting logs_table [id = d7da88fb-96fc-48bd-918e-08dfc5909f68, runId = 39129de1-e9fa-47db-9834-450eae7c0cbf]. Use file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd to store the query checkpoint.
24/06/25 22:14:02 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@d55cd07] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@cfd1036]
24/06/25 22:14:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:14:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:14:02 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:14:02 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:14:03 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:14:03 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:14:03 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:14:03 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:14:03 INFO AppInfoParser: Kafka startTimeMs: 1719353643506
24/06/25 22:14:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/sources/0/0 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/sources/0/.0.50a91082-4d16-4c10-94f6-002c3a9dac49.tmp
24/06/25 22:14:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/sources/0/.0.50a91082-4d16-4c10-94f6-002c3a9dac49.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/sources/0/0
24/06/25 22:14:04 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":174}}
24/06/25 22:14:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/0 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/.0.b0430019-1d4e-45a9-8cc3-b8ee15aa40c0.tmp
24/06/25 22:14:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/.0.b0430019-1d4e-45a9-8cc3-b8ee15aa40c0.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/0
24/06/25 22:14:04 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719353644093,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:14:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:05 INFO CodeGenerator: Code generated in 352.081622 ms
24/06/25 22:14:05 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@267880f2]. The input RDD has 1 partitions.
24/06/25 22:14:05 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:14:05 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:14:05 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:14:05 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:14:05 INFO DAGScheduler: Missing parents: List()
24/06/25 22:14:05 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:14:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.2 KiB, free 434.4 MiB)
24/06/25 22:14:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/25 22:14:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:39135 (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:14:05 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:14:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:14:05 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:14:05 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:14:05 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:14:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:14:05 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:14:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/25 22:14:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 216 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:14:05 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.655 s
24/06/25 22:14:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:14:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:14:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:14:05 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.712046 s
24/06/25 22:14:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@267880f2] is committing.
24/06/25 22:14:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@267880f2] committed.
24/06/25 22:14:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/0 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/.0.0fa60d52-c625-4043-8110-19e7e0e00628.tmp
24/06/25 22:14:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/.0.0fa60d52-c625-4043-8110-19e7e0e00628.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/0
24/06/25 22:14:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "d7da88fb-96fc-48bd-918e-08dfc5909f68",
  "runId" : "39129de1-e9fa-47db-9834-450eae7c0cbf",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:14:02.617Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1482,
    "commitOffsets" : 68,
    "getBatch" : 18,
    "latestOffset" : 1382,
    "queryPlanning" : 245,
    "triggerExecution" : 3369,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 174
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 174
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/25 22:14:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/1 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/.1.b252c935-318d-411f-bc51-e72d2444b31e.tmp
24/06/25 22:14:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/.1.b252c935-318d-411f-bc51-e72d2444b31e.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/1
24/06/25 22:14:06 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719353646052,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:06 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@10346fa5]. The input RDD has 1 partitions.
24/06/25 22:14:06 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:14:06 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:14:06 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:14:06 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:14:06 INFO DAGScheduler: Missing parents: List()
24/06/25 22:14:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:14:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/25 22:14:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/25 22:14:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:39135 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:14:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/25 22:14:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:14:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/25 22:14:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:14:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/25 22:14:06 INFO CodeGenerator: Code generated in 67.018452 ms
24/06/25 22:14:06 INFO CodeGenerator: Code generated in 23.335205 ms
24/06/25 22:14:06 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=174 untilOffset=177, for query queryId=d7da88fb-96fc-48bd-918e-08dfc5909f68 batchId=1 taskId=1 partitionId=0
24/06/25 22:14:06 INFO CodeGenerator: Code generated in 29.620928 ms
24/06/25 22:14:06 INFO CodeGenerator: Code generated in 22.816548 ms
24/06/25 22:14:06 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:14:06 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:14:06 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:14:06 INFO AppInfoParser: Kafka startTimeMs: 1719353646789
24/06/25 22:14:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Assigned to partition(s): logs-0
24/06/25 22:14:06 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Seeking to offset 174 for partition logs-0
24/06/25 22:14:06 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:14:06 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:14:06 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:14:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:14:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Seeking to latest offset of partition logs-0
24/06/25 22:14:07 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=177, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:14:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:39135 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:14:08 INFO CodeGenerator: Code generated in 24.568846 ms
24/06/25 22:14:08 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:14:08 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/25 22:14:08 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor read 3 records through 1 polls (polled  out 3 records), taking 567044669 nanos, during time span of 1839647283 nanos.
24/06/25 22:14:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3460 bytes result sent to driver
24/06/25 22:14:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2312 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:14:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/25 22:14:08 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 2.380 s
24/06/25 22:14:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:14:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/25 22:14:08 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 2.405236 s
24/06/25 22:14:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@10346fa5] is committing.
24/06/25 22:14:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@10346fa5] committed.
24/06/25 22:14:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/1 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/.1.ec7f089a-e6d3-4459-8eed-5ac9b5591a3a.tmp
24/06/25 22:14:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/.1.ec7f089a-e6d3-4459-8eed-5ac9b5591a3a.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/1
24/06/25 22:14:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "d7da88fb-96fc-48bd-918e-08dfc5909f68",
  "runId" : "39129de1-e9fa-47db-9834-450eae7c0cbf",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:14:06.043Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 0.8756567425569176,
  "processedRowsPerSecond" : 1.124859392575928,
  "durationMs" : {
    "addBatch" : 2490,
    "commitOffsets" : 51,
    "getBatch" : 1,
    "latestOffset" : 9,
    "queryPlanning" : 41,
    "triggerExecution" : 2667,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 174
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 177
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 177
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 0.8756567425569176,
    "processedRowsPerSecond" : 1.124859392575928,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:14:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:14:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:14:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/2 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/.2.5ea72453-ddd1-48dd-b8ca-fc6a29362257.tmp
24/06/25 22:14:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/.2.5ea72453-ddd1-48dd-b8ca-fc6a29362257.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/offsets/2
24/06/25 22:14:33 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719353673333,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:14:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:14:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@21de3a29]. The input RDD has 1 partitions.
24/06/25 22:14:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:14:33 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:14:33 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:14:33 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:14:33 INFO DAGScheduler: Missing parents: List()
24/06/25 22:14:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:14:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/25 22:14:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/25 22:14:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:39135 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:14:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/25 22:14:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:14:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/25 22:14:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:14:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/25 22:14:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=177 untilOffset=180, for query queryId=d7da88fb-96fc-48bd-918e-08dfc5909f68 batchId=2 taskId=2 partitionId=0
24/06/25 22:14:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Seeking to offset 177 for partition logs-0
24/06/25 22:14:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:14:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:14:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Seeking to latest offset of partition logs-0
24/06/25 22:14:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=180, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:14:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:14:34 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/25 22:14:34 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor read 3 records through 1 polls (polled  out 3 records), taking 513476881 nanos, during time span of 522052542 nanos.
24/06/25 22:14:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3460 bytes result sent to driver
24/06/25 22:14:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 569 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:14:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/25 22:14:34 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.585 s
24/06/25 22:14:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:14:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/25 22:14:34 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.596178 s
24/06/25 22:14:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@21de3a29] is committing.
24/06/25 22:14:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@21de3a29] committed.
24/06/25 22:14:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/2 using temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/.2.fe21004f-a785-4388-851a-6a204ff7c10d.tmp
24/06/25 22:14:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:39135 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:14:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/.2.fe21004f-a785-4388-851a-6a204ff7c10d.tmp to file:/tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd/commits/2
24/06/25 22:14:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "d7da88fb-96fc-48bd-918e-08dfc5909f68",
  "runId" : "39129de1-e9fa-47db-9834-450eae7c0cbf",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:14:33.326Z",
  "batchId" : 2,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 176.47058823529412,
  "processedRowsPerSecond" : 3.3632286995515694,
  "durationMs" : {
    "addBatch" : 677,
    "commitOffsets" : 85,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 38,
    "triggerExecution" : 892,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 177
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 180
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 180
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 176.47058823529412,
    "processedRowsPerSecond" : 3.3632286995515694,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:14:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:14:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:39135 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:14:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:15:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:15:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:15:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:15:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:15:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:15:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:16:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:16:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:16:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:16:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:16:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:16:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:17:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:17:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:17:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:17:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:17:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:17:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:18:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:18:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:18:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:18:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:18:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:18:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:19:03 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/25 22:19:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:19:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:19:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:19:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:19:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:19:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:20:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:20:06 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:20:06 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1, groupId=spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:20:06 INFO Metrics: Metrics scheduler closed
24/06/25 22:20:06 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:20:06 INFO Metrics: Metrics reporters closed
24/06/25 22:20:06 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-5c725e74-0fee-435c-94c0-dac2de7a7f39-1806729516-executor-1 unregistered
24/06/25 22:20:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:20:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:20:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:20:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:20:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:21:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:21:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:21:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:21:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:21:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:21:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:22:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:22:05 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:22:05 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:22:05 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:22:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:22:05 INFO MemoryStore: MemoryStore cleared
24/06/25 22:22:05 INFO BlockManager: BlockManager stopped
24/06/25 22:22:05 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/25 22:22:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:22:05 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:22:05 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:22:05 INFO ShutdownHookManager: Deleting directory /tmp/temporary-8654c862-3025-400c-b91a-ab52182ad9fd
24/06/25 22:22:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-a3ad8dee-c842-42b4-b544-3df812d1b9da
24/06/25 22:22:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc/pyspark-15444837-7d8d-49c8-a633-52fc698cc17b
24/06/25 22:22:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-af724250-3276-4a15-bee1-eda6631089cc
Starting Spark at Tue Jun 25 10:25:53 PM UTC 2024
24/06/25 22:25:58 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:25:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-a6f0a5ab-06fe-46e9-a964-9ae2ac169df2;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 689ms :: artifacts dl 29ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-a6f0a5ab-06fe-46e9-a964-9ae2ac169df2
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/44ms)
24/06/25 22:26:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:26:01 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:26:01 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:26:01 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:26:02 INFO ResourceUtils: ==============================================================
24/06/25 22:26:02 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:26:02 INFO ResourceUtils: ==============================================================
24/06/25 22:26:02 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:26:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:26:02 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:26:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:26:02 INFO SecurityManager: Changing view acls to: root
24/06/25 22:26:02 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:26:02 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:26:02 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:26:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:26:02 INFO Utils: Successfully started service 'sparkDriver' on port 40185.
24/06/25 22:26:02 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:26:02 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:26:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:26:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:26:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:26:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d47d89df-fda6-4f20-8b8e-c9af58209e43
24/06/25 22:26:02 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:26:02 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:26:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:26:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:40185/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:40185/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:40185/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:40185/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:40185/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:40185/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:40185/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:40185/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:40185/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:40185/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:40185/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:26:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:26:04 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:26:04 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:26:04 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:26:04 INFO Executor: Java version 11.0.20.1
24/06/25 22:26:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:26:04 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2203e8a4 for default.
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:26:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:26:04 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:40185 after 45 ms (0 ms spent in bootstraps)
24/06/25 22:26:04 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp5283903377019100127.tmp
24/06/25 22:26:04 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp5283903377019100127.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:26:04 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:26:04 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354361943
24/06/25 22:26:04 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp16103135574707335693.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp16103135574707335693.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp853587249692246084.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp853587249692246084.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp380560202244718609.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp380560202244718609.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp12896706910080923701.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp12896706910080923701.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp17137060351598493083.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp17137060351598493083.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp7770561730114428416.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp7770561730114428416.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp1913743499282344976.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp1913743499282344976.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp8020693312030548683.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp8020693312030548683.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp6975047840237198094.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp6975047840237198094.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:26:05 INFO Executor: Fetching spark://10.0.2.15:40185/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354361943
24/06/25 22:26:05 INFO Utils: Fetching spark://10.0.2.15:40185/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp6773590039604968981.tmp
24/06/25 22:26:05 INFO Utils: /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/fetchFileTemp6773590039604968981.tmp has been previously copied to /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:26:05 INFO Executor: Adding file:/tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/userFiles-83d8f21c-54b1-4cb9-9e30-e5d2bb1a9e9e/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:26:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46321.
24/06/25 22:26:05 INFO NettyBlockTransferService: Server created on 10.0.2.15:46321
24/06/25 22:26:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:26:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 46321, None)
24/06/25 22:26:05 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:46321 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 46321, None)
24/06/25 22:26:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 46321, None)
24/06/25 22:26:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 46321, None)
24/06/25 22:26:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:26:06 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/25 22:26:09 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:26:10 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:26:10 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc resolved to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc.
24/06/25 22:26:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:26:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/metadata using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/.metadata.3a53f80d-a2a2-408b-aada-0d081cf53520.tmp
24/06/25 22:26:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/.metadata.3a53f80d-a2a2-408b-aada-0d081cf53520.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/metadata
24/06/25 22:26:10 INFO MicroBatchExecution: Starting logs_table [id = ca5b9181-4f40-467e-a98a-b20581ed2f0e, runId = 00fc49b3-8372-426f-9fac-39a1b485e644]. Use file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc to store the query checkpoint.
24/06/25 22:26:10 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@28f5a326] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@380f58d5]
24/06/25 22:26:10 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:26:10 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:26:10 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:26:10 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:26:11 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:26:11 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:26:11 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:26:11 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:26:11 INFO AppInfoParser: Kafka startTimeMs: 1719354371886
24/06/25 22:26:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/sources/0/0 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/sources/0/.0.113d5a7e-b728-459d-912e-815cc68b558d.tmp
24/06/25 22:26:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/sources/0/.0.113d5a7e-b728-459d-912e-815cc68b558d.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/sources/0/0
24/06/25 22:26:12 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":180}}
24/06/25 22:26:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/0 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/.0.663bc8a0-baf6-4071-b5b1-26e2ed5d3d3d.tmp
24/06/25 22:26:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/.0.663bc8a0-baf6-4071-b5b1-26e2ed5d3d3d.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/0
24/06/25 22:26:12 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719354372644,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:26:12 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:26:13 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:26:13 INFO CodeGenerator: Code generated in 564.863978 ms
24/06/25 22:26:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3cd927b6]. The input RDD has 1 partitions.
24/06/25 22:26:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:26:14 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:26:14 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:26:14 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:26:14 INFO DAGScheduler: Missing parents: List()
24/06/25 22:26:14 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:26:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.2 KiB, free 434.4 MiB)
24/06/25 22:26:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/25 22:26:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:46321 (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:26:14 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:26:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:26:14 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:26:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:26:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:26:15 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:26:15 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:26:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1396 bytes result sent to driver
24/06/25 22:26:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 335 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:26:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:26:15 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.814 s
24/06/25 22:26:15 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:26:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:26:15 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.934468 s
24/06/25 22:26:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3cd927b6] is committing.
24/06/25 22:26:15 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3cd927b6] committed.
24/06/25 22:26:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/0 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/.0.75d1c2db-ea11-4f23-9469-70c99f06bb2c.tmp
24/06/25 22:26:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/.0.75d1c2db-ea11-4f23-9469-70c99f06bb2c.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/0
24/06/25 22:26:15 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ca5b9181-4f40-467e-a98a-b20581ed2f0e",
  "runId" : "00fc49b3-8372-426f-9fac-39a1b485e644",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:26:10.743Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 2155,
    "commitOffsets" : 95,
    "getBatch" : 34,
    "latestOffset" : 1841,
    "queryPlanning" : 288,
    "triggerExecution" : 4561,
    "walCommit" : 90
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 180
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 180
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/25 22:26:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:46321 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:28:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:28:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:28:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:28:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/1 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/.1.6a8249d8-61c1-4200-ad07-6878fa800e14.tmp
24/06/25 22:28:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/.1.6a8249d8-61c1-4200-ad07-6878fa800e14.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/1
24/06/25 22:28:26 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719354506791,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:28:26 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:28:26 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1ac2e151]. The input RDD has 1 partitions.
24/06/25 22:28:26 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:28:26 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:28:26 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:28:26 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:28:26 INFO DAGScheduler: Missing parents: List()
24/06/25 22:28:26 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:28:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/25 22:28:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/25 22:28:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:46321 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:28:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/25 22:28:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:28:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/25 22:28:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/25 22:28:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/25 22:28:27 INFO CodeGenerator: Code generated in 33.440939 ms
24/06/25 22:28:27 INFO CodeGenerator: Code generated in 18.518948 ms
24/06/25 22:28:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=180 untilOffset=183, for query queryId=ca5b9181-4f40-467e-a98a-b20581ed2f0e batchId=1 taskId=1 partitionId=0
24/06/25 22:28:27 INFO CodeGenerator: Code generated in 13.467134 ms
24/06/25 22:28:27 INFO CodeGenerator: Code generated in 23.935549 ms
24/06/25 22:28:27 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:28:27 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:28:27 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:28:27 INFO AppInfoParser: Kafka startTimeMs: 1719354507409
24/06/25 22:28:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Assigned to partition(s): logs-0
24/06/25 22:28:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Seeking to offset 180 for partition logs-0
24/06/25 22:28:27 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:28:27 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:28:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:28:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:28:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Seeking to latest offset of partition logs-0
24/06/25 22:28:28 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=183, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:28:29 INFO CodeGenerator: Code generated in 27.879235 ms
24/06/25 22:28:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:28:29 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/25 22:28:29 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor read 3 records through 1 polls (polled  out 3 records), taking 583243051 nanos, during time span of 1927854727 nanos.
24/06/25 22:28:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3460 bytes result sent to driver
24/06/25 22:28:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2342 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:28:29 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 2.412 s
24/06/25 22:28:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:28:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/25 22:28:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/25 22:28:29 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 2.426218 s
24/06/25 22:28:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1ac2e151] is committing.
24/06/25 22:28:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1ac2e151] committed.
24/06/25 22:28:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/1 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/.1.71c98697-09b6-4e49-9e46-4e07b62aa89e.tmp
24/06/25 22:28:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/.1.71c98697-09b6-4e49-9e46-4e07b62aa89e.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/1
24/06/25 22:28:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ca5b9181-4f40-467e-a98a-b20581ed2f0e",
  "runId" : "00fc49b3-8372-426f-9fac-39a1b485e644",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:28:26.784Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 187.5,
  "processedRowsPerSecond" : 1.1299435028248588,
  "durationMs" : {
    "addBatch" : 2490,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 26,
    "triggerExecution" : 2655,
    "walCommit" : 65
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 180
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 183
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 183
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 187.5,
    "processedRowsPerSecond" : 1.1299435028248588,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:28:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:46321 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:28:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:28:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:28:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:29:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:29:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:29:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/2 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/.2.5e73203e-ef02-48f7-8799-a1754911bb41.tmp
24/06/25 22:29:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/.2.5e73203e-ef02-48f7-8799-a1754911bb41.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/offsets/2
24/06/25 22:29:27 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719354567167,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:29:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:29:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:29:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:29:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:29:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:29:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:29:27 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@591c3ba8]. The input RDD has 1 partitions.
24/06/25 22:29:27 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:29:27 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:29:27 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:29:27 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:29:27 INFO DAGScheduler: Missing parents: List()
24/06/25 22:29:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:29:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/25 22:29:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/25 22:29:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:46321 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:29:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/25 22:29:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:29:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/25 22:29:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/25 22:29:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/25 22:29:27 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=183 untilOffset=186, for query queryId=ca5b9181-4f40-467e-a98a-b20581ed2f0e batchId=2 taskId=2 partitionId=0
24/06/25 22:29:27 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Seeking to offset 183 for partition logs-0
24/06/25 22:29:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:29:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:29:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Seeking to latest offset of partition logs-0
24/06/25 22:29:27 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=186, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:29:27 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:29:27 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/25 22:29:27 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor read 3 records through 1 polls (polled  out 3 records), taking 510547173 nanos, during time span of 517081519 nanos.
24/06/25 22:29:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3461 bytes result sent to driver
24/06/25 22:29:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 578 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:29:27 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.592 s
24/06/25 22:29:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:29:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/25 22:29:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/25 22:29:27 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.603996 s
24/06/25 22:29:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@591c3ba8] is committing.
24/06/25 22:29:27 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@591c3ba8] committed.
24/06/25 22:29:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/2 using temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/.2.29e009d2-0ddb-4761-b5da-598babd052b8.tmp
24/06/25 22:29:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/.2.29e009d2-0ddb-4761-b5da-598babd052b8.tmp to file:/tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc/commits/2
24/06/25 22:29:28 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ca5b9181-4f40-467e-a98a-b20581ed2f0e",
  "runId" : "00fc49b3-8372-426f-9fac-39a1b485e644",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:29:27.163Z",
  "batchId" : 2,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 187.5,
  "processedRowsPerSecond" : 3.53356890459364,
  "durationMs" : {
    "addBatch" : 659,
    "commitOffsets" : 87,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 25,
    "triggerExecution" : 849,
    "walCommit" : 72
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 183
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 186
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 186
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 187.5,
    "processedRowsPerSecond" : 3.53356890459364,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:29:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:29:47 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:46321 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:29:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:29:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:30:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:30:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:30:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:30:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:30:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:30:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:31:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:31:12 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/25 22:31:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:31:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:31:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:31:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:31:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:32:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:32:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:32:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:32:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:32:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:32:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:33:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:33:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:33:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:33:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:33:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:33:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o17.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
24/06/25 22:34:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:34:07 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1, groupId=spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:34:07 INFO Metrics: Metrics scheduler closed
24/06/25 22:34:07 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:34:07 INFO Metrics: Metrics reporters closed
24/06/25 22:34:07 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-9114da70-612c-4aa9-9e33-22700aea2fe4-259545366-executor-1 unregistered
24/06/25 22:34:07 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:34:07 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:34:07 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:34:07 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:34:07 INFO MemoryStore: MemoryStore cleared
24/06/25 22:34:07 INFO BlockManager: BlockManager stopped
24/06/25 22:34:07 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/25 22:34:07 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:34:07 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:34:07 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:34:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-6d22b723-b840-4976-9020-f3c9eb73abcb
24/06/25 22:34:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7/pyspark-2c8503e7-71b5-4f36-a9c6-45826efc809c
24/06/25 22:34:07 INFO ShutdownHookManager: Deleting directory /tmp/temporary-1493c0e1-7459-4527-8cee-ff15f61d5fdc
24/06/25 22:34:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-59297fec-a48d-469c-ac9a-f724158da8f7
Starting Spark at Tue Jun 25 10:34:23 PM UTC 2024
24/06/25 22:34:25 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:34:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-e49bc117-0f65-4322-88b7-52f9d776bf7a;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 506ms :: artifacts dl 75ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-e49bc117-0f65-4322-88b7-52f9d776bf7a
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/10ms)
24/06/25 22:34:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:34:27 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:34:27 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:34:27 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:34:27 INFO ResourceUtils: ==============================================================
24/06/25 22:34:27 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:34:27 INFO ResourceUtils: ==============================================================
24/06/25 22:34:27 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:34:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:34:27 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:34:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:34:27 INFO SecurityManager: Changing view acls to: root
24/06/25 22:34:27 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:34:27 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:34:27 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:34:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:34:27 INFO Utils: Successfully started service 'sparkDriver' on port 44133.
24/06/25 22:34:27 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:34:27 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:34:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:34:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:34:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:34:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60a597c8-8d83-4d82-acbe-29ba05f98ca7
24/06/25 22:34:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:34:28 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:34:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:34:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44133/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44133/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:44133/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:44133/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:44133/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:44133/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:44133/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:44133/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:44133/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:44133/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:44133/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:34:28 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:34:28 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:34:28 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:34:28 INFO Executor: Java version 11.0.20.1
24/06/25 22:34:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:34:28 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@21f25072 for default.
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:34:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:34:28 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:28 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:44133 after 53 ms (0 ms spent in bootstraps)
24/06/25 22:34:28 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp12090849571582484910.tmp
24/06/25 22:34:28 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp12090849571582484910.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp15311018045176537489.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp15311018045176537489.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp5692929998934359276.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp5692929998934359276.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp6920875480508314788.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp6920875480508314788.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp3704086059631182828.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp3704086059631182828.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp10126753979315245941.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp10126753979315245941.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp17007674909298275601.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp17007674909298275601.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp748987939560541126.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp748987939560541126.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp1870391649146220160.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp1870391649146220160.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp142714580033337755.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp142714580033337755.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:34:29 INFO Executor: Fetching spark://10.0.2.15:44133/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719354867397
24/06/25 22:34:29 INFO Utils: Fetching spark://10.0.2.15:44133/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp17251021861748390977.tmp
24/06/25 22:34:29 INFO Utils: /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/fetchFileTemp17251021861748390977.tmp has been previously copied to /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:34:29 INFO Executor: Adding file:/tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/userFiles-e310dbd6-cb54-49ec-ad27-47ed6a427b67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:34:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37163.
24/06/25 22:34:29 INFO NettyBlockTransferService: Server created on 10.0.2.15:37163
24/06/25 22:34:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:34:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 37163, None)
24/06/25 22:34:29 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:37163 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 37163, None)
24/06/25 22:34:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 37163, None)
24/06/25 22:34:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 37163, None)
INFO:__main__:==========================================================================================Spark session created
24/06/25 22:34:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:34:30 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
INFO:__main__:==========================================================================================Kafka stream initialized
INFO:__main__:==========================================================================================Messages parsed
24/06/25 22:34:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:34:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:34:32 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6 resolved to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6.
24/06/25 22:34:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:34:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/metadata using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/.metadata.0fb731ec-a096-4e51-8ea8-c15066fa11ff.tmp
24/06/25 22:34:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/.metadata.0fb731ec-a096-4e51-8ea8-c15066fa11ff.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/metadata
24/06/25 22:34:32 INFO MicroBatchExecution: Starting logs_table [id = dd05c723-0697-4da8-8452-cdc232d89bff, runId = 4580de9c-745f-47d7-82f4-28755d35cf25]. Use file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6 to store the query checkpoint.
24/06/25 22:34:32 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@530a9740] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@42891c8a]
24/06/25 22:34:32 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:34:32 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:34:32 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:34:32 INFO MicroBatchExecution: Stream started from {}
INFO:__main__:==========================================================================================Streaming query started
24/06/25 22:34:33 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:34:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:34:33 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:34:33 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:34:33 INFO AppInfoParser: Kafka startTimeMs: 1719354873378
24/06/25 22:34:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/sources/0/0 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/sources/0/.0.b3a65772-dd10-47ae-861b-77ad4db42d51.tmp
24/06/25 22:34:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/sources/0/.0.b3a65772-dd10-47ae-861b-77ad4db42d51.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/sources/0/0
24/06/25 22:34:33 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":186}}
24/06/25 22:34:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/0 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/.0.ee643a94-7572-497a-837e-17555d859a80.tmp
24/06/25 22:34:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/.0.ee643a94-7572-497a-837e-17555d859a80.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/0
24/06/25 22:34:33 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719354873891,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:34:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:34:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:34:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:34:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:34:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:34:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:34:34 INFO CodeGenerator: Code generated in 258.077663 ms
24/06/25 22:34:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a2ea8c2]. The input RDD has 1 partitions.
24/06/25 22:34:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:34:34 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:34:34 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:34:34 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:34:34 INFO DAGScheduler: Missing parents: List()
24/06/25 22:34:34 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:34:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/25 22:34:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/25 22:34:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:37163 (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:34:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:34:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[4] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:34:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:34:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:34:34 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:34:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:34:35 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:34:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/25 22:34:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 194 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:34:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:34:35 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.378 s
24/06/25 22:34:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:34:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:34:35 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.442649 s
24/06/25 22:34:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a2ea8c2] is committing.
24/06/25 22:34:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a2ea8c2] committed.
24/06/25 22:34:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/0 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/.0.a54d26fc-a7e8-404e-a23e-b3fd03712dc7.tmp
24/06/25 22:34:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/.0.a54d26fc-a7e8-404e-a23e-b3fd03712dc7.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/0
24/06/25 22:34:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dd05c723-0697-4da8-8452-cdc232d89bff",
  "runId" : "4580de9c-745f-47d7-82f4-28755d35cf25",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:34:32.710Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1016,
    "commitOffsets" : 59,
    "getBatch" : 19,
    "latestOffset" : 1151,
    "queryPlanning" : 159,
    "triggerExecution" : 2514,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 186
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 186
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/25 22:34:43 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:37163 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:34:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:34:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:35:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:35:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:35:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/1 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/.1.10f3e7f3-6bb4-490c-adc3-29d0748a8dbd.tmp
24/06/25 22:35:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/.1.10f3e7f3-6bb4-490c-adc3-29d0748a8dbd.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/1
24/06/25 22:35:21 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719354921295,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:35:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:35:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:35:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:35:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:35:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:35:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:35:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@a9bdb61]. The input RDD has 1 partitions.
24/06/25 22:35:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:35:21 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:35:21 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:35:21 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:35:21 INFO DAGScheduler: Missing parents: List()
24/06/25 22:35:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:35:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 22.6 KiB, free 434.4 MiB)
24/06/25 22:35:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/25 22:35:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:37163 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:35:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/25 22:35:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:35:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/25 22:35:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/25 22:35:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/25 22:35:21 INFO CodeGenerator: Code generated in 29.846714 ms
24/06/25 22:35:21 INFO CodeGenerator: Code generated in 15.730603 ms
24/06/25 22:35:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=186 untilOffset=189, for query queryId=dd05c723-0697-4da8-8452-cdc232d89bff batchId=1 taskId=1 partitionId=0
24/06/25 22:35:21 INFO CodeGenerator: Code generated in 15.506175 ms
24/06/25 22:35:21 INFO CodeGenerator: Code generated in 22.941251 ms
24/06/25 22:35:21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:35:21 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:35:21 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:35:21 INFO AppInfoParser: Kafka startTimeMs: 1719354921864
24/06/25 22:35:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Assigned to partition(s): logs-0
24/06/25 22:35:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Seeking to offset 186 for partition logs-0
24/06/25 22:35:21 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:35:21 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:35:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:35:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:35:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Seeking to latest offset of partition logs-0
24/06/25 22:35:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=189, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:35:23 INFO CodeGenerator: Code generated in 13.132927 ms
24/06/25 22:35:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:35:23 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/25 22:35:23 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor read 3 records through 1 polls (polled  out 3 records), taking 570707973 nanos, during time span of 1684010409 nanos.
24/06/25 22:35:23 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3345 bytes result sent to driver
24/06/25 22:35:23 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2022 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:35:23 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/25 22:35:23 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 2.095 s
24/06/25 22:35:23 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:35:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/25 22:35:23 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 2.109584 s
24/06/25 22:35:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@a9bdb61] is committing.
24/06/25 22:35:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@a9bdb61] committed.
24/06/25 22:35:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/1 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/.1.6a3bc410-6b09-4b29-a2f9-cac253d027d4.tmp
24/06/25 22:35:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/.1.6a3bc410-6b09-4b29-a2f9-cac253d027d4.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/1
24/06/25 22:35:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dd05c723-0697-4da8-8452-cdc232d89bff",
  "runId" : "4580de9c-745f-47d7-82f4-28755d35cf25",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:35:21.290Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 136.36363636363637,
  "processedRowsPerSecond" : 1.2836970474967906,
  "durationMs" : {
    "addBatch" : 2183,
    "commitOffsets" : 52,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 33,
    "triggerExecution" : 2337,
    "walCommit" : 61
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 186
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 189
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 189
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 136.36363636363637,
    "processedRowsPerSecond" : 1.2836970474967906,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:35:28 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:37163 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:35:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:35:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:35:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:36:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:36:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:36:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:36:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:36:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:36:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:37:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:37:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/2 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/.2.17420419-b8cc-4747-82c6-6d1756a1359c.tmp
24/06/25 22:37:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/.2.17420419-b8cc-4747-82c6-6d1756a1359c.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/offsets/2
24/06/25 22:37:04 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719355024679,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:37:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:37:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:37:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:37:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:37:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:37:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:37:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@327438b4]. The input RDD has 1 partitions.
24/06/25 22:37:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:37:04 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:37:04 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:37:04 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:37:04 INFO DAGScheduler: Missing parents: List()
24/06/25 22:37:04 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:37:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.6 KiB, free 434.4 MiB)
24/06/25 22:37:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/25 22:37:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:37163 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:37:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/25 22:37:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:37:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/25 22:37:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/25 22:37:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/25 22:37:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=189 untilOffset=192, for query queryId=dd05c723-0697-4da8-8452-cdc232d89bff batchId=2 taskId=2 partitionId=0
24/06/25 22:37:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Seeking to offset 189 for partition logs-0
24/06/25 22:37:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:37:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:37:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Seeking to latest offset of partition logs-0
24/06/25 22:37:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=192, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:37:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:37:05 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/25 22:37:05 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor read 3 records through 1 polls (polled  out 3 records), taking 521358311 nanos, during time span of 535230745 nanos.
24/06/25 22:37:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3259 bytes result sent to driver
24/06/25 22:37:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 588 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:37:05 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 0.601 s
24/06/25 22:37:05 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:37:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/25 22:37:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/25 22:37:05 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 0.619359 s
24/06/25 22:37:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@327438b4] is committing.
24/06/25 22:37:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@327438b4] committed.
24/06/25 22:37:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/2 using temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/.2.68805ca4-e6d4-409f-b67f-c0cc9c4648b0.tmp
24/06/25 22:37:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/.2.68805ca4-e6d4-409f-b67f-c0cc9c4648b0.tmp to file:/tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6/commits/2
24/06/25 22:37:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dd05c723-0697-4da8-8452-cdc232d89bff",
  "runId" : "4580de9c-745f-47d7-82f4-28755d35cf25",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:37:04.673Z",
  "batchId" : 2,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 200.0,
  "processedRowsPerSecond" : 3.5211267605633805,
  "durationMs" : {
    "addBatch" : 693,
    "commitOffsets" : 60,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 25,
    "triggerExecution" : 852,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 189
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 200.0,
    "processedRowsPerSecond" : 3.5211267605633805,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:37:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:37163 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:37:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:37:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:37:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:37:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:37:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:38:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:38:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:38:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:38:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
INFO:py4j.clientserver:Error while receiving.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>
INFO:py4j.clientserver:Closing down clientserver connection
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
INFO:py4j.clientserver:Error while receiving.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o17.sc
INFO:py4j.clientserver:Closing down clientserver connection
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o17.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
INFO:py4j.clientserver:Closing down clientserver connection
24/06/25 22:38:41 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:38:41 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1, groupId=spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:38:41 INFO Metrics: Metrics scheduler closed
24/06/25 22:38:41 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:38:41 INFO Metrics: Metrics reporters closed
24/06/25 22:38:41 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-00c08257-d83b-4516-b5e6-2142f6f6270b--1193828520-executor-1 unregistered
24/06/25 22:38:41 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:38:41 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:38:41 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:38:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:38:41 INFO MemoryStore: MemoryStore cleared
24/06/25 22:38:41 INFO BlockManager: BlockManager stopped
24/06/25 22:38:41 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/25 22:38:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:38:41 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:38:41 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a225fc47-13ca-45f6-895b-983259f7edec
24/06/25 22:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b
24/06/25 22:38:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-921f4b7a-45ac-4cd8-a6b5-f23102b2ec5b/pyspark-3ed25db5-1e04-468f-8168-7709d53985a8
24/06/25 22:38:41 INFO ShutdownHookManager: Deleting directory /tmp/temporary-d2db5bdc-e5d3-44c3-8181-cdb68ec817a6
Starting Spark at Tue Jun 25 10:38:52 PM UTC 2024
24/06/25 22:38:54 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:38:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-6982d945-d91b-4884-9504-b5b3e4717bfe;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 579ms :: artifacts dl 12ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-6982d945-d91b-4884-9504-b5b3e4717bfe
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/12ms)
24/06/25 22:38:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:38:56 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:38:56 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:38:56 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:38:56 INFO ResourceUtils: ==============================================================
24/06/25 22:38:56 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:38:56 INFO ResourceUtils: ==============================================================
24/06/25 22:38:56 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:38:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:38:56 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:38:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:38:56 INFO SecurityManager: Changing view acls to: root
24/06/25 22:38:56 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:38:56 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:38:56 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:38:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:38:57 INFO Utils: Successfully started service 'sparkDriver' on port 44615.
24/06/25 22:38:57 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:38:57 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:38:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:38:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:38:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:38:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-15a15ead-d4fe-46df-873a-1ee0be8cb9c4
24/06/25 22:38:57 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:38:57 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:38:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:38:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44615/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44615/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:44615/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:44615/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:44615/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:44615/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:44615/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:44615/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:44615/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:44615/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:44615/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:38:57 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719355136687
24/06/25 22:38:57 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:38:58 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:38:58 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:38:58 INFO Executor: Java version 11.0.20.1
24/06/25 22:38:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:38:58 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7fe39fdf for default.
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:38:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:44615 after 33 ms (0 ms spent in bootstraps)
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp16341076394555281484.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp16341076394555281484.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp4300769432086554497.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp4300769432086554497.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp6845781580138556783.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp6845781580138556783.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp52811497344625128.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp52811497344625128.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp17607523621548235056.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp17607523621548235056.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp15182848294706605275.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp15182848294706605275.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp676685781433556729.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp676685781433556729.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp10527950978195918377.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp10527950978195918377.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp4006144957818272804.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp4006144957818272804.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:38:58 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:38:58 INFO Executor: Fetching spark://10.0.2.15:44615/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719355136687
24/06/25 22:38:58 INFO Utils: Fetching spark://10.0.2.15:44615/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp17042060692612402926.tmp
24/06/25 22:38:58 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp17042060692612402926.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:38:59 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:38:59 INFO Executor: Fetching spark://10.0.2.15:44615/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719355136687
24/06/25 22:38:59 INFO Utils: Fetching spark://10.0.2.15:44615/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp14551983381535667428.tmp
24/06/25 22:38:59 INFO Utils: /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/fetchFileTemp14551983381535667428.tmp has been previously copied to /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:38:59 INFO Executor: Adding file:/tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/userFiles-0be9ac1d-82cf-4ed8-8453-8c2f6a34d631/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:38:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34205.
24/06/25 22:38:59 INFO NettyBlockTransferService: Server created on 10.0.2.15:34205
24/06/25 22:38:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:38:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34205, None)
24/06/25 22:38:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34205 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 34205, None)
24/06/25 22:38:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34205, None)
24/06/25 22:38:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34205, None)
24/06/25 22:38:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:38:59 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/25 22:39:01 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:39:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:39:01 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9 resolved to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9.
24/06/25 22:39:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:39:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/metadata using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/.metadata.12dfc949-4263-4f90-8dfd-aca2c50aa642.tmp
24/06/25 22:39:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/.metadata.12dfc949-4263-4f90-8dfd-aca2c50aa642.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/metadata
24/06/25 22:39:01 INFO MicroBatchExecution: Starting logs_table [id = 4178d045-aa3e-483a-8890-f55a82bafb0f, runId = 3961123f-367c-4f38-8bda-2c0c55422a36]. Use file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9 to store the query checkpoint.
24/06/25 22:39:02 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@661a5112] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@249f0545]
24/06/25 22:39:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:39:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:39:02 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:39:02 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:39:02 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:39:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:39:02 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00 resolved to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00.
24/06/25 22:39:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:39:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/metadata using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/.metadata.e207239e-9a0e-4a26-94f4-93804d6a2211.tmp
24/06/25 22:39:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/.metadata.e207239e-9a0e-4a26-94f4-93804d6a2211.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/metadata
24/06/25 22:39:02 INFO MicroBatchExecution: Starting [id = 6ee08699-6bc2-4886-94fe-3d7e0cfdbde9, runId = 621ac50d-f085-4520-872b-92e52835d306]. Use file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00 to store the query checkpoint.
24/06/25 22:39:02 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@661a5112] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@249f0545]
24/06/25 22:39:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:39:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:39:02 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:39:02 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:39:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:39:02 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:39:02 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:39:02 INFO AppInfoParser: Kafka startTimeMs: 1719355142638
24/06/25 22:39:02 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:39:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:39:02 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:39:02 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:39:02 INFO AppInfoParser: Kafka startTimeMs: 1719355142666
24/06/25 22:39:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/sources/0/0 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/sources/0/.0.f2998ec4-f426-4876-98bb-bebdaa855d98.tmp
24/06/25 22:39:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/sources/0/0 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/sources/0/.0.0910c3ae-486f-4ff2-98fd-b6a6a8af9993.tmp
24/06/25 22:39:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/sources/0/.0.0910c3ae-486f-4ff2-98fd-b6a6a8af9993.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/sources/0/0
24/06/25 22:39:03 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":192}}
24/06/25 22:39:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/sources/0/.0.f2998ec4-f426-4876-98bb-bebdaa855d98.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/sources/0/0
24/06/25 22:39:03 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":192}}
24/06/25 22:39:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/0 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.0.8803afa5-cf6a-4234-8ab7-481226d06ebb.tmp
24/06/25 22:39:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/0 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.0.ca5a2134-4d99-4716-ab81-0ccab9851073.tmp
24/06/25 22:39:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.0.ca5a2134-4d99-4716-ab81-0ccab9851073.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/0
24/06/25 22:39:03 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719355143223,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:39:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.0.8803afa5-cf6a-4234-8ab7-481226d06ebb.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/0
24/06/25 22:39:03 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719355143223,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:04 INFO CodeGenerator: Code generated in 267.559684 ms
24/06/25 22:39:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@50d8b55b]. The input RDD has 1 partitions.
24/06/25 22:39:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:39:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:39:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:39:04 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:39:04 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:39:04 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:39:04 INFO DAGScheduler: Missing parents: List()
24/06/25 22:39:04 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:39:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/25 22:39:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/25 22:39:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:34205 (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:39:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:39:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:39:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:39:04 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:39:04 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:39:04 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:39:04 INFO DAGScheduler: Missing parents: List()
24/06/25 22:39:04 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:39:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/25 22:39:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/25 22:39:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:34205 (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:39:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/25 22:39:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:39:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/25 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:39:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:39:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:39:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/25 22:39:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:39:04 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:39:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:39:04 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/25 22:39:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/25 22:39:04 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/25 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 128 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/25 22:39:04 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:39:04 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:39:04 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.213 s
24/06/25 22:39:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:39:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/25 22:39:04 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.489858 s
24/06/25 22:39:04 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.457 s
24/06/25 22:39:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:39:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@50d8b55b] is committing.
24/06/25 22:39:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:39:04 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.498744 s
24/06/25 22:39:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@50d8b55b] committed.
24/06/25 22:39:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/25 22:39:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/0 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.0.ab232c74-7cb3-4268-9d1b-11ab09556489.tmp
24/06/25 22:39:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.0.ab232c74-7cb3-4268-9d1b-11ab09556489.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/0
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/25 22:39:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:39:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/0 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.0.b615dc60-4acc-4c83-a252-40a55640f8ce.tmp
24/06/25 22:39:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4178d045-aa3e-483a-8890-f55a82bafb0f",
  "runId" : "3961123f-367c-4f38-8bda-2c0c55422a36",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:39:02.059Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1195,
    "commitOffsets" : 99,
    "getBatch" : 24,
    "latestOffset" : 1144,
    "queryPlanning" : 173,
    "triggerExecution" : 2747,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/25 22:39:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.0.b615dc60-4acc-4c83-a252-40a55640f8ce.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/0
24/06/25 22:39:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "6ee08699-6bc2-4886-94fe-3d7e0cfdbde9",
  "runId" : "621ac50d-f085-4520-872b-92e52835d306",
  "name" : null,
  "timestamp" : "2024-06-25T22:39:02.632Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1340,
    "commitOffsets" : 54,
    "getBatch" : 19,
    "latestOffset" : 584,
    "queryPlanning" : 173,
    "triggerExecution" : 2275,
    "walCommit" : 90
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@10145c1",
    "numOutputRows" : 0
  }
}
24/06/25 22:39:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:34205 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:39:10 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:34205 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:39:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/1 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.1.ae8451e7-9d80-49e9-94ca-961152dd43c4.tmp
24/06/25 22:39:22 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/1 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.1.071fa7b0-9c19-4e89-950a-cc5f1b6168bf.tmp
24/06/25 22:39:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.1.ae8451e7-9d80-49e9-94ca-961152dd43c4.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/1
24/06/25 22:39:22 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719355162719,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:39:22 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.1.071fa7b0-9c19-4e89-950a-cc5f1b6168bf.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/1
24/06/25 22:39:22 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719355162723,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:39:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:39:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:39:22 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:39:22 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:39:22 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:39:22 INFO DAGScheduler: Missing parents: List()
24/06/25 22:39:22 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:39:22 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6610b450]. The input RDD has 1 partitions.
24/06/25 22:39:22 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:39:23 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.4 KiB, free 434.4 MiB)
24/06/25 22:39:23 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.4 MiB)
24/06/25 22:39:23 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:34205 (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:39:23 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/25 22:39:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:39:23 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/25 22:39:23 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:39:23 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:39:23 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:39:23 INFO DAGScheduler: Missing parents: List()
24/06/25 22:39:23 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:39:23 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:39:23 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/25 22:39:23 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/25 22:39:23 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/25 22:39:23 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:34205 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:39:23 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/25 22:39:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:39:23 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/25 22:39:23 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/25 22:39:23 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/25 22:39:23 INFO CodeGenerator: Code generated in 32.751356 ms
24/06/25 22:39:23 INFO CodeGenerator: Code generated in 14.77426 ms
24/06/25 22:39:23 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=192 untilOffset=195, for query queryId=6ee08699-6bc2-4886-94fe-3d7e0cfdbde9 batchId=1 taskId=2 partitionId=0
24/06/25 22:39:23 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=192 untilOffset=195, for query queryId=4178d045-aa3e-483a-8890-f55a82bafb0f batchId=1 taskId=3 partitionId=0
24/06/25 22:39:23 INFO CodeGenerator: Code generated in 14.833406 ms
24/06/25 22:39:23 INFO CodeGenerator: Code generated in 28.038586 ms
24/06/25 22:39:23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:39:23 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:39:23 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:39:23 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:39:23 INFO AppInfoParser: Kafka startTimeMs: 1719355163475
24/06/25 22:39:23 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:39:23 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:39:23 INFO AppInfoParser: Kafka startTimeMs: 1719355163475
24/06/25 22:39:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Assigned to partition(s): logs-0
24/06/25 22:39:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Assigned to partition(s): logs-0
24/06/25 22:39:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to offset 192 for partition logs-0
24/06/25 22:39:23 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to offset 192 for partition logs-0
24/06/25 22:39:23 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:39:23 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:39:23 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:39:23 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:39:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:39:23 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:39:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:39:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to latest offset of partition logs-0
24/06/25 22:39:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:39:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to latest offset of partition logs-0
24/06/25 22:39:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=195, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:39:24 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=195, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:39:24 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:39:24 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/25 22:39:24 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor read 3 records through 1 polls (polled  out 3 records), taking 586315327 nanos, during time span of 766486673 nanos.
24/06/25 22:39:24 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2420 bytes result sent to driver
24/06/25 22:39:24 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1236 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:39:24 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/25 22:39:24 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.310 s
24/06/25 22:39:24 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:39:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/25 22:39:24 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.332383 s
24/06/25 22:39:24 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/25 22:39:24 INFO CodeGenerator: Code generated in 6.447221 ms
24/06/25 22:39:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:34205 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:39:25 INFO CodeGenerator: Code generated in 11.536634 ms
24/06/25 22:39:25 INFO CodeGenerator: Code generated in 16.572906 ms
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/25 22:39:25 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:39:25 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/25 22:39:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:39:25 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor read 3 records through 1 polls (polled  out 3 records), taking 588056214 nanos, during time span of 1796434259 nanos.
24/06/25 22:39:25 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3345 bytes result sent to driver
24/06/25 22:39:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/1 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.1.a3b60f93-7ddc-4c21-837d-9205f77398cc.tmp
24/06/25 22:39:25 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2259 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:39:25 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/25 22:39:25 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.277 s
24/06/25 22:39:25 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:39:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/25 22:39:25 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.355053 s
24/06/25 22:39:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6610b450] is committing.
24/06/25 22:39:25 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6610b450] committed.
24/06/25 22:39:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/1 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.1.c68b35d2-2213-41ff-8836-77e1e80b2295.tmp
24/06/25 22:39:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.1.a3b60f93-7ddc-4c21-837d-9205f77398cc.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/1
24/06/25 22:39:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "6ee08699-6bc2-4886-94fe-3d7e0cfdbde9",
  "runId" : "621ac50d-f085-4520-872b-92e52835d306",
  "name" : null,
  "timestamp" : "2024-06-25T22:39:22.708Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 111.11111111111111,
  "processedRowsPerSecond" : 1.1278195488721805,
  "durationMs" : {
    "addBatch" : 2432,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 33,
    "triggerExecution" : 2660,
    "walCommit" : 113
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 195
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 195
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 111.11111111111111,
    "processedRowsPerSecond" : 1.1278195488721805,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@10145c1",
    "numOutputRows" : 3
  }
}
24/06/25 22:39:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.1.c68b35d2-2213-41ff-8836-77e1e80b2295.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/1
24/06/25 22:39:25 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4178d045-aa3e-483a-8890-f55a82bafb0f",
  "runId" : "3961123f-367c-4f38-8bda-2c0c55422a36",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:39:22.708Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 115.38461538461539,
  "processedRowsPerSecond" : 1.1139992573338284,
  "durationMs" : {
    "addBatch" : 2459,
    "commitOffsets" : 74,
    "getBatch" : 0,
    "latestOffset" : 15,
    "queryPlanning" : 33,
    "triggerExecution" : 2693,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 192
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 195
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 195
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 115.38461538461539,
    "processedRowsPerSecond" : 1.1139992573338284,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:39:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:39:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/2 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.2.9ce8fce2-c9b1-40a3-b0f8-59fb65a54de0.tmp
24/06/25 22:40:15 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/2 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.2.67756f08-5356-488b-b4b4-ed65dd754fec.tmp
24/06/25 22:40:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.2.9ce8fce2-c9b1-40a3-b0f8-59fb65a54de0.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/2
24/06/25 22:40:15 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719355215255,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:40:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.2.67756f08-5356-488b-b4b4-ed65dd754fec.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/2
24/06/25 22:40:15 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719355215256,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:40:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:40:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:40:15 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:40:15 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:40:15 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:40:15 INFO DAGScheduler: Missing parents: List()
24/06/25 22:40:15 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:40:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/25 22:40:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/25 22:40:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:34205 (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:40:15 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/25 22:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:40:15 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/25 22:40:15 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:40:15 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/25 22:40:15 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@117dda8a]. The input RDD has 1 partitions.
24/06/25 22:40:15 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:40:15 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:40:15 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:40:15 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:40:15 INFO DAGScheduler: Missing parents: List()
24/06/25 22:40:15 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:40:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=195 untilOffset=198, for query queryId=6ee08699-6bc2-4886-94fe-3d7e0cfdbde9 batchId=2 taskId=4 partitionId=0
24/06/25 22:40:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/25 22:40:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/25 22:40:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:34205 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:40:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/25 22:40:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:40:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/25 22:40:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/25 22:40:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/25 22:40:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to offset 195 for partition logs-0
24/06/25 22:40:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:40:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=195 untilOffset=198, for query queryId=4178d045-aa3e-483a-8890-f55a82bafb0f batchId=2 taskId=5 partitionId=0
24/06/25 22:40:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:34205 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:40:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to offset 195 for partition logs-0
24/06/25 22:40:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:40:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:40:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to latest offset of partition logs-0
24/06/25 22:40:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=198, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:40:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:40:16 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
24/06/25 22:40:16 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor read 3 records through 1 polls (polled  out 3 records), taking 590525788 nanos, during time span of 594181529 nanos.
24/06/25 22:40:16 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2377 bytes result sent to driver
24/06/25 22:40:16 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 646 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:40:16 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.658 s
24/06/25 22:40:16 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/25 22:40:16 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:40:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/25 22:40:16 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.674703 s
24/06/25 22:40:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 2
-------------------------------------------
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/25 22:40:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:40:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:40:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to latest offset of partition logs-0
24/06/25 22:40:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=198, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:40:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:40:16 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
24/06/25 22:40:16 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor read 3 records through 1 polls (polled  out 3 records), taking 548292402 nanos, during time span of 557137220 nanos.
24/06/25 22:40:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/2 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.2.698b1d65-163d-4b07-8495-3f7b66ff3a9d.tmp
24/06/25 22:40:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3345 bytes result sent to driver
24/06/25 22:40:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 690 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:40:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/25 22:40:16 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.715 s
24/06/25 22:40:16 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:40:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/25 22:40:16 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.727529 s
24/06/25 22:40:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@117dda8a] is committing.
24/06/25 22:40:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@117dda8a] committed.
24/06/25 22:40:16 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:34205 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:40:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/2 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.2.0c3ebd5e-2c0a-426b-a7d7-2b10281a208f.tmp
24/06/25 22:40:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.2.698b1d65-163d-4b07-8495-3f7b66ff3a9d.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/2
24/06/25 22:40:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "6ee08699-6bc2-4886-94fe-3d7e0cfdbde9",
  "runId" : "621ac50d-f085-4520-872b-92e52835d306",
  "name" : null,
  "timestamp" : "2024-06-25T22:40:15.251Z",
  "batchId" : 2,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 2.9732408325074333,
  "durationMs" : {
    "addBatch" : 774,
    "commitOffsets" : 124,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 19,
    "triggerExecution" : 1009,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 195
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 198
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 198
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 2.9732408325074333,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@10145c1",
    "numOutputRows" : 3
  }
}
24/06/25 22:40:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.2.0c3ebd5e-2c0a-426b-a7d7-2b10281a208f.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/2
24/06/25 22:40:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4178d045-aa3e-483a-8890-f55a82bafb0f",
  "runId" : "3961123f-367c-4f38-8bda-2c0c55422a36",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:40:15.251Z",
  "batchId" : 2,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 2.9440628066732093,
  "durationMs" : {
    "addBatch" : 829,
    "commitOffsets" : 87,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 28,
    "triggerExecution" : 1019,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 195
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 198
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 198
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 2.9440628066732093,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:40:21 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:34205 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:40:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:41:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/3 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.3.f6442b03-451a-4ef2-b833-2bda591b48f8.tmp
24/06/25 22:42:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/3 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.3.52ba154e-9c46-4b16-8e24-6e70b1d5b539.tmp
24/06/25 22:42:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/.3.f6442b03-451a-4ef2-b833-2bda591b48f8.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/offsets/3
24/06/25 22:42:43 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719355363103,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:42:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/.3.52ba154e-9c46-4b16-8e24-6e70b1d5b539.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/offsets/3
24/06/25 22:42:43 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719355363103,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:42:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:42:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:42:43 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:42:43 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:42:43 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:42:43 INFO DAGScheduler: Missing parents: List()
24/06/25 22:42:43 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:42:43 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.4 KiB, free 434.4 MiB)
24/06/25 22:42:43 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.4 MiB)
24/06/25 22:42:43 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:34205 (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:42:43 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
24/06/25 22:42:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:42:43 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/25 22:42:43 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:42:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3044bb0a]. The input RDD has 1 partitions.
24/06/25 22:42:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:42:43 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:42:43 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:42:43 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:42:43 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
24/06/25 22:42:43 INFO DAGScheduler: Missing parents: List()
24/06/25 22:42:43 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[33] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:42:43 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/25 22:42:43 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/25 22:42:43 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:34205 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:42:43 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
24/06/25 22:42:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[33] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:42:43 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/06/25 22:42:43 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/25 22:42:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=198 untilOffset=201, for query queryId=6ee08699-6bc2-4886-94fe-3d7e0cfdbde9 batchId=3 taskId=6 partitionId=0
24/06/25 22:42:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to offset 198 for partition logs-0
24/06/25 22:42:43 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:42:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=198 untilOffset=201, for query queryId=4178d045-aa3e-483a-8890-f55a82bafb0f batchId=3 taskId=7 partitionId=0
24/06/25 22:42:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to offset 198 for partition logs-0
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Seeking to latest offset of partition logs-0
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=201, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:42:43 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:42:43 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
24/06/25 22:42:43 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor read 3 records through 1 polls (polled  out 3 records), taking 519323195 nanos, during time span of 523244197 nanos.
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Seeking to latest offset of partition logs-0
24/06/25 22:42:43 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2420 bytes result sent to driver
24/06/25 22:42:43 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 648 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:42:43 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/25 22:42:43 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.667 s
24/06/25 22:42:43 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:42:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/25 22:42:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=201, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:42:43 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.680637 s
24/06/25 22:42:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------24/06/25 22:42:44 INFO DataWritingSparkTask: Writer for partition 0 is committing.

Batch: 324/06/25 22:42:44 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)

-------------------------------------------24/06/25 22:42:44 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor read 3 records through 1 polls (polled  out 3 records), taking 600139160 nanos, during time span of 613393675 nanos.

24/06/25 22:42:44 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3302 bytes result sent to driver
24/06/25 22:42:44 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 703 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:42:44 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/06/25 22:42:44 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.720 s
24/06/25 22:42:44 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:42:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/06/25 22:42:44 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.737702 s
24/06/25 22:42:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3044bb0a] is committing.
24/06/25 22:42:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3044bb0a] committed.
24/06/25 22:42:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/3 using temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.3.d131d466-aafa-4cc5-97a4-d22ad5e96495.tmp
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/25 22:42:44 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:42:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/3 using temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.3.637a64f4-48d3-41af-aaf0-77cdc1b5a8f6.tmp
24/06/25 22:42:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/.3.d131d466-aafa-4cc5-97a4-d22ad5e96495.tmp to file:/tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9/commits/3
24/06/25 22:42:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4178d045-aa3e-483a-8890-f55a82bafb0f",
  "runId" : "3961123f-367c-4f38-8bda-2c0c55422a36",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:42:43.098Z",
  "batchId" : 3,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 200.0,
  "processedRowsPerSecond" : 2.9069767441860463,
  "durationMs" : {
    "addBatch" : 827,
    "commitOffsets" : 88,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 19,
    "triggerExecution" : 1032,
    "walCommit" : 93
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 198
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 200.0,
    "processedRowsPerSecond" : 2.9069767441860463,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:42:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/.3.637a64f4-48d3-41af-aaf0-77cdc1b5a8f6.tmp to file:/tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00/commits/3
24/06/25 22:42:44 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "6ee08699-6bc2-4886-94fe-3d7e0cfdbde9",
  "runId" : "621ac50d-f085-4520-872b-92e52835d306",
  "name" : null,
  "timestamp" : "2024-06-25T22:42:43.098Z",
  "batchId" : 3,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 200.0,
  "processedRowsPerSecond" : 2.819548872180451,
  "durationMs" : {
    "addBatch" : 883,
    "commitOffsets" : 69,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 25,
    "triggerExecution" : 1064,
    "walCommit" : 82
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 198
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 200.0,
    "processedRowsPerSecond" : 2.819548872180451,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@10145c1",
    "numOutputRows" : 3
  }
}
24/06/25 22:42:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:34205 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:42:51 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:34205 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:42:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:42:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:43:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:03 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/25 22:44:03 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/25 22:44:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:44:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:45:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:46:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:47:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:23 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:48:23 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1, groupId=spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:48:23 INFO Metrics: Metrics scheduler closed
24/06/25 22:48:23 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:48:23 INFO Metrics: Metrics reporters closed
24/06/25 22:48:23 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-9ffdafb6-fbff-44d0-92c4-efdc6aa90312--524399326-executor-1 unregistered
24/06/25 22:48:23 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:48:23 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2, groupId=spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:48:23 INFO Metrics: Metrics scheduler closed
24/06/25 22:48:23 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:48:23 INFO Metrics: Metrics reporters closed
24/06/25 22:48:23 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d9492679-82aa-4eb5-ae34-2c810c66789a-836414552-executor-2 unregistered
24/06/25 22:48:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:48:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:49:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:50:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:51:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:52:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:53:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:54:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:55:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o17.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
24/06/25 22:55:37 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:55:37 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:55:37 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:55:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:55:37 INFO MemoryStore: MemoryStore cleared
24/06/25 22:55:37 INFO BlockManager: BlockManager stopped
24/06/25 22:55:37 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/25 22:55:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:55:37 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:55:37 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:55:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e
Traceback (most recent call last):
  File "/vagrant_data/spark_kafka_consumer.py", line 44, in <module>
24/06/25 22:55:37 INFO ShutdownHookManager: Deleting directory /tmp/temporary-b428e5f7-dbbe-4d43-9753-af06d6d1db00
    query_console = (logs
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
24/06/25 22:55:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-02f0b3f4-9f6a-4650-8ddf-a3b17f6d5b46
py4j.protocol.Py4JError: An error occurred while calling o50.awaitTermination
Error in sys.excepthook:
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/apport_python_hook.py", line 72, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File "/usr/lib/python3/dist-packages/apport/__init__.py", line 5, in <module>
    from apport.report import Report
  File "/usr/lib/python3/dist-packages/apport/report.py", line 30, in <module>
    import problem_report
  File "/usr/lib/python3/dist-packages/problem_report.py", line 15, in <module>
24/06/25 22:55:37 INFO ShutdownHookManager: Deleting directory /tmp/temporary-cf453717-9596-4b42-a26b-bbd5841247e9
    import zlib, base64, time, sys, gzip, struct, os
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o55.cancelAllJobs.
: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.
This stopped SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.base/java.lang.Thread.run(Thread.java:829)

The currently active SparkContext was created at:

org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
py4j.ClientServerConnection.run(ClientServerConnection.java:106)
java.base/java.lang.Thread.run(Thread.java:829)
         
	at org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:122)
	at org.apache.spark.SparkContext.cancelAllJobs(SparkContext.scala:2596)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


Original exception was:
Traceback (most recent call last):
  File "/vagrant_data/spark_kafka_consumer.py", line 44, in <module>
    query_console = (logs
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/usr/local/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o50.awaitTermination
24/06/25 22:55:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-c1098683-22a9-4a4b-bca5-bbe17eb56e9e/pyspark-87dacaa6-c67d-4306-a706-04bb7cba4808
Starting Spark at Tue Jun 25 10:55:55 PM UTC 2024
24/06/25 22:55:57 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:55:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-b099841a-e7e1-4a6d-b5be-d82a6cc429a1;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 539ms :: artifacts dl 13ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-b099841a-e7e1-4a6d-b5be-d82a6cc429a1
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/8ms)
24/06/25 22:55:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:55:59 INFO SparkContext: Running Spark version 3.5.1
24/06/25 22:55:59 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:55:59 INFO SparkContext: Java version 11.0.20.1
24/06/25 22:55:59 INFO ResourceUtils: ==============================================================
24/06/25 22:55:59 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/25 22:55:59 INFO ResourceUtils: ==============================================================
24/06/25 22:55:59 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/25 22:55:59 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/25 22:55:59 INFO ResourceProfile: Limiting resource is cpu
24/06/25 22:55:59 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/25 22:55:59 INFO SecurityManager: Changing view acls to: root
24/06/25 22:55:59 INFO SecurityManager: Changing modify acls to: root
24/06/25 22:55:59 INFO SecurityManager: Changing view acls groups to: 
24/06/25 22:55:59 INFO SecurityManager: Changing modify acls groups to: 
24/06/25 22:55:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/25 22:56:00 INFO Utils: Successfully started service 'sparkDriver' on port 33873.
24/06/25 22:56:00 INFO SparkEnv: Registering MapOutputTracker
24/06/25 22:56:00 INFO SparkEnv: Registering BlockManagerMaster
24/06/25 22:56:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/25 22:56:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/25 22:56:00 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/25 22:56:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4ed58cb4-aae2-4a98-84a8-ce3557c48a45
24/06/25 22:56:00 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/25 22:56:00 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/25 22:56:00 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/25 22:56:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:33873/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:33873/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:33873/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:33873/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:33873/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:33873/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:33873/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:33873/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:33873/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:33873/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:33873/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:56:00 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:00 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:56:01 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:56:01 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/25 22:56:01 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/25 22:56:01 INFO Executor: Java version 11.0.20.1
24/06/25 22:56:01 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/25 22:56:01 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@43bc8049 for default.
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:56:01 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:33873 after 36 ms (0 ms spent in bootstraps)
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp8047196269808598551.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp8047196269808598551.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp1300269088825520109.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp1300269088825520109.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/commons-logging_commons-logging-1.1.3.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp14856812057862164266.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp14856812057862164266.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.lz4_lz4-java-1.8.0.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp3140573686820292669.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp3140573686820292669.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp6051982175831664925.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp6051982175831664925.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.slf4j_slf4j-api-2.0.7.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp9006352080148866768.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp9006352080148866768.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp4736442521998575274.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp4736442521998575274.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.commons_commons-pool2-2.11.1.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp10360646295025368473.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp10360646295025368473.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp5238640923921188363.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp5238640923921188363.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp13210562251802841744.tmp
24/06/25 22:56:01 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp13210562251802841744.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/25 22:56:01 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/25 22:56:01 INFO Executor: Fetching spark://10.0.2.15:33873/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719356159781
24/06/25 22:56:01 INFO Utils: Fetching spark://10.0.2.15:33873/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp3264162067972367054.tmp
24/06/25 22:56:02 INFO Utils: /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/fetchFileTemp3264162067972367054.tmp has been previously copied to /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/25 22:56:02 INFO Executor: Adding file:/tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/userFiles-d291ca9b-cf50-4324-a81f-ceb01cdc53bb/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/25 22:56:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34841.
24/06/25 22:56:02 INFO NettyBlockTransferService: Server created on 10.0.2.15:34841
24/06/25 22:56:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/25 22:56:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34841, None)
24/06/25 22:56:02 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34841 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 34841, None)
24/06/25 22:56:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34841, None)
24/06/25 22:56:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34841, None)
24/06/25 22:56:02 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/25 22:56:02 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/25 22:56:04 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/25 22:56:04 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:56:04 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b resolved to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b.
24/06/25 22:56:04 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:56:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/metadata using temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/.metadata.fbd6bea6-aae7-4896-bc49-704d2fc1fc9a.tmp
24/06/25 22:56:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/.metadata.fbd6bea6-aae7-4896-bc49-704d2fc1fc9a.tmp to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/metadata
24/06/25 22:56:05 INFO MicroBatchExecution: Starting logs_table [id = a5aadb17-54e1-4eea-afbf-8dc448fd42fc, runId = 3e5252ff-7a80-43b8-9dd0-257b9de1abea]. Use file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b to store the query checkpoint.
24/06/25 22:56:05 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7f8d3104] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@111936eb]
24/06/25 22:56:05 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:56:05 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:56:05 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:56:05 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:56:05 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:56:05 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/25 22:56:05 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce resolved to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce.
24/06/25 22:56:05 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/25 22:56:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/metadata using temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/.metadata.9e07ce0d-d591-4fcf-ba9a-55c8311f74da.tmp
24/06/25 22:56:05 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:56:05 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:56:05 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:56:05 INFO AppInfoParser: Kafka startTimeMs: 1719356165886
24/06/25 22:56:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/.metadata.9e07ce0d-d591-4fcf-ba9a-55c8311f74da.tmp to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/metadata
24/06/25 22:56:05 INFO MicroBatchExecution: Starting [id = 15a30101-56b0-4e27-ade9-3f3a41e856fe, runId = 35a2044c-eaa5-47c0-9e66-c55f6a38a399]. Use file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce to store the query checkpoint.
24/06/25 22:56:05 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7f8d3104] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@111936eb]
24/06/25 22:56:05 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:56:05 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/25 22:56:05 INFO MicroBatchExecution: Starting new streaming query.
24/06/25 22:56:05 INFO MicroBatchExecution: Stream started from {}
24/06/25 22:56:05 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/25 22:56:05 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/25 22:56:05 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:56:05 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:56:05 INFO AppInfoParser: Kafka startTimeMs: 1719356165986
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/25 22:56:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/sources/0/0 using temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/sources/0/.0.d6d924ee-5322-4f2c-b5fd-81603e1de1fb.tmp
24/06/25 22:56:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/sources/0/0 using temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/sources/0/.0.f07532b6-8d1a-4282-b3a3-de0519833a7a.tmp
24/06/25 22:56:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/sources/0/.0.d6d924ee-5322-4f2c-b5fd-81603e1de1fb.tmp to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/sources/0/0
24/06/25 22:56:06 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":201}}
24/06/25 22:56:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/sources/0/.0.f07532b6-8d1a-4282-b3a3-de0519833a7a.tmp to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/sources/0/0
24/06/25 22:56:06 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":201}}
24/06/25 22:56:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/0 using temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/.0.476d1c5a-892e-4ada-8ca7-f9f31caebaf0.tmp
24/06/25 22:56:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/0 using temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/.0.590ce444-e8fa-4561-9a5a-f98a1166aa93.tmp
24/06/25 22:56:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/.0.476d1c5a-892e-4ada-8ca7-f9f31caebaf0.tmp to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/0
24/06/25 22:56:06 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719356166591,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:56:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/.0.590ce444-e8fa-4561-9a5a-f98a1166aa93.tmp to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/0
24/06/25 22:56:06 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719356166610,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:56:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:06 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:56:07 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/25 22:56:07 INFO CodeGenerator: Code generated in 353.281616 ms
24/06/25 22:56:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:56:07 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13996cfa]. The input RDD has 1 partitions.
24/06/25 22:56:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:56:07 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:56:07 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:56:07 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:56:07 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:56:07 INFO DAGScheduler: Missing parents: List()
24/06/25 22:56:07 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:56:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/25 22:56:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/25 22:56:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:34841 (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:56:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/25 22:56:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:56:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/25 22:56:08 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:56:08 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:56:08 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:56:08 INFO DAGScheduler: Missing parents: List()
24/06/25 22:56:08 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:56:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/25 22:56:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/25 22:56:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:34841 (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:56:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/25 22:56:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:56:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/25 22:56:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:56:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/25 22:56:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/25 22:56:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/25 22:56:08 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:56:08 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/25 22:56:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/25 22:56:08 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:56:08 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/25 22:56:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 240 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:56:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/25 22:56:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/25 22:56:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 188 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:56:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/25 22:56:08 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.492 s
24/06/25 22:56:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:56:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/25 22:56:08 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.601870 s
24/06/25 22:56:08 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.290 s
24/06/25 22:56:08 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:56:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/25 22:56:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/25 22:56:08 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.597473 s
24/06/25 22:56:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13996cfa] is committing.
-------------------------------------------
24/06/25 22:56:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@13996cfa] committed.
Batch: 0
-------------------------------------------
24/06/25 22:56:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/0 using temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/.0.3d3bc1d9-b4ee-4382-8dd2-a6c0e3e9705b.tmp
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/25 22:56:08 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:56:08 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/0 using temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/.0.3f2c5164-2f6f-4391-b8b6-3401486eac7e.tmp
24/06/25 22:56:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/.0.3d3bc1d9-b4ee-4382-8dd2-a6c0e3e9705b.tmp to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/0
24/06/25 22:56:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "a5aadb17-54e1-4eea-afbf-8dc448fd42fc",
  "runId" : "3e5252ff-7a80-43b8-9dd0-257b9de1abea",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:56:05.137Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1480,
    "commitOffsets" : 118,
    "getBatch" : 36,
    "latestOffset" : 1453,
    "queryPlanning" : 255,
    "triggerExecution" : 3511,
    "walCommit" : 141
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/25 22:56:08 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/.0.3f2c5164-2f6f-4391-b8b6-3401486eac7e.tmp to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/0
24/06/25 22:56:08 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "15a30101-56b0-4e27-ade9-3f3a41e856fe",
  "runId" : "35a2044c-eaa5-47c0-9e66-c55f6a38a399",
  "name" : null,
  "timestamp" : "2024-06-25T22:56:05.958Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1567,
    "commitOffsets" : 117,
    "getBatch" : 40,
    "latestOffset" : 627,
    "queryPlanning" : 259,
    "triggerExecution" : 2778,
    "walCommit" : 139
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@7b4afa06",
    "numOutputRows" : 0
  }
}
24/06/25 22:56:16 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:34841 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/25 22:56:16 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:34841 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/25 22:56:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [25/Jun/2024 22:56:51] "GET / HTTP/1.1" 404 -
24/06/25 22:56:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:56:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/1 using temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/.1.56e58a74-202f-485d-bffc-a9610701befb.tmp
24/06/25 22:57:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/1 using temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/.1.8a7bdc6e-1d63-4684-8c81-3b2aeef73750.tmp
24/06/25 22:57:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/.1.56e58a74-202f-485d-bffc-a9610701befb.tmp to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/offsets/1
24/06/25 22:57:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719356224057,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:57:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/.1.8a7bdc6e-1d63-4684-8c81-3b2aeef73750.tmp to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/offsets/1
24/06/25 22:57:04 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719356224062,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/25 22:57:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/25 22:57:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51692b85]. The input RDD has 1 partitions.
24/06/25 22:57:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:57:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/25 22:57:04 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:57:04 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:57:04 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:57:04 INFO DAGScheduler: Missing parents: List()
24/06/25 22:57:04 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:57:04 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.4 KiB, free 434.4 MiB)
24/06/25 22:57:04 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.4 MiB)
24/06/25 22:57:04 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:34841 (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:57:04 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/25 22:57:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:57:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/25 22:57:04 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:57:04 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/25 22:57:04 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:57:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:57:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/25 22:57:04 INFO DAGScheduler: Missing parents: List()
24/06/25 22:57:04 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:57:04 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/25 22:57:04 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/25 22:57:04 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:34841 (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:57:04 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/25 22:57:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:57:04 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/25 22:57:04 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/25 22:57:04 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/25 22:57:04 INFO CodeGenerator: Code generated in 30.848909 ms
24/06/25 22:57:04 INFO CodeGenerator: Code generated in 13.462191 ms
24/06/25 22:57:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=201 untilOffset=204, for query queryId=15a30101-56b0-4e27-ade9-3f3a41e856fe batchId=1 taskId=2 partitionId=0
24/06/25 22:57:04 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=201 untilOffset=204, for query queryId=a5aadb17-54e1-4eea-afbf-8dc448fd42fc batchId=1 taskId=3 partitionId=0
24/06/25 22:57:04 INFO CodeGenerator: Code generated in 16.273693 ms
24/06/25 22:57:04 INFO CodeGenerator: Code generated in 34.965136 ms
24/06/25 22:57:04 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:57:04 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/25 22:57:04 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:57:04 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:57:04 INFO AppInfoParser: Kafka startTimeMs: 1719356224676
24/06/25 22:57:04 INFO AppInfoParser: Kafka version: 3.4.1
24/06/25 22:57:04 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/25 22:57:04 INFO AppInfoParser: Kafka startTimeMs: 1719356224676
24/06/25 22:57:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Assigned to partition(s): logs-0
24/06/25 22:57:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Assigned to partition(s): logs-0
24/06/25 22:57:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Seeking to offset 201 for partition logs-0
24/06/25 22:57:04 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Seeking to offset 201 for partition logs-0
24/06/25 22:57:04 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:57:04 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:57:04 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/25 22:57:04 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/25 22:57:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:57:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Seeking to earliest offset of partition logs-0
24/06/25 22:57:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:57:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Seeking to latest offset of partition logs-0
24/06/25 22:57:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:57:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Seeking to latest offset of partition logs-0
24/06/25 22:57:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=204, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:57:05 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=204, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/25 22:57:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/25 22:57:05 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/25 22:57:05 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor read 3 records through 1 polls (polled  out 3 records), taking 661907816 nanos, during time span of 853477437 nanos.
24/06/25 22:57:05 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2377 bytes result sent to driver
24/06/25 22:57:05 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1247 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:57:05 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/25 22:57:05 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.309 s
24/06/25 22:57:05 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:57:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/25 22:57:05 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.326573 s
24/06/25 22:57:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/25 22:57:05 INFO CodeGenerator: Code generated in 9.419495 ms
24/06/25 22:57:06 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:34841 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/25 22:57:06 INFO CodeGenerator: Code generated in 12.832927 ms
24/06/25 22:57:06 INFO CodeGenerator: Code generated in 13.129227 ms
24/06/25 22:57:06 INFO DataWritingSparkTask: Writer for partition 0 is committing.
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/25 22:57:06 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/25 22:57:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/25 22:57:06 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor read 3 records through 1 polls (polled  out 3 records), taking 661701330 nanos, during time span of 1938144951 nanos.
24/06/25 22:57:06 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3345 bytes result sent to driver
24/06/25 22:57:06 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2287 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:57:06 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.307 s
24/06/25 22:57:06 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:57:06 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/25 22:57:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/25 22:57:06 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.410664 s
24/06/25 22:57:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51692b85] is committing.
24/06/25 22:57:06 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@51692b85] committed.
24/06/25 22:57:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/1 using temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/.1.18d570f8-3b67-4cda-9a1b-3dfd63d1f49c.tmp
24/06/25 22:57:06 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/1 using temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/.1.3f538b54-f6e9-4663-aa26-25592b2188ff.tmp
24/06/25 22:57:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/.1.18d570f8-3b67-4cda-9a1b-3dfd63d1f49c.tmp to file:/tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce/commits/1
24/06/25 22:57:06 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/.1.3f538b54-f6e9-4663-aa26-25592b2188ff.tmp to file:/tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b/commits/1
24/06/25 22:57:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "15a30101-56b0-4e27-ade9-3f3a41e856fe",
  "runId" : "35a2044c-eaa5-47c0-9e66-c55f6a38a399",
  "name" : null,
  "timestamp" : "2024-06-25T22:57:04.055Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 1.1240164855751218,
  "durationMs" : {
    "addBatch" : 2463,
    "commitOffsets" : 95,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 29,
    "triggerExecution" : 2669,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 1.1240164855751218,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@7b4afa06",
    "numOutputRows" : 3
  }
}
24/06/25 22:57:06 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "a5aadb17-54e1-4eea-afbf-8dc448fd42fc",
  "runId" : "3e5252ff-7a80-43b8-9dd0-257b9de1abea",
  "name" : "logs_table",
  "timestamp" : "2024-06-25T22:57:04.052Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 200.0,
  "processedRowsPerSecond" : 1.1206574523720583,
  "durationMs" : {
    "addBatch" : 2487,
    "commitOffsets" : 75,
    "getBatch" : 1,
    "latestOffset" : 4,
    "queryPlanning" : 34,
    "triggerExecution" : 2677,
    "walCommit" : 74
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 201
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 200.0,
    "processedRowsPerSecond" : 1.1206574523720583,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/25 22:57:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:34841 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/25 22:57:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:38 INFO CodeGenerator: Code generated in 50.099402 ms
24/06/25 22:57:38 INFO CodeGenerator: Code generated in 7.672196 ms
127.0.0.1 - - [25/Jun/2024 22:57:38] "GET /logs HTTP/1.1" 200 -
24/06/25 22:57:45 INFO CodeGenerator: Code generated in 30.435988 ms
24/06/25 22:57:45 INFO CodeGenerator: Code generated in 16.159453 ms
24/06/25 22:57:45 INFO SparkContext: Starting job: listTables at NativeMethodAccessorImpl.java:0
24/06/25 22:57:45 INFO DAGScheduler: Got job 4 (listTables at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:57:45 INFO DAGScheduler: Final stage: ResultStage 4 (listTables at NativeMethodAccessorImpl.java:0)
24/06/25 22:57:45 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:57:45 INFO DAGScheduler: Missing parents: List()
24/06/25 22:57:45 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[20] at listTables at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:57:45 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.8 KiB, free 434.4 MiB)
24/06/25 22:57:45 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 434.4 MiB)
24/06/25 22:57:45 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:34841 (size: 4.3 KiB, free: 434.4 MiB)
24/06/25 22:57:45 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/25 22:57:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[20] at listTables at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:57:45 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/25 22:57:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9835 bytes) 
24/06/25 22:57:45 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/25 22:57:45 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1326 bytes result sent to driver
24/06/25 22:57:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 34 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:57:46 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/25 22:57:46 INFO DAGScheduler: ResultStage 4 (listTables at NativeMethodAccessorImpl.java:0) finished in 0.057 s
24/06/25 22:57:46 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:57:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/25 22:57:46 INFO DAGScheduler: Job 4 finished: listTables at NativeMethodAccessorImpl.java:0, took 0.068611 s
24/06/25 22:57:46 INFO CodeGenerator: Code generated in 47.617465 ms
24/06/25 22:57:46 INFO CodeGenerator: Code generated in 18.972724 ms
24/06/25 22:57:46 INFO SparkContext: Starting job: hasNext at NativeMethodAccessorImpl.java:0
24/06/25 22:57:46 INFO DAGScheduler: Got job 5 (hasNext at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/25 22:57:46 INFO DAGScheduler: Final stage: ResultStage 5 (hasNext at NativeMethodAccessorImpl.java:0)
24/06/25 22:57:46 INFO DAGScheduler: Parents of final stage: List()
24/06/25 22:57:46 INFO DAGScheduler: Missing parents: List()
24/06/25 22:57:46 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at toLocalIterator at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/25 22:57:46 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.5 KiB, free 434.4 MiB)
24/06/25 22:57:46 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 434.4 MiB)
24/06/25 22:57:46 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:34841 (size: 3.0 KiB, free: 434.4 MiB)
24/06/25 22:57:46 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/25 22:57:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at toLocalIterator at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/25 22:57:46 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/25 22:57:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9883 bytes) 
24/06/25 22:57:46 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/25 22:57:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1337 bytes result sent to driver
24/06/25 22:57:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 38 ms on 10.0.2.15 (executor driver) (1/1)
24/06/25 22:57:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/25 22:57:46 INFO DAGScheduler: ResultStage 5 (hasNext at NativeMethodAccessorImpl.java:0) finished in 0.057 s
24/06/25 22:57:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/25 22:57:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/25 22:57:46 INFO DAGScheduler: Job 5 finished: hasNext at NativeMethodAccessorImpl.java:0, took 0.071972 s
24/06/25 22:57:46 INFO CodeGenerator: Code generated in 27.451905 ms
10.0.2.2 - - [25/Jun/2024 22:57:46] "GET /tables HTTP/1.1" 200 -
24/06/25 22:57:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:47 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:34841 in memory (size: 3.0 KiB, free: 434.4 MiB)
24/06/25 22:57:47 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:34841 in memory (size: 4.3 KiB, free: 434.4 MiB)
10.0.2.2 - - [25/Jun/2024 22:57:51] "GET /logs HTTP/1.1" 200 -
24/06/25 22:57:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:57:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:58:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:58:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:58:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:58:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/25 22:58:19 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/25 22:58:19 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node 0 disconnected.
24/06/25 22:58:19 INFO NetworkClient: [AdminClient clientId=adminclient-1] Cancelled in-flight METADATA request with correlation id 22579 due to node 0 being disconnected (elapsed time since creation: 14ms, elapsed time since send: 14ms, request timeout: 30000ms)
24/06/25 22:58:19 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/25 22:58:19 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node 0 disconnected.
24/06/25 22:58:19 INFO NetworkClient: [AdminClient clientId=adminclient-2] Cancelled in-flight METADATA request with correlation id 22624 due to node 0 being disconnected (elapsed time since creation: 25ms, elapsed time since send: 25ms, request timeout: 30000ms)
24/06/25 22:58:21 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node 0 disconnected.
24/06/25 22:58:22 WARN NetworkClient: [AdminClient clientId=adminclient-1] Connection to node 0 (DESKTOP-G8N3TE3.mshome.net/192.168.33.1:9092) could not be established. Broker may not be available.
24/06/25 22:58:22 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node 0 disconnected.
24/06/25 22:58:22 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 0 (DESKTOP-G8N3TE3.mshome.net/192.168.33.1:9092) could not be established. Broker may not be available.
24/06/25 22:58:22 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:58:22 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1, groupId=spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:58:22 INFO Metrics: Metrics scheduler closed
24/06/25 22:58:22 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:58:22 INFO Metrics: Metrics reporters closed
24/06/25 22:58:22 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d491fe58-d484-4184-b82b-3baae4514e59-1710128977-executor-1 unregistered
24/06/25 22:58:22 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/25 22:58:22 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2, groupId=spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/25 22:58:22 INFO Metrics: Metrics scheduler closed
24/06/25 22:58:22 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/25 22:58:22 INFO Metrics: Metrics reporters closed
24/06/25 22:58:22 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-dd328d34-93e4-4b56-a961-58eccc1216e1--233982963-executor-2 unregistered
24/06/25 22:58:22 INFO SparkContext: Invoking stop() from shutdown hook
24/06/25 22:58:22 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/25 22:58:22 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/25 22:58:22 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/25 22:58:22 INFO MemoryStore: MemoryStore cleared
24/06/25 22:58:22 INFO BlockManager: BlockManager stopped
24/06/25 22:58:22 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/25 22:58:22 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/25 22:58:22 INFO SparkContext: Successfully stopped SparkContext
24/06/25 22:58:22 INFO ShutdownHookManager: Shutdown hook called
24/06/25 22:58:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00
24/06/25 22:58:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-3c0c2fe6-6826-4dc3-9315-76083e306d00/pyspark-b3121781-2d78-4e5e-8117-c70ffbf4e1cc
24/06/25 22:58:22 INFO ShutdownHookManager: Deleting directory /tmp/temporary-d72bae1f-82ab-41d4-b2ff-dcf3f694f1ce
24/06/25 22:58:22 INFO ShutdownHookManager: Deleting directory /tmp/spark-91fd8546-e0e3-472b-b2ed-f877f53c7da3
24/06/25 22:58:22 INFO ShutdownHookManager: Deleting directory /tmp/temporary-7f30b5df-696b-40bb-a458-983525d4022b
Starting Spark at Wed Jun 26 10:51:07 AM UTC 2024
24/06/26 10:51:12 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 10:51:13 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-231356a2-0e43-4470-85f4-debf51b91d14;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 687ms :: artifacts dl 27ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-231356a2-0e43-4470-85f4-debf51b91d14
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/14ms)
24/06/26 10:51:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 10:51:16 INFO SparkContext: Running Spark version 3.5.1
24/06/26 10:51:16 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 10:51:16 INFO SparkContext: Java version 11.0.20.1
24/06/26 10:51:17 INFO ResourceUtils: ==============================================================
24/06/26 10:51:17 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 10:51:17 INFO ResourceUtils: ==============================================================
24/06/26 10:51:17 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 10:51:17 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 10:51:17 INFO ResourceProfile: Limiting resource is cpu
24/06/26 10:51:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 10:51:17 INFO SecurityManager: Changing view acls to: root
24/06/26 10:51:17 INFO SecurityManager: Changing modify acls to: root
24/06/26 10:51:17 INFO SecurityManager: Changing view acls groups to: 
24/06/26 10:51:17 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 10:51:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 10:51:17 INFO Utils: Successfully started service 'sparkDriver' on port 44163.
24/06/26 10:51:17 INFO SparkEnv: Registering MapOutputTracker
24/06/26 10:51:18 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 10:51:18 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 10:51:18 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 10:51:18 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 10:51:18 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-2a5171ff-660c-4494-8063-6791664642ea
24/06/26 10:51:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 10:51:18 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 10:51:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 10:51:18 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44163/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44163/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:44163/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:44163/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:44163/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:44163/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:44163/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:44163/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:44163/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:44163/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:44163/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 10:51:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 10:51:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 10:51:18 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 10:51:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 10:51:18 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:18 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 10:51:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.lz4_lz4-java-1.8.0.jar
24/06/26 10:51:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 10:51:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 10:51:19 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 10:51:19 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/commons-logging_commons-logging-1.1.3.jar
24/06/26 10:51:19 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 10:51:19 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 10:51:19 INFO Executor: Java version 11.0.20.1
24/06/26 10:51:19 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 10:51:19 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@622432cc for default.
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/commons-logging_commons-logging-1.1.3.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.lz4_lz4-java-1.8.0.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 10:51:19 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:19 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 10:51:19 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:44163 after 39 ms (0 ms spent in bootstraps)
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp5388282777476192921.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp5388282777476192921.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp6349780413106886296.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp6349780413106886296.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp11557412335697602906.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp11557412335697602906.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp18072069569657496149.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp18072069569657496149.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp6930240725815046520.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp6930240725815046520.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/commons-logging_commons-logging-1.1.3.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp5012450799767813867.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp5012450799767813867.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp13269143390131393721.tmp
24/06/26 10:51:20 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp13269143390131393721.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 10:51:20 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 10:51:20 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719399076972
24/06/26 10:51:20 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp10302704074870262908.tmp
24/06/26 10:51:21 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp10302704074870262908.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.lz4_lz4-java-1.8.0.jar
24/06/26 10:51:21 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 10:51:21 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719399076972
24/06/26 10:51:21 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp10738058365771014437.tmp
24/06/26 10:51:21 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp10738058365771014437.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 10:51:21 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 10:51:21 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719399076972
24/06/26 10:51:21 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp6616305042689834687.tmp
24/06/26 10:51:21 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp6616305042689834687.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 10:51:21 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 10:51:21 INFO Executor: Fetching spark://10.0.2.15:44163/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719399076972
24/06/26 10:51:21 INFO Utils: Fetching spark://10.0.2.15:44163/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp10072646685396132059.tmp
24/06/26 10:51:21 INFO Utils: /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/fetchFileTemp10072646685396132059.tmp has been previously copied to /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 10:51:21 INFO Executor: Adding file:/tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/userFiles-ad42c4db-74db-48d5-a37d-ce1e65ba593f/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 10:51:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42769.
24/06/26 10:51:21 INFO NettyBlockTransferService: Server created on 10.0.2.15:42769
24/06/26 10:51:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 10:51:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 42769, None)
24/06/26 10:51:21 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:42769 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 42769, None)
24/06/26 10:51:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 42769, None)
24/06/26 10:51:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 42769, None)
24/06/26 10:51:22 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 10:51:22 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 10:51:25 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 10:51:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 10:51:25 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1 resolved to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1.
24/06/26 10:51:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 10:51:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/metadata using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/.metadata.31345b5a-b808-45b7-94fd-36540ee07a69.tmp
24/06/26 10:51:25 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/.metadata.31345b5a-b808-45b7-94fd-36540ee07a69.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/metadata
24/06/26 10:51:25 INFO MicroBatchExecution: Starting logs_table [id = 7b0546ab-d66d-431f-81c7-27d2cead50a3, runId = cfbab90f-3635-4b6d-81cb-ab8baa0dfc68]. Use file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1 to store the query checkpoint.
24/06/26 10:51:25 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@67b77b4d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5bc2238]
24/06/26 10:51:25 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 10:51:25 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 10:51:25 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 10:51:25 INFO MicroBatchExecution: Stream started from {}
24/06/26 10:51:26 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 10:51:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 10:51:26 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091 resolved to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091.
24/06/26 10:51:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 10:51:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/metadata using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/.metadata.f9a54224-f9fa-411c-bb9b-a331487e6a54.tmp
24/06/26 10:51:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/.metadata.f9a54224-f9fa-411c-bb9b-a331487e6a54.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/metadata
24/06/26 10:51:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 10:51:26 INFO MicroBatchExecution: Starting [id = f9f67bb9-a075-4a62-b6ed-c3b01ffae271, runId = e8109bcf-695a-4c11-83e8-768984452610]. Use file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091 to store the query checkpoint.
24/06/26 10:51:26 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 10:51:26 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 10:51:26 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@67b77b4d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5bc2238]
24/06/26 10:51:26 INFO AppInfoParser: Kafka startTimeMs: 1719399086786
24/06/26 10:51:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 10:51:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 10:51:26 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 10:51:26 INFO MicroBatchExecution: Stream started from {}
24/06/26 10:51:26 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 10:51:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 10:51:26 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 10:51:26 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 10:51:26 INFO AppInfoParser: Kafka startTimeMs: 1719399086926
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 10:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/sources/0/0 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/sources/0/.0.40312c57-ba3f-4e1a-aacd-cd5f3a807006.tmp
24/06/26 10:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/sources/0/0 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/sources/0/.0.cb6d5ca3-4903-4dbc-ba33-3f5750b3d775.tmp
24/06/26 10:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/sources/0/.0.40312c57-ba3f-4e1a-aacd-cd5f3a807006.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/sources/0/0
24/06/26 10:51:27 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":204}}
24/06/26 10:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/sources/0/.0.cb6d5ca3-4903-4dbc-ba33-3f5750b3d775.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/sources/0/0
24/06/26 10:51:27 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":204}}
24/06/26 10:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/0 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/.0.f542ab0a-fca7-4b54-bb1e-c9e02347aafa.tmp
24/06/26 10:51:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/0 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/.0.7adcbfb1-bc41-40c8-8ac3-1a14305375cd.tmp
24/06/26 10:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/.0.7adcbfb1-bc41-40c8-8ac3-1a14305375cd.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/0
24/06/26 10:51:27 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719399087685,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 10:51:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/.0.f542ab0a-fca7-4b54-bb1e-c9e02347aafa.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/0
24/06/26 10:51:27 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719399087666,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 10:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:51:28 INFO CodeGenerator: Code generated in 327.480175 ms
24/06/26 10:51:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 10:51:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@651fda1]. The input RDD has 1 partitions.
24/06/26 10:51:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 10:51:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 10:51:28 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 10:51:28 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 10:51:28 INFO DAGScheduler: Parents of final stage: List()
24/06/26 10:51:28 INFO DAGScheduler: Missing parents: List()
24/06/26 10:51:28 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 10:51:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 10:51:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 10:51:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:42769 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 10:51:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 10:51:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 10:51:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 10:51:29 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 10:51:29 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 10:51:29 INFO DAGScheduler: Parents of final stage: List()
24/06/26 10:51:29 INFO DAGScheduler: Missing parents: List()
24/06/26 10:51:29 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 10:51:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 10:51:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 10:51:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:42769 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 10:51:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 10:51:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 10:51:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 10:51:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 10:51:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 10:51:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 10:51:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 10:51:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 10:51:29 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 10:51:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 10:51:29 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 10:51:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 10:51:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 10:51:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 176 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 10:51:29 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.283 s
24/06/26 10:51:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 10:51:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 278 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 10:51:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 10:51:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 10:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 10:51:29 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.602502 s
24/06/26 10:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@651fda1] is committing.
24/06/26 10:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@651fda1] committed.
24/06/26 10:51:29 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.580 s
24/06/26 10:51:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 10:51:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 10:51:29 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.643397 s
24/06/26 10:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 10:51:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/0 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/.0.e5500d94-3039-42b7-8b77-8cf76ce2e878.tmp
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 10:51:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 10:51:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/.0.e5500d94-3039-42b7-8b77-8cf76ce2e878.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/0
24/06/26 10:51:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/0 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/.0.cf0bdc3e-d123-43de-929e-31eb9ec8873f.tmp
24/06/26 10:51:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/.0.cf0bdc3e-d123-43de-929e-31eb9ec8873f.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/0
24/06/26 10:51:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "f9f67bb9-a075-4a62-b6ed-c3b01ffae271",
  "runId" : "e8109bcf-695a-4c11-83e8-768984452610",
  "name" : null,
  "timestamp" : "2024-06-26T10:51:26.805Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1604,
    "commitOffsets" : 68,
    "getBatch" : 29,
    "latestOffset" : 850,
    "queryPlanning" : 216,
    "triggerExecution" : 2890,
    "walCommit" : 101
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@5db5a3ec",
    "numOutputRows" : 0
  }
}
24/06/26 10:51:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7b0546ab-d66d-431f-81c7-27d2cead50a3",
  "runId" : "cfbab90f-3635-4b6d-81cb-ab8baa0dfc68",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T10:51:25.915Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1468,
    "commitOffsets" : 146,
    "getBatch" : 34,
    "latestOffset" : 1748,
    "queryPlanning" : 216,
    "triggerExecution" : 3728,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 10:51:32 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:42769 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 10:51:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:42769 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 10:51:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:51:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:51:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:51:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:51:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:51:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 10:52:05] "GET / HTTP/1.1" 404 -
24/06/26 10:52:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 10:52:13] "GET /logs HTTP/1.1" 200 -
24/06/26 10:52:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 10:52:50] "GET /logs HTTP/1.1" 200 -
127.0.0.1 - - [26/Jun/2024 10:52:51] "GET /logs HTTP/1.1" 200 -
24/06/26 10:52:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:52:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/1 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/.1.0f481b27-42cc-4652-aab0-302771c43d1c.tmp
24/06/26 10:53:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/1 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/.1.09450e07-714f-4f19-97fe-d5d4ef058cfc.tmp
24/06/26 10:53:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/.1.0f481b27-42cc-4652-aab0-302771c43d1c.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/1
24/06/26 10:53:43 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719399223284,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 10:53:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/.1.09450e07-714f-4f19-97fe-d5d4ef058cfc.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/1
24/06/26 10:53:43 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719399223299,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 10:53:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@24cc3d15]. The input RDD has 1 partitions.
24/06/26 10:53:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 10:53:43 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 10:53:43 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 10:53:43 INFO DAGScheduler: Parents of final stage: List()
24/06/26 10:53:43 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 10:53:43 INFO DAGScheduler: Missing parents: List()
24/06/26 10:53:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 10:53:43 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 10:53:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 10:53:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 10:53:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:42769 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 10:53:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 10:53:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 10:53:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 10:53:43 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 10:53:43 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 10:53:43 INFO DAGScheduler: Parents of final stage: List()
24/06/26 10:53:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 10:53:43 INFO DAGScheduler: Missing parents: List()
24/06/26 10:53:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 10:53:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 10:53:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 10:53:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 10:53:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:42769 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 10:53:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 10:53:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 10:53:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 10:53:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/26 10:53:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 10:53:44 INFO CodeGenerator: Code generated in 223.454213 ms
24/06/26 10:53:44 INFO CodeGenerator: Code generated in 122.840039 ms
24/06/26 10:53:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=204 untilOffset=207, for query queryId=f9f67bb9-a075-4a62-b6ed-c3b01ffae271 batchId=1 taskId=3 partitionId=0
24/06/26 10:53:44 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=204 untilOffset=207, for query queryId=7b0546ab-d66d-431f-81c7-27d2cead50a3 batchId=1 taskId=2 partitionId=0
24/06/26 10:53:44 INFO CodeGenerator: Code generated in 69.719885 ms
24/06/26 10:53:44 INFO CodeGenerator: Code generated in 65.260953 ms
24/06/26 10:53:44 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 10:53:45 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 10:53:45 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 10:53:45 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 10:53:45 INFO AppInfoParser: Kafka startTimeMs: 1719399225167
24/06/26 10:53:45 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 10:53:45 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 10:53:45 INFO AppInfoParser: Kafka startTimeMs: 1719399225167
24/06/26 10:53:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Assigned to partition(s): logs-0
24/06/26 10:53:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Assigned to partition(s): logs-0
24/06/26 10:53:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Seeking to offset 204 for partition logs-0
24/06/26 10:53:45 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Seeking to offset 204 for partition logs-0
24/06/26 10:53:45 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 10:53:45 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 10:53:45 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 10:53:45 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 10:53:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Seeking to earliest offset of partition logs-0
24/06/26 10:53:45 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Seeking to earliest offset of partition logs-0
24/06/26 10:53:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 10:53:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Seeking to latest offset of partition logs-0
24/06/26 10:53:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 10:53:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Seeking to latest offset of partition logs-0
24/06/26 10:53:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=207, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 10:53:46 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=207, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 10:53:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 10:53:46 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 10:53:46 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor read 3 records through 1 polls (polled  out 3 records), taking 850126541 nanos, during time span of 1239066003 nanos.
24/06/26 10:53:46 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2420 bytes result sent to driver
24/06/26 10:53:46 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2602 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 10:53:46 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.671 s
24/06/26 10:53:46 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 10:53:46 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 10:53:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 10:53:46 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.850081 s
24/06/26 10:53:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 10:53:46 INFO CodeGenerator: Code generated in 28.163935 ms
127.0.0.1 - - [26/Jun/2024 10:53:47] "GET /logs HTTP/1.1" 200 -
24/06/26 10:53:48 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:42769 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 10:53:49 INFO CodeGenerator: Code generated in 31.565283 ms
24/06/26 10:53:49 INFO CodeGenerator: Code generated in 31.753075 ms
24/06/26 10:53:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 10:53:49 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 10:53:49 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor read 3 records through 1 polls (polled  out 3 records), taking 848808353 nanos, during time span of 3909543317 nanos.
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/26 10:53:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 10:53:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3345 bytes result sent to driver
24/06/26 10:53:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5408 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 10:53:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 10:53:49 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 5.519 s
24/06/26 10:53:49 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 10:53:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 10:53:49 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 5.651183 s
24/06/26 10:53:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@24cc3d15] is committing.
24/06/26 10:53:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@24cc3d15] committed.
24/06/26 10:53:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/1 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/.1.4297a239-0807-43c0-a704-719fe9006e2b.tmp
24/06/26 10:53:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/1 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/.1.f8141fb9-54de-4a22-9276-1edad2491de8.tmp
24/06/26 10:53:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/.1.4297a239-0807-43c0-a704-719fe9006e2b.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/1
24/06/26 10:53:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "f9f67bb9-a075-4a62-b6ed-c3b01ffae271",
  "runId" : "e8109bcf-695a-4c11-83e8-768984452610",
  "name" : null,
  "timestamp" : "2024-06-26T10:53:43.273Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 0.49059689288634506,
  "durationMs" : {
    "addBatch" : 5697,
    "commitOffsets" : 237,
    "getBatch" : 0,
    "latestOffset" : 10,
    "queryPlanning" : 44,
    "triggerExecution" : 6115,
    "walCommit" : 122
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 207
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 207
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 0.49059689288634506,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@5db5a3ec",
    "numOutputRows" : 3
  }
}
24/06/26 10:53:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/.1.f8141fb9-54de-4a22-9276-1edad2491de8.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/1
24/06/26 10:53:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7b0546ab-d66d-431f-81c7-27d2cead50a3",
  "runId" : "cfbab90f-3635-4b6d-81cb-ab8baa0dfc68",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T10:53:43.290Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 0.4871711594673595,
  "durationMs" : {
    "addBatch" : 5754,
    "commitOffsets" : 234,
    "getBatch" : 0,
    "latestOffset" : 9,
    "queryPlanning" : 40,
    "triggerExecution" : 6158,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 204
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 207
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 207
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 0.4871711594673595,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/26 10:53:58 INFO CodeGenerator: Code generated in 46.970791 ms
24/06/26 10:53:58 INFO CodeGenerator: Code generated in 8.465525 ms
127.0.0.1 - - [26/Jun/2024 10:53:58] "GET /logs HTTP/1.1" 200 -
24/06/26 10:53:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:53:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:42769 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 10:54:05 INFO CodeGenerator: Code generated in 22.583545 ms
24/06/26 10:54:05 INFO CodeGenerator: Code generated in 6.695676 ms
24/06/26 10:54:05 INFO SparkContext: Starting job: listTables at NativeMethodAccessorImpl.java:0
24/06/26 10:54:05 INFO DAGScheduler: Got job 4 (listTables at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 10:54:05 INFO DAGScheduler: Final stage: ResultStage 4 (listTables at NativeMethodAccessorImpl.java:0)
24/06/26 10:54:05 INFO DAGScheduler: Parents of final stage: List()
24/06/26 10:54:05 INFO DAGScheduler: Missing parents: List()
24/06/26 10:54:05 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[20] at listTables at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 10:54:05 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.8 KiB, free 434.4 MiB)
24/06/26 10:54:05 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 434.4 MiB)
24/06/26 10:54:05 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:42769 (size: 4.3 KiB, free: 434.4 MiB)
24/06/26 10:54:05 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 10:54:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[20] at listTables at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 10:54:05 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 10:54:05 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9835 bytes) 
24/06/26 10:54:05 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 10:54:05 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1326 bytes result sent to driver
24/06/26 10:54:05 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 21 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 10:54:05 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 10:54:05 INFO DAGScheduler: ResultStage 4 (listTables at NativeMethodAccessorImpl.java:0) finished in 0.045 s
24/06/26 10:54:05 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 10:54:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/26 10:54:05 INFO DAGScheduler: Job 4 finished: listTables at NativeMethodAccessorImpl.java:0, took 0.055049 s
24/06/26 10:54:05 INFO CodeGenerator: Code generated in 33.977438 ms
24/06/26 10:54:05 INFO CodeGenerator: Code generated in 15.435864 ms
24/06/26 10:54:05 INFO SparkContext: Starting job: hasNext at NativeMethodAccessorImpl.java:0
24/06/26 10:54:05 INFO DAGScheduler: Got job 5 (hasNext at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 10:54:05 INFO DAGScheduler: Final stage: ResultStage 5 (hasNext at NativeMethodAccessorImpl.java:0)
24/06/26 10:54:05 INFO DAGScheduler: Parents of final stage: List()
24/06/26 10:54:05 INFO DAGScheduler: Missing parents: List()
24/06/26 10:54:05 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at toLocalIterator at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 10:54:05 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.5 KiB, free 434.4 MiB)
24/06/26 10:54:05 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 434.4 MiB)
24/06/26 10:54:05 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:42769 (size: 3.0 KiB, free: 434.4 MiB)
24/06/26 10:54:05 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 10:54:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at toLocalIterator at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 10:54:05 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/26 10:54:05 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:42769 in memory (size: 4.3 KiB, free: 434.4 MiB)
24/06/26 10:54:05 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9883 bytes) 
24/06/26 10:54:05 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/26 10:54:05 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1380 bytes result sent to driver
24/06/26 10:54:05 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 58 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 10:54:05 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/26 10:54:05 INFO DAGScheduler: ResultStage 5 (hasNext at NativeMethodAccessorImpl.java:0) finished in 0.128 s
24/06/26 10:54:05 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 10:54:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/26 10:54:05 INFO DAGScheduler: Job 5 finished: hasNext at NativeMethodAccessorImpl.java:0, took 0.138047 s
24/06/26 10:54:05 INFO CodeGenerator: Code generated in 28.082638 ms
127.0.0.1 - - [26/Jun/2024 10:54:05] "GET /tables HTTP/1.1" 200 -
24/06/26 10:54:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:11 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:42769 in memory (size: 3.0 KiB, free: 434.4 MiB)
24/06/26 10:54:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:54:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:55:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:27 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 10:56:27 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 10:56:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:56:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:57:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:58:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 10:59:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 10:59:44 INFO Metrics: Metrics scheduler closed
24/06/26 10:59:44 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 10:59:44 INFO Metrics: Metrics reporters closed
24/06/26 10:59:44 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-1 unregistered
24/06/26 10:59:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 10:59:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 10:59:44 INFO Metrics: Metrics scheduler closed
24/06/26 10:59:44 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 10:59:44 INFO Metrics: Metrics reporters closed
24/06/26 10:59:44 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-2 unregistered
24/06/26 10:59:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 10:59:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:00:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:01:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:02:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:03:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:04:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:05:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:06:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:07:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:08:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:09:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:10:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:11:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:12:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:13:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:14:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:15:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:16:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:17:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/2 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/.2.04d89852-2b68-496b-a1e0-d77910a72dd5.tmp
24/06/26 11:18:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/2 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/.2.afd208d8-a261-4b63-856c-d88bec3d6190.tmp
24/06/26 11:18:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/.2.04d89852-2b68-496b-a1e0-d77910a72dd5.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/offsets/2
24/06/26 11:18:16 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719400696689,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 11:18:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/.2.afd208d8-a261-4b63-856c-d88bec3d6190.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/offsets/2
24/06/26 11:18:16 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719400696692,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 11:18:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3ddd1b0f]. The input RDD has 1 partitions.
24/06/26 11:18:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 11:18:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 11:18:16 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 11:18:16 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
24/06/26 11:18:16 INFO DAGScheduler: Parents of final stage: List()
24/06/26 11:18:16 INFO DAGScheduler: Missing parents: List()
24/06/26 11:18:16 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[31] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 11:18:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 11:18:17 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.4 KiB, free 434.4 MiB)
24/06/26 11:18:17 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.4 MiB)
24/06/26 11:18:17 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:42769 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 11:18:17 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
24/06/26 11:18:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[31] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 11:18:17 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/26 11:18:17 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/26 11:18:17 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 11:18:17 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
24/06/26 11:18:17 INFO DAGScheduler: Parents of final stage: List()
24/06/26 11:18:17 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
24/06/26 11:18:17 INFO DAGScheduler: Missing parents: List()
24/06/26 11:18:17 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 11:18:17 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/26 11:18:17 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/26 11:18:17 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:42769 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 11:18:17 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
24/06/26 11:18:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[32] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 11:18:17 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/06/26 11:18:17 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 11:18:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=207 untilOffset=226, for query queryId=f9f67bb9-a075-4a62-b6ed-c3b01ffae271 batchId=2 taskId=6 partitionId=0
24/06/26 11:18:17 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
24/06/26 11:18:17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 11:18:17 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 11:18:17 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 11:18:17 INFO AppInfoParser: Kafka startTimeMs: 1719400697118
24/06/26 11:18:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Assigned to partition(s): logs-0
24/06/26 11:18:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Seeking to offset 207 for partition logs-0
24/06/26 11:18:17 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=207 untilOffset=226, for query queryId=7b0546ab-d66d-431f-81c7-27d2cead50a3 batchId=2 taskId=7 partitionId=0
24/06/26 11:18:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 11:18:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 11:18:17 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Seeking to earliest offset of partition logs-0
24/06/26 11:18:17 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 11:18:17 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 11:18:17 INFO AppInfoParser: Kafka startTimeMs: 1719400697294
24/06/26 11:18:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Assigned to partition(s): logs-0
24/06/26 11:18:17 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Seeking to offset 207 for partition logs-0
24/06/26 11:18:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 11:18:17 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Seeking to earliest offset of partition logs-0
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Seeking to latest offset of partition logs-0
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=226, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 11:18:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 11:18:17 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
24/06/26 11:18:17 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor read 19 records through 1 polls (polled  out 19 records), taking 672474527 nanos, during time span of 682145114 nanos.
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Seeking to latest offset of partition logs-0
24/06/26 11:18:17 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4470 bytes result sent to driver
24/06/26 11:18:17 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=226, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 11:18:17 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 801 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 11:18:17 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/26 11:18:17 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.863 s
24/06/26 11:18:17 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 11:18:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/26 11:18:17 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.877808 s
24/06/26 11:18:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 2
-------------------------------------------
24/06/26 11:18:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 11:18:17 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
24/06/26 11:18:17 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor read 19 records through 1 polls (polled  out 19 records), taking 529007716 nanos, during time span of 555396169 nanos.
24/06/26 11:18:17 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 4964 bytes result sent to driver
24/06/26 11:18:17 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 784 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 11:18:17 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/06/26 11:18:17 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.804 s
24/06/26 11:18:17 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 11:18:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/06/26 11:18:17 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.894437 s
24/06/26 11:18:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3ddd1b0f] is committing.
24/06/26 11:18:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@3ddd1b0f] committed.
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
|     NULL|     NULL|2024/06/26 11:15:...|
+---------+---------+--------------------+

24/06/26 11:18:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/2 using temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/.2.3dfb139c-66d1-46ea-987b-72c232d75a66.tmp
24/06/26 11:18:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 11:18:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/2 using temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/.2.85eb5020-9a4a-43b8-a2ec-1ab53c4633b2.tmp
24/06/26 11:18:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/.2.3dfb139c-66d1-46ea-987b-72c232d75a66.tmp to file:/tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1/commits/2
24/06/26 11:18:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "7b0546ab-d66d-431f-81c7-27d2cead50a3",
  "runId" : "cfbab90f-3635-4b6d-81cb-ab8baa0dfc68",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T11:18:16.660Z",
  "batchId" : 2,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 1187.5,
  "processedRowsPerSecond" : 14.36130007558579,
  "durationMs" : {
    "addBatch" : 1034,
    "commitOffsets" : 113,
    "getBatch" : 2,
    "latestOffset" : 29,
    "queryPlanning" : 24,
    "triggerExecution" : 1323,
    "walCommit" : 118
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 207
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 1187.5,
    "processedRowsPerSecond" : 14.36130007558579,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 19
  }
}
24/06/26 11:18:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/.2.85eb5020-9a4a-43b8-a2ec-1ab53c4633b2.tmp to file:/tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091/commits/2
24/06/26 11:18:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "f9f67bb9-a075-4a62-b6ed-c3b01ffae271",
  "runId" : "e8109bcf-695a-4c11-83e8-768984452610",
  "name" : null,
  "timestamp" : "2024-06-26T11:18:16.664Z",
  "batchId" : 2,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 1000.0,
  "processedRowsPerSecond" : 14.318010550113037,
  "durationMs" : {
    "addBatch" : 1073,
    "commitOffsets" : 81,
    "getBatch" : 0,
    "latestOffset" : 28,
    "queryPlanning" : 16,
    "triggerExecution" : 1327,
    "walCommit" : 127
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 207
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 1000.0,
    "processedRowsPerSecond" : 14.318010550113037,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@5db5a3ec",
    "numOutputRows" : 19
  }
}
24/06/26 11:18:22 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:42769 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 11:18:22 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:42769 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 11:18:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:18:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:19:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:20:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:21:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:22:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 11:23:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3, groupId=spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 11:23:44 INFO Metrics: Metrics scheduler closed
24/06/26 11:23:44 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 11:23:44 INFO Metrics: Metrics reporters closed
24/06/26 11:23:44 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-f0c7788b-da95-4f32-83e1-591f0b055774--1469020163-executor-3 unregistered
24/06/26 11:23:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 11:23:44 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4, groupId=spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 11:23:44 INFO Metrics: Metrics scheduler closed
24/06/26 11:23:44 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 11:23:44 INFO Metrics: Metrics reporters closed
24/06/26 11:23:44 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-71a4bfe9-4967-4fb5-b41d-87d3aa12d9a6-1296775818-executor-4 unregistered
24/06/26 11:23:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:23:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 11:24:46] "GET /logs HTTP/1.1" 200 -
24/06/26 11:24:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:24:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
10.0.2.2 - - [26/Jun/2024 11:25:01] "GET / HTTP/1.1" 404 -
10.0.2.2 - - [26/Jun/2024 11:25:02] "GET / HTTP/1.1" 404 -
10.0.2.2 - - [26/Jun/2024 11:25:02] "GET /favicon.ico HTTP/1.1" 404 -
10.0.2.2 - - [26/Jun/2024 11:25:05] "GET /logs HTTP/1.1" 200 -
24/06/26 11:25:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:25:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
10.0.2.2 - - [26/Jun/2024 11:26:35] "GET /logs HTTP/1.1" 200 -
24/06/26 11:26:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:26:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:27:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:28:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:29:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:30:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:31:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:32:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:33:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:34:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:35:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:36:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:37:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:38:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:39:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:40:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:41:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:42:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:43:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:44:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:45:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:46:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:47:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:48:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:49:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:50:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:51:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:52:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:53:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:54:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:55:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:56:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:57:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:58:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 11:59:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:00:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:01:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:02:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:03:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:04:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:05:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:06:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:07:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:08:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:09:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:10:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:11:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:12:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:13:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:14:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:15:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:16:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:17:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:18:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:19:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:20:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:21:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:22:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:23:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:24:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:25:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:26:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:27:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:28:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:29:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:30:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:31:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:32:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:33:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:34:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:35:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:36:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:40:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:41:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:42:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:43:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:44:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:45:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:46:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:47:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:48:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:49:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:50:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:51:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:52:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:53:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:54:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:55:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:56:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:57:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:58:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 12:59:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:00:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:01:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:02:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:03:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:04:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:05:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:06:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:07:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:08:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:09:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:10:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:11:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:12:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:13:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:14:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:15:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:16:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:17:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:18:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:19:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:20:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:21:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:22:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:23:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:24:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:25:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:26:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:27:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:28:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:29:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:30:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:31:43 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 13:31:43 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 13:31:43 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 13:31:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 13:31:43 INFO MemoryStore: MemoryStore cleared
24/06/26 13:31:43 INFO BlockManager: BlockManager stopped
24/06/26 13:31:43 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 13:31:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 13:31:43 INFO SparkContext: Successfully stopped SparkContext
24/06/26 13:31:43 INFO ShutdownHookManager: Shutdown hook called
24/06/26 13:31:43 INFO ShutdownHookManager: Deleting directory /tmp/temporary-b40c2093-2192-4858-8f66-49ed827cb091
24/06/26 13:31:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-6aedf25c-bf7a-4f8c-8af2-27cd7e627187
24/06/26 13:31:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7
24/06/26 13:31:43 INFO ShutdownHookManager: Deleting directory /tmp/temporary-2268e39e-568a-473c-b0a7-2c1dfdc9c5a1
24/06/26 13:31:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-b5b1d05b-0374-4b51-8a5a-be8fa9fae1c7/pyspark-9de1c6aa-ebf4-4115-8cb6-d9aaaeaa71f0
Starting Spark at Wed Jun 26 01:33:18 PM UTC 2024
24/06/26 13:33:20 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 13:33:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-566b2c73-6649-4e64-b15d-364c825b3df2;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 589ms :: artifacts dl 13ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-566b2c73-6649-4e64-b15d-364c825b3df2
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/10ms)
24/06/26 13:33:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 13:33:23 INFO SparkContext: Running Spark version 3.5.1
24/06/26 13:33:23 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:33:23 INFO SparkContext: Java version 11.0.20.1
24/06/26 13:33:23 INFO ResourceUtils: ==============================================================
24/06/26 13:33:23 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 13:33:23 INFO ResourceUtils: ==============================================================
24/06/26 13:33:23 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 13:33:23 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 13:33:23 INFO ResourceProfile: Limiting resource is cpu
24/06/26 13:33:23 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 13:33:23 INFO SecurityManager: Changing view acls to: root
24/06/26 13:33:23 INFO SecurityManager: Changing modify acls to: root
24/06/26 13:33:23 INFO SecurityManager: Changing view acls groups to: 
24/06/26 13:33:23 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 13:33:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 13:33:23 INFO Utils: Successfully started service 'sparkDriver' on port 45985.
24/06/26 13:33:23 INFO SparkEnv: Registering MapOutputTracker
24/06/26 13:33:23 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 13:33:23 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 13:33:23 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 13:33:23 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 13:33:23 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a5029676-7227-4681-8d92-0627502a1a19
24/06/26 13:33:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 13:33:23 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 13:33:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 13:33:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:45985/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:45985/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:45985/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:45985/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:45985/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:45985/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:45985/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:45985/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:45985/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:45985/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:45985/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:33:24 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719408803215
24/06/26 13:33:24 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:33:24 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 13:33:24 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:33:24 INFO Executor: Java version 11.0.20.1
24/06/26 13:33:24 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 13:33:24 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5f314ded for default.
24/06/26 13:33:24 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:33:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:45985 after 46 ms (0 ms spent in bootstraps)
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp12213149911574119743.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp12213149911574119743.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp6055167682840274213.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp6055167682840274213.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp3088342172788077984.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp3088342172788077984.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp412166032309078332.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp412166032309078332.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp6171847015005702659.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp6171847015005702659.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp13267234107169645328.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp13267234107169645328.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp11174020361966570636.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp11174020361966570636.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp6708045609326566494.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp6708045609326566494.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp10696560810735572497.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp10696560810735572497.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp2688175818613096324.tmp
24/06/26 13:33:25 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp2688175818613096324.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:33:25 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 13:33:25 INFO Executor: Fetching spark://10.0.2.15:45985/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719408803215
24/06/26 13:33:25 INFO Utils: Fetching spark://10.0.2.15:45985/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp7239618977652501817.tmp
24/06/26 13:33:26 INFO Utils: /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/fetchFileTemp7239618977652501817.tmp has been previously copied to /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:33:26 INFO Executor: Adding file:/tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/userFiles-4fee8526-5f2d-45e3-9db2-68c394c1b81a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 13:33:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41269.
24/06/26 13:33:26 INFO NettyBlockTransferService: Server created on 10.0.2.15:41269
24/06/26 13:33:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 13:33:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 41269, None)
24/06/26 13:33:26 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:41269 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 41269, None)
24/06/26 13:33:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 41269, None)
24/06/26 13:33:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 41269, None)
24/06/26 13:33:26 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 13:33:26 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 13:33:29 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 13:33:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:33:29 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b resolved to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b.
24/06/26 13:33:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:33:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/metadata using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/.metadata.8a5df70b-f864-4c88-8982-df5ddc404640.tmp
24/06/26 13:33:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/.metadata.8a5df70b-f864-4c88-8982-df5ddc404640.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/metadata
24/06/26 13:33:29 INFO MicroBatchExecution: Starting logs_table [id = cf8019aa-9520-451e-9931-b1115d2bb252, runId = cfd4de51-2ffa-4266-b44f-0ef23d50984a]. Use file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b to store the query checkpoint.
24/06/26 13:33:29 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@530a9740] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@42891c8a]
24/06/26 13:33:29 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:33:29 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:33:29 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:33:29 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:33:29 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:33:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:33:29 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51 resolved to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51.
24/06/26 13:33:29 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:33:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/metadata using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/.metadata.5e14c554-36f8-4f4a-b0c2-4e7abca52ffa.tmp
24/06/26 13:33:30 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:33:30 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:33:30 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:33:30 INFO AppInfoParser: Kafka startTimeMs: 1719408810080
24/06/26 13:33:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/.metadata.5e14c554-36f8-4f4a-b0c2-4e7abca52ffa.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/metadata
24/06/26 13:33:30 INFO MicroBatchExecution: Starting [id = 8cc172d6-2225-4944-b207-39c1c74e7f55, runId = 3b3ba592-58f5-44c5-a31f-de062b72e611]. Use file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51 to store the query checkpoint.
24/06/26 13:33:30 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@530a9740] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@42891c8a]
24/06/26 13:33:30 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:33:30 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:33:30 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:33:30 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:33:30 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:33:30 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:33:30 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:33:30 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:33:30 INFO AppInfoParser: Kafka startTimeMs: 1719408810163
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 13:33:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/sources/0/0 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/sources/0/.0.0ce68d26-6d14-4440-ae52-368cac603dbe.tmp
24/06/26 13:33:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/sources/0/0 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/sources/0/.0.2899ab21-ecc9-4818-ba67-1d0335fa5ea7.tmp
24/06/26 13:33:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/sources/0/.0.0ce68d26-6d14-4440-ae52-368cac603dbe.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/sources/0/0
24/06/26 13:33:30 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":226}}
24/06/26 13:33:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/sources/0/.0.2899ab21-ecc9-4818-ba67-1d0335fa5ea7.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/sources/0/0
24/06/26 13:33:30 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":226}}
24/06/26 13:33:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/0 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.0.e58af15b-e1a7-4d22-8229-16ca75cf385e.tmp
24/06/26 13:33:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/0 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/.0.c19043ad-2344-446d-86eb-2d7088d046b3.tmp
24/06/26 13:33:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.0.e58af15b-e1a7-4d22-8229-16ca75cf385e.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/0
24/06/26 13:33:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/.0.c19043ad-2344-446d-86eb-2d7088d046b3.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/0
24/06/26 13:33:30 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719408810834,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:33:30 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719408810820,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:33:31 INFO CodeGenerator: Code generated in 299.180721 ms
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 13:33:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:33:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5140034a]. The input RDD has 1 partitions.
24/06/26 13:33:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:33:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:33:32 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:33:32 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:33:32 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:33:32 INFO DAGScheduler: Missing parents: List()
24/06/26 13:33:32 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:33:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 13:33:32 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 13:33:32 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:41269 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:33:32 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 13:33:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:33:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 13:33:32 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:33:32 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:33:32 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:33:32 INFO DAGScheduler: Missing parents: List()
24/06/26 13:33:32 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:33:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 13:33:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 13:33:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:41269 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:33:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 13:33:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:33:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 13:33:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:33:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:33:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 13:33:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 13:33:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:33:32 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 13:33:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:33:32 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 13:33:32 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1396 bytes result sent to driver
24/06/26 13:33:32 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1295 bytes result sent to driver
24/06/26 13:33:32 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 202 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:33:32 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 13:33:32 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 278 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:33:32 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 13:33:32 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.315 s
24/06/26 13:33:32 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:33:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 13:33:32 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.562 s
24/06/26 13:33:32 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:33:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 13:33:32 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.609137 s
24/06/26 13:33:32 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.613176 s
24/06/26 13:33:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/26 13:33:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5140034a] is committing.
24/06/26 13:33:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@5140034a] committed.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 13:33:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/0 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/.0.72059b72-26bd-4d6e-a27c-2a5d70d2686b.tmp
24/06/26 13:33:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/.0.72059b72-26bd-4d6e-a27c-2a5d70d2686b.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/0
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 13:33:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:33:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/0 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.0.51c4bd70-89c3-40db-8f6c-66ef3c165340.tmp
24/06/26 13:33:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "cf8019aa-9520-451e-9931-b1115d2bb252",
  "runId" : "cfd4de51-2ffa-4266-b44f-0ef23d50984a",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:33:29.457Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1423,
    "commitOffsets" : 103,
    "getBatch" : 71,
    "latestOffset" : 1355,
    "queryPlanning" : 240,
    "triggerExecution" : 3321,
    "walCommit" : 94
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 13:33:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.0.51c4bd70-89c3-40db-8f6c-66ef3c165340.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/0
24/06/26 13:33:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "8cc172d6-2225-4944-b207-39c1c74e7f55",
  "runId" : "3b3ba592-58f5-44c5-a31f-de062b72e611",
  "name" : null,
  "timestamp" : "2024-06-26T13:33:30.116Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1561,
    "commitOffsets" : 69,
    "getBatch" : 71,
    "latestOffset" : 699,
    "queryPlanning" : 240,
    "triggerExecution" : 2762,
    "walCommit" : 105
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2409235e",
    "numOutputRows" : 0
  }
}
24/06/26 13:33:38 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:41269 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:33:38 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:41269 in memory (size: 2035.0 B, free: 434.4 MiB)
127.0.0.1 - - [26/Jun/2024 13:33:42] "GET / HTTP/1.1" 404 -
24/06/26 13:33:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:33:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:33:49] "GET /logs HTTP/1.1" 200 -
24/06/26 13:33:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:33:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:34:50] "GET /logs HTTP/1.1" 200 -
24/06/26 13:34:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:34:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:35:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/1 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.1.fec87f07-5891-4cc8-b89f-779a02534bd6.tmp
24/06/26 13:36:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/1 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/.1.2d3b04b7-f1e1-4ff7-9dba-3fa6808ca19f.tmp
24/06/26 13:36:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.1.fec87f07-5891-4cc8-b89f-779a02534bd6.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/1
24/06/26 13:36:31 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719408991827,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:36:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/.1.2d3b04b7-f1e1-4ff7-9dba-3fa6808ca19f.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/1
24/06/26 13:36:31 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719408991836,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:31 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@f85f612]. The input RDD has 1 partitions.
24/06/26 13:36:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:36:32 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:36:32 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:36:32 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:36:32 INFO DAGScheduler: Missing parents: List()
24/06/26 13:36:32 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:36:32 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:36:32 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:36:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 13:36:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 13:36:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:41269 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 13:36:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 13:36:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:36:32 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 13:36:32 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:36:32 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:36:32 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:36:32 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 13:36:32 INFO DAGScheduler: Missing parents: List()
24/06/26 13:36:32 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:36:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 13:36:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 13:36:32 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 13:36:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:41269 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:36:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 13:36:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:36:32 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 13:36:32 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 13:36:32 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 13:36:32 INFO CodeGenerator: Code generated in 73.996297 ms
24/06/26 13:36:32 INFO CodeGenerator: Code generated in 16.576359 ms
24/06/26 13:36:32 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=226 untilOffset=245, for query queryId=cf8019aa-9520-451e-9931-b1115d2bb252 batchId=1 taskId=2 partitionId=0
24/06/26 13:36:32 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=226 untilOffset=227, for query queryId=8cc172d6-2225-4944-b207-39c1c74e7f55 batchId=1 taskId=3 partitionId=0
24/06/26 13:36:32 INFO CodeGenerator: Code generated in 18.464156 ms
24/06/26 13:36:32 INFO CodeGenerator: Code generated in 32.38125 ms
24/06/26 13:36:32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 13:36:32 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 13:36:32 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:36:32 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:36:32 INFO AppInfoParser: Kafka startTimeMs: 1719408992633
24/06/26 13:36:32 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:36:32 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:36:32 INFO AppInfoParser: Kafka startTimeMs: 1719408992633
24/06/26 13:36:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Assigned to partition(s): logs-0
24/06/26 13:36:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Assigned to partition(s): logs-0
24/06/26 13:36:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Seeking to offset 226 for partition logs-0
24/06/26 13:36:32 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Seeking to offset 226 for partition logs-0
24/06/26 13:36:32 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 13:36:32 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 13:36:32 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 13:36:32 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 13:36:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Seeking to earliest offset of partition logs-0
24/06/26 13:36:32 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Seeking to earliest offset of partition logs-0
24/06/26 13:36:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Seeking to latest offset of partition logs-0
24/06/26 13:36:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Seeking to latest offset of partition logs-0
24/06/26 13:36:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=245, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=245, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:33 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:36:33 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 13:36:33 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor read 1 records through 1 polls (polled  out 19 records), taking 596070209 nanos, during time span of 766324526 nanos.
24/06/26 13:36:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2242 bytes result sent to driver
24/06/26 13:36:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1270 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:36:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 13:36:33 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.294 s
24/06/26 13:36:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:36:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 13:36:33 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.371387 s
24/06/26 13:36:33 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 13:36:33 INFO CodeGenerator: Code generated in 7.58579 ms
24/06/26 13:36:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:41269 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:36:34 INFO CodeGenerator: Code generated in 15.660432 ms
24/06/26 13:36:34 INFO CodeGenerator: Code generated in 21.011804 ms
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|/docker-entrypoin...|
+---------+---------+--------------------+

24/06/26 13:36:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:36:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:36:34 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 13:36:34 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor read 19 records through 1 polls (polled  out 19 records), taking 596607751 nanos, during time span of 1975337339 nanos.
24/06/26 13:36:34 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4964 bytes result sent to driver
24/06/26 13:36:34 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2516 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:36:34 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 2.585 s
24/06/26 13:36:34 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:36:34 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 13:36:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 13:36:34 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 2.602815 s
24/06/26 13:36:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@f85f612] is committing.
24/06/26 13:36:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@f85f612] committed.
24/06/26 13:36:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/1 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.1.0887a421-e396-4a95-a8e8-3110503ee4fc.tmp
24/06/26 13:36:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/1 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/.1.49f0f867-e90f-4f17-9796-837f8ea29b89.tmp
24/06/26 13:36:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.1.0887a421-e396-4a95-a8e8-3110503ee4fc.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/1
24/06/26 13:36:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "8cc172d6-2225-4944-b207-39c1c74e7f55",
  "runId" : "3b3ba592-58f5-44c5-a31f-de062b72e611",
  "name" : null,
  "timestamp" : "2024-06-26T13:36:31.823Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 45.45454545454546,
  "processedRowsPerSecond" : 0.3394433129667345,
  "durationMs" : {
    "addBatch" : 2674,
    "commitOffsets" : 151,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 37,
    "triggerExecution" : 2946,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 227
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 227
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 45.45454545454546,
    "processedRowsPerSecond" : 0.3394433129667345,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2409235e",
    "numOutputRows" : 1
  }
}
24/06/26 13:36:34 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/2 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.2.debeab6e-a14b-41e0-9105-f02fe19044e1.tmp
24/06/26 13:36:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/.1.49f0f867-e90f-4f17-9796-837f8ea29b89.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/1
24/06/26 13:36:34 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "cf8019aa-9520-451e-9931-b1115d2bb252",
  "runId" : "cfd4de51-2ffa-4266-b44f-0ef23d50984a",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:36:31.824Z",
  "batchId" : 1,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 826.0869565217391,
  "processedRowsPerSecond" : 6.231551328304362,
  "durationMs" : {
    "addBatch" : 2707,
    "commitOffsets" : 221,
    "getBatch" : 0,
    "latestOffset" : 12,
    "queryPlanning" : 38,
    "triggerExecution" : 3049,
    "walCommit" : 70
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 226
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 245
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 245
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 826.0869565217391,
    "processedRowsPerSecond" : 6.231551328304362,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 19
  }
}
24/06/26 13:36:34 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.2.debeab6e-a14b-41e0-9105-f02fe19044e1.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/2
24/06/26 13:36:34 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719408994825,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:36:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:35 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:36:35 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:36:35 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:36:35 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:36:35 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:36:35 INFO DAGScheduler: Missing parents: List()
24/06/26 13:36:35 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:36:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 13:36:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 13:36:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:41269 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:36:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 13:36:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:36:35 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 13:36:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:41269 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 13:36:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 13:36:35 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 13:36:35 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=227 untilOffset=245, for query queryId=8cc172d6-2225-4944-b207-39c1c74e7f55 batchId=2 taskId=4 partitionId=0
24/06/26 13:36:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:36:35 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
24/06/26 13:36:35 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor read 18 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 12524578 nanos.
24/06/26 13:36:35 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4325 bytes result sent to driver
24/06/26 13:36:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 132 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:36:35 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 13:36:35 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.274 s
24/06/26 13:36:35 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:36:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/26 13:36:35 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.299665 s
24/06/26 13:36:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 2
-------------------------------------------
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
|     NULL|     NULL|2024/06/26 13:36:...|
+---------+---------+--------------------+

24/06/26 13:36:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:36:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/2 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.2.2aeaac5a-b1b4-4795-9444-cf9ea7b5d97d.tmp
24/06/26 13:36:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.2.2aeaac5a-b1b4-4795-9444-cf9ea7b5d97d.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/2
24/06/26 13:36:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "8cc172d6-2225-4944-b207-39c1c74e7f55",
  "runId" : "3b3ba592-58f5-44c5-a31f-de062b72e611",
  "name" : null,
  "timestamp" : "2024-06-26T13:36:34.818Z",
  "batchId" : 2,
  "numInputRows" : 18,
  "inputRowsPerSecond" : 6.010016694490818,
  "processedRowsPerSecond" : 27.397260273972602,
  "durationMs" : {
    "addBatch" : 458,
    "commitOffsets" : 72,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 33,
    "triggerExecution" : 657,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 227
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 245
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 245
      }
    },
    "numInputRows" : 18,
    "inputRowsPerSecond" : 6.010016694490818,
    "processedRowsPerSecond" : 27.397260273972602,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2409235e",
    "numOutputRows" : 18
  }
}
24/06/26 13:36:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/3 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.3.5a8a932c-ecf9-4fc7-bf36-13eaa13ce9e3.tmp
24/06/26 13:36:38 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/2 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/.2.e9c37c1e-97e3-4fa3-8453-337d5cb97e97.tmp
24/06/26 13:36:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/.2.e9c37c1e-97e3-4fa3-8453-337d5cb97e97.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/offsets/2
24/06/26 13:36:38 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719408998826,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:36:38 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/.3.5a8a932c-ecf9-4fc7-bf36-13eaa13ce9e3.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/offsets/3
24/06/26 13:36:38 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719408998827,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:36:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:41269 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:36:38 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52d8e302]. The input RDD has 1 partitions.
24/06/26 13:36:38 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:36:38 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:36:38 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:36:38 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:36:39 INFO DAGScheduler: Missing parents: List()
24/06/26 13:36:39 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:36:39 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 13:36:39 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 13:36:39 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:41269 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 13:36:39 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 13:36:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:36:39 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/26 13:36:39 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 13:36:39 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/26 13:36:39 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:36:39 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:36:39 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:36:39 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:36:39 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:36:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=245 untilOffset=246, for query queryId=cf8019aa-9520-451e-9931-b1115d2bb252 batchId=2 taskId=5 partitionId=0
24/06/26 13:36:39 INFO DAGScheduler: Missing parents: List()
24/06/26 13:36:39 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:36:39 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 13:36:39 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 13:36:39 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:41269 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:36:39 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
24/06/26 13:36:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:36:39 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/26 13:36:39 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 13:36:39 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
24/06/26 13:36:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Seeking to offset 245 for partition logs-0
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Seeking to earliest offset of partition logs-0
24/06/26 13:36:39 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=245 untilOffset=246, for query queryId=8cc172d6-2225-4944-b207-39c1c74e7f55 batchId=3 taskId=6 partitionId=0
24/06/26 13:36:39 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Seeking to offset 245 for partition logs-0
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Seeking to earliest offset of partition logs-0
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Seeking to latest offset of partition logs-0
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=246, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:39 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:36:39 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
24/06/26 13:36:39 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor read 1 records through 1 polls (polled  out 1 records), taking 516717452 nanos, during time span of 522100749 nanos.
24/06/26 13:36:39 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3229 bytes result sent to driver
24/06/26 13:36:39 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 573 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:36:39 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/26 13:36:39 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.597 s
24/06/26 13:36:39 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:36:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/26 13:36:39 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.613486 s
24/06/26 13:36:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52d8e302] is committing.
24/06/26 13:36:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@52d8e302] committed.
24/06/26 13:36:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/2 using temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/.2.528a8ff7-5d97-4000-9e65-24e68a1c8e6f.tmp
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Seeking to latest offset of partition logs-0
24/06/26 13:36:39 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=246, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:36:39 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:36:39 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
24/06/26 13:36:39 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor read 1 records through 1 polls (polled  out 1 records), taking 532198333 nanos, during time span of 535918442 nanos.
24/06/26 13:36:39 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2199 bytes result sent to driver
24/06/26 13:36:39 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 597 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:36:39 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/26 13:36:39 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.618 s
24/06/26 13:36:39 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:36:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/26 13:36:39 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.636865 s
24/06/26 13:36:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 3
-------------------------------------------
24/06/26 13:36:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/.2.528a8ff7-5d97-4000-9e65-24e68a1c8e6f.tmp to file:/tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b/commits/2
24/06/26 13:36:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "cf8019aa-9520-451e-9931-b1115d2bb252",
  "runId" : "cfd4de51-2ffa-4266-b44f-0ef23d50984a",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:36:38.822Z",
  "batchId" : 2,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 52.631578947368425,
  "processedRowsPerSecond" : 1.081081081081081,
  "durationMs" : {
    "addBatch" : 669,
    "commitOffsets" : 136,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 39,
    "triggerExecution" : 925,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 245
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 52.631578947368425,
    "processedRowsPerSecond" : 1.081081081081081,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|172.17.0.1 - - [2...|
+---------+---------+--------------------+

24/06/26 13:36:39 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:36:39 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/3 using temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.3.74e3e4be-4d7b-45f7-998b-36e1cf1febfc.tmp
24/06/26 13:36:39 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/.3.74e3e4be-4d7b-45f7-998b-36e1cf1febfc.tmp to file:/tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51/commits/3
24/06/26 13:36:39 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "8cc172d6-2225-4944-b207-39c1c74e7f55",
  "runId" : "3b3ba592-58f5-44c5-a31f-de062b72e611",
  "name" : null,
  "timestamp" : "2024-06-26T13:36:38.823Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 1.0141987829614605,
  "durationMs" : {
    "addBatch" : 819,
    "commitOffsets" : 53,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 37,
    "triggerExecution" : 986,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 245
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 1.0141987829614605,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2409235e",
    "numOutputRows" : 1
  }
}
24/06/26 13:36:41 INFO CodeGenerator: Code generated in 40.752281 ms
24/06/26 13:36:41 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:41269 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 13:36:41 INFO CodeGenerator: Code generated in 14.084805 ms
24/06/26 13:36:41 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:41269 in memory (size: 10.2 KiB, free: 434.4 MiB)
127.0.0.1 - - [26/Jun/2024 13:36:41] "GET /logs HTTP/1.1" 500 -
24/06/26 13:36:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
10.0.2.2 - - [26/Jun/2024 13:36:51] "GET /logs HTTP/1.1" 500 -
24/06/26 13:36:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:36:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
10.0.2.2 - - [26/Jun/2024 13:37:58] "GET /logs HTTP/1.1" 500 -
24/06/26 13:37:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:37:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:30 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 13:38:30 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 13:38:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:38:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:39:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:40:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:41:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:42:32 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 13:42:32 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1, groupId=spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 13:42:32 INFO Metrics: Metrics scheduler closed
24/06/26 13:42:32 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 13:42:32 INFO Metrics: Metrics reporters closed
24/06/26 13:42:32 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-9f79547f-4370-4248-846c-02929378a875-1166940001-executor-1 unregistered
24/06/26 13:42:32 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 13:42:32 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2, groupId=spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 13:42:32 INFO Metrics: Metrics scheduler closed
24/06/26 13:42:32 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 13:42:32 INFO Metrics: Metrics reporters closed
24/06/26 13:42:32 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-963b2655-db32-47ee-ad1a-3753a291798e-1604635561-executor-2 unregistered
24/06/26 13:42:35 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 13:42:35 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 13:42:35 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 13:42:35 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 13:42:35 INFO MemoryStore: MemoryStore cleared
24/06/26 13:42:35 INFO BlockManager: BlockManager stopped
24/06/26 13:42:35 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 13:42:35 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 13:42:35 INFO SparkContext: Successfully stopped SparkContext
24/06/26 13:42:35 INFO ShutdownHookManager: Shutdown hook called
24/06/26 13:42:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-b71e1209-2169-4499-938a-d495239bb14b
24/06/26 13:42:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c/pyspark-012eea3e-9349-4f25-a0fc-8e2292d6acd7
24/06/26 13:42:35 INFO ShutdownHookManager: Deleting directory /tmp/spark-85efaff9-f061-425c-95cf-a9f404d0ed7c
24/06/26 13:42:35 INFO ShutdownHookManager: Deleting directory /tmp/temporary-c2705c97-1474-4133-956f-0f1dac187c3b
24/06/26 13:42:35 INFO ShutdownHookManager: Deleting directory /tmp/temporary-aa7ac209-5f10-4b54-b364-d3f402db4e51
Starting Spark at Wed Jun 26 01:43:19 PM UTC 2024
24/06/26 13:43:21 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 13:43:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-5a077e39-66bf-4334-adcf-2042ef660d10;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 509ms :: artifacts dl 11ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-5a077e39-66bf-4334-adcf-2042ef660d10
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/15ms)
24/06/26 13:43:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 13:43:23 INFO SparkContext: Running Spark version 3.5.1
24/06/26 13:43:23 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:43:23 INFO SparkContext: Java version 11.0.20.1
24/06/26 13:43:23 INFO ResourceUtils: ==============================================================
24/06/26 13:43:23 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 13:43:23 INFO ResourceUtils: ==============================================================
24/06/26 13:43:23 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 13:43:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 13:43:24 INFO ResourceProfile: Limiting resource is cpu
24/06/26 13:43:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 13:43:24 INFO SecurityManager: Changing view acls to: root
24/06/26 13:43:24 INFO SecurityManager: Changing modify acls to: root
24/06/26 13:43:24 INFO SecurityManager: Changing view acls groups to: 
24/06/26 13:43:24 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 13:43:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 13:43:24 INFO Utils: Successfully started service 'sparkDriver' on port 36821.
24/06/26 13:43:24 INFO SparkEnv: Registering MapOutputTracker
24/06/26 13:43:24 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 13:43:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 13:43:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 13:43:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 13:43:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5aa27dbd-b003-4bdc-802d-c0708c2a9353
24/06/26 13:43:24 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 13:43:24 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 13:43:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 13:43:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:36821/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:36821/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:36821/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:36821/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:36821/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:36821/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:36821/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:36821/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:36821/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:36821/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:36821/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:43:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:43:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:43:24 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:43:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:43:24 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:24 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:43:25 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:43:25 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:43:25 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:43:25 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:43:25 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:43:25 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 13:43:25 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:43:25 INFO Executor: Java version 11.0.20.1
24/06/26 13:43:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 13:43:25 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7fe39fdf for default.
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:43:25 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:43:25 INFO Executor: Fetching spark://10.0.2.15:36821/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:36821 after 36 ms (0 ms spent in bootstraps)
24/06/26 13:43:25 INFO Utils: Fetching spark://10.0.2.15:36821/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp9161799496706988617.tmp
24/06/26 13:43:25 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp9161799496706988617.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:43:25 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 13:43:25 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp7410164400544740586.tmp
24/06/26 13:43:25 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp7410164400544740586.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:43:25 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:43:25 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409403942
24/06/26 13:43:25 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp15841674244441817662.tmp
24/06/26 13:43:25 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp15841674244441817662.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:43:26 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 13:43:26 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409403942
24/06/26 13:43:26 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp9120751471745170251.tmp
24/06/26 13:43:26 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp9120751471745170251.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:43:26 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 13:43:26 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409403942
24/06/26 13:43:26 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp14139065181666801674.tmp
24/06/26 13:43:26 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp14139065181666801674.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:43:26 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 13:43:26 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409403942
24/06/26 13:43:26 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp17437240149570501560.tmp
24/06/26 13:43:26 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp17437240149570501560.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:43:26 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:43:26 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409403942
24/06/26 13:43:26 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp14314533648199496475.tmp
24/06/26 13:43:26 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp14314533648199496475.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:43:26 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 13:43:26 INFO Executor: Fetching spark://10.0.2.15:36821/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409403942
24/06/26 13:43:26 INFO Utils: Fetching spark://10.0.2.15:36821/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp9604359372880137347.tmp
24/06/26 13:43:26 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp9604359372880137347.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:43:26 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 13:43:26 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:26 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp7899005096179614806.tmp
24/06/26 13:43:26 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp7899005096179614806.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:43:27 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 13:43:27 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409403942
24/06/26 13:43:27 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp369338115238645479.tmp
24/06/26 13:43:27 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp369338115238645479.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:43:27 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 13:43:27 INFO Executor: Fetching spark://10.0.2.15:36821/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409403942
24/06/26 13:43:27 INFO Utils: Fetching spark://10.0.2.15:36821/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp17704101096193789352.tmp
24/06/26 13:43:27 INFO Utils: /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/fetchFileTemp17704101096193789352.tmp has been previously copied to /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:43:27 INFO Executor: Adding file:/tmp/spark-70e25811-045e-410c-96e6-a81279eac206/userFiles-4d13f931-956f-4c65-8594-fd598f5bae7d/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 13:43:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40041.
24/06/26 13:43:27 INFO NettyBlockTransferService: Server created on 10.0.2.15:40041
24/06/26 13:43:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 13:43:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 40041, None)
24/06/26 13:43:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:40041 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 40041, None)
24/06/26 13:43:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 40041, None)
24/06/26 13:43:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 40041, None)
24/06/26 13:43:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 13:43:28 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 13:43:31 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 13:43:31 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:43:31 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec resolved to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec.
24/06/26 13:43:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:43:31 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/metadata using temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/.metadata.a8944f1a-b221-4420-bf26-316c92e14a8f.tmp
24/06/26 13:43:31 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/.metadata.a8944f1a-b221-4420-bf26-316c92e14a8f.tmp to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/metadata
24/06/26 13:43:31 INFO MicroBatchExecution: Starting logs_table [id = 93e8c8cc-8a18-4465-9e40-4fae74afdb94, runId = 381fb3a5-3eac-4a08-a9b0-ab5cb930ef57]. Use file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec to store the query checkpoint.
24/06/26 13:43:31 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2ab43c41] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@30d01d76]
24/06/26 13:43:31 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:43:31 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:43:31 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:43:31 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:43:32 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:43:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:43:32 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8 resolved to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8.
24/06/26 13:43:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:43:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/metadata using temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/.metadata.ae240841-8d33-4e3a-b61b-2b902d5c751e.tmp
24/06/26 13:43:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:43:32 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:43:32 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:43:32 INFO AppInfoParser: Kafka startTimeMs: 1719409412609
24/06/26 13:43:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/.metadata.ae240841-8d33-4e3a-b61b-2b902d5c751e.tmp to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/metadata
24/06/26 13:43:32 INFO MicroBatchExecution: Starting [id = c3d7a5d0-7cd3-48af-8292-666b179d3f61, runId = f55cb777-94a8-4349-b74c-423e51d44140]. Use file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8 to store the query checkpoint.
24/06/26 13:43:32 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@2ab43c41] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@30d01d76]
24/06/26 13:43:32 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:43:32 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:43:32 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:43:32 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:43:32 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:43:32 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:43:32 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:43:32 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:43:32 INFO AppInfoParser: Kafka startTimeMs: 1719409412758
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 13:43:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/sources/0/0 using temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/sources/0/.0.d676b61c-a3de-40e8-a534-513828e8746d.tmp
24/06/26 13:43:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/sources/0/0 using temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/sources/0/.0.f821e908-93e0-400c-a555-ad230a3ba1ef.tmp
24/06/26 13:43:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/sources/0/.0.d676b61c-a3de-40e8-a534-513828e8746d.tmp to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/sources/0/0
24/06/26 13:43:33 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":246}}
24/06/26 13:43:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/sources/0/.0.f821e908-93e0-400c-a555-ad230a3ba1ef.tmp to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/sources/0/0
24/06/26 13:43:33 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":246}}
24/06/26 13:43:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/0 using temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/.0.d600674b-0408-4790-a654-d57747b50de8.tmp
24/06/26 13:43:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/0 using temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/.0.07cc988b-b54c-4e8c-99ce-d730fed32ced.tmp
24/06/26 13:43:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/.0.d600674b-0408-4790-a654-d57747b50de8.tmp to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/0
24/06/26 13:43:33 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719409413399,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:43:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/.0.07cc988b-b54c-4e8c-99ce-d730fed32ced.tmp to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/0
24/06/26 13:43:33 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719409413399,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:34 INFO CodeGenerator: Code generated in 487.484241 ms
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 13:43:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@708ad97f]. The input RDD has 1 partitions.
24/06/26 13:43:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:43:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:43:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:43:34 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:43:34 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:43:34 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:43:34 INFO DAGScheduler: Missing parents: List()
24/06/26 13:43:34 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:43:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 13:43:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 13:43:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:40041 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:43:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 13:43:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:43:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 13:43:35 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:43:35 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:43:35 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:43:35 INFO DAGScheduler: Missing parents: List()
24/06/26 13:43:35 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:43:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 13:43:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 13:43:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:40041 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:43:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 13:43:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:43:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 13:43:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:43:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:43:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 13:43:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 13:43:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:43:35 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 13:43:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:43:35 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 13:43:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 13:43:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 13:43:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 244 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:43:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 13:43:35 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.526 s
24/06/26 13:43:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 203 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:43:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 13:43:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:43:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 13:43:35 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.584940 s
24/06/26 13:43:35 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.304 s
24/06/26 13:43:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:43:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/26 13:43:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 13:43:35 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.589675 s
24/06/26 13:43:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@708ad97f] is committing.
-------------------------------------------
Batch: 0
24/06/26 13:43:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@708ad97f] committed.
-------------------------------------------
24/06/26 13:43:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/0 using temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/.0.956c3262-e733-40e6-8e62-7006018263ef.tmp
+----+------+---+
|time|stream|log|
+----+------+---+
+----+------+---+

24/06/26 13:43:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:43:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/0 using temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/.0.cbb81fb1-7678-493b-8c3d-bbb6fc83743e.tmp
24/06/26 13:43:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/.0.cbb81fb1-7678-493b-8c3d-bbb6fc83743e.tmp to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/0
24/06/26 13:43:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/.0.956c3262-e733-40e6-8e62-7006018263ef.tmp to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/0
24/06/26 13:43:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "93e8c8cc-8a18-4465-9e40-4fae74afdb94",
  "runId" : "381fb3a5-3eac-4a08-a9b0-ab5cb930ef57",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:43:31.756Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1653,
    "commitOffsets" : 202,
    "getBatch" : 27,
    "latestOffset" : 1611,
    "queryPlanning" : 266,
    "triggerExecution" : 3884,
    "walCommit" : 84
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 13:43:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c3d7a5d0-7cd3-48af-8292-666b179d3f61",
  "runId" : "f55cb777-94a8-4349-b74c-423e51d44140",
  "name" : null,
  "timestamp" : "2024-06-26T13:43:32.717Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1731,
    "commitOffsets" : 104,
    "getBatch" : 4,
    "latestOffset" : 677,
    "queryPlanning" : 231,
    "triggerExecution" : 2903,
    "walCommit" : 142
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2d151ae5",
    "numOutputRows" : 0
  }
}
127.0.0.1 - - [26/Jun/2024 13:43:36] "GET / HTTP/1.1" 404 -
24/06/26 13:43:37 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:40041 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:43:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:40041 in memory (size: 2.4 KiB, free: 434.4 MiB)
127.0.0.1 - - [26/Jun/2024 13:43:40] "GET /logs HTTP/1.1" 200 -
24/06/26 13:43:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:43:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:43:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/1 using temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/.1.bae0f98b-6ce2-4bc3-9b72-843b3e4f2e40.tmp
24/06/26 13:43:51 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/1 using temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/.1.8194e623-505f-44af-b84a-abf352caa5ac.tmp
24/06/26 13:43:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/.1.bae0f98b-6ce2-4bc3-9b72-843b3e4f2e40.tmp to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/offsets/1
24/06/26 13:43:52 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719409431937,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:43:52 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/.1.8194e623-505f-44af-b84a-abf352caa5ac.tmp to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/offsets/1
24/06/26 13:43:52 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719409431937,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:43:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:43:52 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@20607f21]. The input RDD has 1 partitions.
24/06/26 13:43:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:43:52 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:43:52 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:43:52 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:43:52 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:43:52 INFO DAGScheduler: Missing parents: List()
24/06/26 13:43:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:43:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.4 KiB, free 434.4 MiB)
24/06/26 13:43:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.4 MiB)
24/06/26 13:43:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:40041 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:43:52 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 13:43:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:43:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 13:43:52 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:43:52 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:43:52 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:43:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/26 13:43:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 13:43:52 INFO DAGScheduler: Missing parents: List()
24/06/26 13:43:52 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:43:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/26 13:43:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/26 13:43:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:40041 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 13:43:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 13:43:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:43:52 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 13:43:52 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 13:43:52 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 13:43:52 INFO CodeGenerator: Code generated in 38.132811 ms
24/06/26 13:43:52 INFO CodeGenerator: Code generated in 14.270749 ms
24/06/26 13:43:52 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=246 untilOffset=247, for query queryId=c3d7a5d0-7cd3-48af-8292-666b179d3f61 batchId=1 taskId=2 partitionId=0
24/06/26 13:43:52 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=246 untilOffset=247, for query queryId=93e8c8cc-8a18-4465-9e40-4fae74afdb94 batchId=1 taskId=3 partitionId=0
24/06/26 13:43:52 INFO CodeGenerator: Code generated in 27.423785 ms
24/06/26 13:43:52 INFO CodeGenerator: Code generated in 40.715963 ms
24/06/26 13:43:52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 13:43:52 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 13:43:52 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:43:52 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:43:52 INFO AppInfoParser: Kafka startTimeMs: 1719409432550
24/06/26 13:43:52 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:43:52 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:43:52 INFO AppInfoParser: Kafka startTimeMs: 1719409432551
24/06/26 13:43:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Assigned to partition(s): logs-0
24/06/26 13:43:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Assigned to partition(s): logs-0
24/06/26 13:43:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Seeking to offset 246 for partition logs-0
24/06/26 13:43:52 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Seeking to offset 246 for partition logs-0
24/06/26 13:43:52 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 13:43:52 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 13:43:52 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 13:43:52 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 13:43:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Seeking to earliest offset of partition logs-0
24/06/26 13:43:52 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Seeking to earliest offset of partition logs-0
24/06/26 13:43:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:43:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Seeking to latest offset of partition logs-0
24/06/26 13:43:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:43:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Seeking to latest offset of partition logs-0
24/06/26 13:43:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=247, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:43:53 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=247, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 13:43:53 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:43:53 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 13:43:53 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor read 1 records through 1 polls (polled  out 1 records), taking 608265616 nanos, during time span of 787405709 nanos.
24/06/26 13:43:53 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2311 bytes result sent to driver
24/06/26 13:43:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1212 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:43:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 13:43:53 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.262 s
24/06/26 13:43:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:43:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 13:43:53 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.276017 s
24/06/26 13:43:53 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 13:43:53 INFO CodeGenerator: Code generated in 9.446546 ms
24/06/26 13:43:53 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:40041 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 13:43:54 INFO CodeGenerator: Code generated in 12.997922 ms
24/06/26 13:43:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:43:54 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 13:43:54 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor read 1 records through 1 polls (polled  out 1 records), taking 610212356 nanos, during time span of 2018733100 nanos.
24/06/26 13:43:54 INFO CodeGenerator: Code generated in 22.352837 ms
24/06/26 13:43:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3334 bytes result sent to driver
+----+------+--------------------+
|time|stream|                 log|
+----+------+--------------------+
|NULL|stdout|{"offset":2868,"f...|
+----+------+--------------------+

24/06/26 13:43:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:43:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2413 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:43:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 13:43:54 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.442 s
24/06/26 13:43:54 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:43:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 13:43:54 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.518860 s
24/06/26 13:43:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@20607f21] is committing.
24/06/26 13:43:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@20607f21] committed.
24/06/26 13:43:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/1 using temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/.1.22bafbd3-3754-4c46-a23b-b0a47bf92e3f.tmp
24/06/26 13:43:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/1 using temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/.1.0786c1d3-c00c-4116-bc7f-d70f763b5d4d.tmp
24/06/26 13:43:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/.1.22bafbd3-3754-4c46-a23b-b0a47bf92e3f.tmp to file:/tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8/commits/1
24/06/26 13:43:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/.1.0786c1d3-c00c-4116-bc7f-d70f763b5d4d.tmp to file:/tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec/commits/1
24/06/26 13:43:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c3d7a5d0-7cd3-48af-8292-666b179d3f61",
  "runId" : "f55cb777-94a8-4349-b74c-423e51d44140",
  "name" : null,
  "timestamp" : "2024-06-26T13:43:51.931Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 32.25806451612903,
  "processedRowsPerSecond" : 0.362844702467344,
  "durationMs" : {
    "addBatch" : 2557,
    "commitOffsets" : 88,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 29,
    "triggerExecution" : 2756,
    "walCommit" : 75
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 32.25806451612903,
    "processedRowsPerSecond" : 0.362844702467344,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@2d151ae5",
    "numOutputRows" : 1
  }
}
24/06/26 13:43:54 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "93e8c8cc-8a18-4465-9e40-4fae74afdb94",
  "runId" : "381fb3a5-3eac-4a08-a9b0-ab5cb930ef57",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:43:51.930Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 38.46153846153846,
  "processedRowsPerSecond" : 0.3623188405797102,
  "durationMs" : {
    "addBatch" : 2580,
    "commitOffsets" : 74,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 28,
    "triggerExecution" : 2760,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 246
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 38.46153846153846,
    "processedRowsPerSecond" : 0.3623188405797102,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 13:44:00 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:40041 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 13:44:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:16 INFO CodeGenerator: Code generated in 67.526977 ms
24/06/26 13:44:16 INFO CodeGenerator: Code generated in 8.090322 ms
127.0.0.1 - - [26/Jun/2024 13:44:16] "GET /logs HTTP/1.1" 200 -
24/06/26 13:44:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 63.069142 ms
24/06/26 13:44:51 INFO DAGScheduler: Registering RDD 20 (collect at /vagrant_data/spark_kafka_consumer.py:106) as input to shuffle 0
24/06/26 13:44:51 INFO DAGScheduler: Got map stage job 4 (collect at /vagrant_data/spark_kafka_consumer.py:106) with 1 output partitions
24/06/26 13:44:51 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (collect at /vagrant_data/spark_kafka_consumer.py:106)
24/06/26 13:44:51 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:44:51 INFO DAGScheduler: Missing parents: List()
24/06/26 13:44:51 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at collect at /vagrant_data/spark_kafka_consumer.py:106), which has no missing parents
24/06/26 13:44:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 31.9 KiB, free 434.4 MiB)
24/06/26 13:44:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.6 KiB, free 434.4 MiB)
24/06/26 13:44:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:40041 (size: 14.6 KiB, free: 434.4 MiB)
24/06/26 13:44:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 13:44:51 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at collect at /vagrant_data/spark_kafka_consumer.py:106) (first 15 tasks are for partitions Vector(0))
24/06/26 13:44:51 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 13:44:51 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10016 bytes) 
24/06/26 13:44:51 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 72.757431 ms
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 12.185891 ms
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 6.55857 ms
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 6.209044 ms
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 7.298159 ms
24/06/26 13:44:51 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2488 bytes result sent to driver
24/06/26 13:44:51 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 235 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:44:51 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 13:44:51 INFO DAGScheduler: ShuffleMapStage 4 (collect at /vagrant_data/spark_kafka_consumer.py:106) finished in 0.283 s
24/06/26 13:44:51 INFO DAGScheduler: looking for newly runnable stages
24/06/26 13:44:51 INFO DAGScheduler: running: Set()
24/06/26 13:44:51 INFO DAGScheduler: waiting: Set()
24/06/26 13:44:51 INFO DAGScheduler: failed: Set()
24/06/26 13:44:51 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/06/26 13:44:51 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 24.277256 ms
24/06/26 13:44:51 INFO SparkContext: Starting job: collect at /vagrant_data/spark_kafka_consumer.py:106
24/06/26 13:44:51 INFO DAGScheduler: Got job 5 (collect at /vagrant_data/spark_kafka_consumer.py:106) with 1 output partitions
24/06/26 13:44:51 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /vagrant_data/spark_kafka_consumer.py:106)
24/06/26 13:44:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
24/06/26 13:44:51 INFO DAGScheduler: Missing parents: List()
24/06/26 13:44:51 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at collect at /vagrant_data/spark_kafka_consumer.py:106), which has no missing parents
24/06/26 13:44:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 33.6 KiB, free 434.3 MiB)
24/06/26 13:44:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 434.3 MiB)
24/06/26 13:44:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:40041 (size: 15.5 KiB, free: 434.4 MiB)
24/06/26 13:44:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 13:44:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at collect at /vagrant_data/spark_kafka_consumer.py:106) (first 15 tasks are for partitions Vector(0))
24/06/26 13:44:51 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/26 13:44:51 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 13:44:51 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
24/06/26 13:44:51 INFO ShuffleBlockFetcherIterator: Getting 1 (72.0 B) non-empty blocks including 1 (72.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 13:44:51 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms
24/06/26 13:44:51 INFO CodeGenerator: Code generated in 17.465349 ms
24/06/26 13:44:51 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 4939 bytes result sent to driver
24/06/26 13:44:51 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 113 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:44:51 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/26 13:44:51 INFO DAGScheduler: ResultStage 6 (collect at /vagrant_data/spark_kafka_consumer.py:106) finished in 0.137 s
24/06/26 13:44:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:44:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/26 13:44:51 INFO DAGScheduler: Job 5 finished: collect at /vagrant_data/spark_kafka_consumer.py:106, took 0.161664 s
127.0.0.1 - - [26/Jun/2024 13:44:51] "GET /logs/stats HTTP/1.1" 500 -
24/06/26 13:44:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:44:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:00 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:40041 in memory (size: 14.6 KiB, free: 434.4 MiB)
24/06/26 13:45:00 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:40041 in memory (size: 15.5 KiB, free: 434.4 MiB)
24/06/26 13:45:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:45:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:46:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:47:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:48:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:48:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:48:05 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 13:48:05 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2, groupId=spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 13:48:05 INFO Metrics: Metrics scheduler closed
24/06/26 13:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 13:48:05 INFO Metrics: Metrics reporters closed
24/06/26 13:48:05 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d7197a8e-acb0-4fd2-bd68-b5220da506a6--1038576031-executor-2 unregistered
24/06/26 13:48:05 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 13:48:05 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1, groupId=spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 13:48:05 INFO Metrics: Metrics scheduler closed
24/06/26 13:48:05 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 13:48:05 INFO Metrics: Metrics reporters closed
24/06/26 13:48:05 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-5afc31fe-8238-4a54-a856-ef240780c236-1204388709-executor-1 unregistered
24/06/26 13:48:05 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 13:48:05 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 13:48:05 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 13:48:05 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 13:48:06 INFO MemoryStore: MemoryStore cleared
24/06/26 13:48:06 INFO BlockManager: BlockManager stopped
24/06/26 13:48:06 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 13:48:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 13:48:06 INFO SparkContext: Successfully stopped SparkContext
24/06/26 13:48:06 INFO ShutdownHookManager: Shutdown hook called
24/06/26 13:48:06 INFO ShutdownHookManager: Deleting directory /tmp/temporary-bc172628-2c4f-410b-adaf-6b651978f5ec
24/06/26 13:48:06 INFO ShutdownHookManager: Deleting directory /tmp/temporary-09800b8a-0ff4-4c8e-bb54-a11217eeb4d8
24/06/26 13:48:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-70e25811-045e-410c-96e6-a81279eac206/pyspark-33267d7e-e8f9-4b60-bbe9-d6c6b91d9b13
24/06/26 13:48:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-70e25811-045e-410c-96e6-a81279eac206
24/06/26 13:48:06 INFO ShutdownHookManager: Deleting directory /tmp/spark-2210cf90-42a2-49a1-ba5b-5bffbd209cbc
Starting Spark at Wed Jun 26 01:48:21 PM UTC 2024
24/06/26 13:48:24 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 13:48:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-94cff073-a0af-4fba-9eea-39b4ef9b9d77;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 567ms :: artifacts dl 12ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-94cff073-a0af-4fba-9eea-39b4ef9b9d77
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/9ms)
24/06/26 13:48:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 13:48:26 INFO SparkContext: Running Spark version 3.5.1
24/06/26 13:48:26 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:48:26 INFO SparkContext: Java version 11.0.20.1
24/06/26 13:48:26 INFO ResourceUtils: ==============================================================
24/06/26 13:48:26 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 13:48:26 INFO ResourceUtils: ==============================================================
24/06/26 13:48:26 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 13:48:26 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 13:48:26 INFO ResourceProfile: Limiting resource is cpu
24/06/26 13:48:26 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 13:48:26 INFO SecurityManager: Changing view acls to: root
24/06/26 13:48:26 INFO SecurityManager: Changing modify acls to: root
24/06/26 13:48:26 INFO SecurityManager: Changing view acls groups to: 
24/06/26 13:48:26 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 13:48:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 13:48:27 INFO Utils: Successfully started service 'sparkDriver' on port 35605.
24/06/26 13:48:27 INFO SparkEnv: Registering MapOutputTracker
24/06/26 13:48:27 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 13:48:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 13:48:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 13:48:27 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 13:48:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-47273541-07c9-47ac-8959-7344c63f519c
24/06/26 13:48:27 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 13:48:27 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 13:48:27 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 13:48:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:35605/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:35605/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:35605/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:35605/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:35605/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:35605/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:35605/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:35605/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:35605/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:35605/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:35605/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:48:27 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409706750
24/06/26 13:48:27 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:48:28 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 13:48:28 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:48:28 INFO Executor: Java version 11.0.20.1
24/06/26 13:48:28 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 13:48:28 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@c60feea for default.
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:48:28 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:48:28 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:35605 after 47 ms (0 ms spent in bootstraps)
24/06/26 13:48:28 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp9026860735200834297.tmp
24/06/26 13:48:28 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp9026860735200834297.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:48:28 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 13:48:28 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp14684206243977659037.tmp
24/06/26 13:48:28 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp14684206243977659037.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:48:28 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 13:48:28 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:28 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp5341548102842973543.tmp
24/06/26 13:48:28 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp5341548102842973543.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp6572386298198555282.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp6572386298198555282.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp4373625335275148586.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp4373625335275148586.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp12030377149798757189.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp12030377149798757189.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp12131346879915054210.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp12131346879915054210.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp12176761384414339875.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp12176761384414339875.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp2141502135183546415.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp2141502135183546415.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp8287398190100274950.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp8287398190100274950.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:48:29 INFO Executor: Fetching spark://10.0.2.15:35605/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719409706750
24/06/26 13:48:29 INFO Utils: Fetching spark://10.0.2.15:35605/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp16911881457830806387.tmp
24/06/26 13:48:29 INFO Utils: /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/fetchFileTemp16911881457830806387.tmp has been previously copied to /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:48:29 INFO Executor: Adding file:/tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/userFiles-a306cd87-d902-4864-8eca-ecbe74b63b7a/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 13:48:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39343.
24/06/26 13:48:29 INFO NettyBlockTransferService: Server created on 10.0.2.15:39343
24/06/26 13:48:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 13:48:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 39343, None)
24/06/26 13:48:29 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:39343 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 39343, None)
24/06/26 13:48:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 39343, None)
24/06/26 13:48:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 39343, None)
24/06/26 13:48:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 13:48:30 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 13:48:32 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 13:48:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:48:32 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca resolved to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca.
24/06/26 13:48:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:48:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/metadata using temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/.metadata.fb8067de-b28a-4dda-8b5b-1bbd1f35a631.tmp
24/06/26 13:48:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/.metadata.fb8067de-b28a-4dda-8b5b-1bbd1f35a631.tmp to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/metadata
24/06/26 13:48:32 INFO MicroBatchExecution: Starting logs_table [id = ca200421-d8d7-4818-b11d-c503f74dfd98, runId = aefca199-5190-4012-ab24-d6bb4e7a40be]. Use file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca to store the query checkpoint.
24/06/26 13:48:32 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5e5c15f9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@22516c25]
24/06/26 13:48:32 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:48:32 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:48:32 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:48:32 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:48:33 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:48:33 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:48:33 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15 resolved to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15.
24/06/26 13:48:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:48:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/metadata using temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/.metadata.92e3b0d1-5228-4bd7-b149-54b1f390a9eb.tmp
24/06/26 13:48:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/.metadata.92e3b0d1-5228-4bd7-b149-54b1f390a9eb.tmp to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/metadata
24/06/26 13:48:33 INFO MicroBatchExecution: Starting [id = 75b14c82-b133-4034-a752-eb2ebe91d4f1, runId = 0df87843-b32b-4bee-84ab-bc80a0566765]. Use file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15 to store the query checkpoint.
24/06/26 13:48:33 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5e5c15f9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@22516c25]
24/06/26 13:48:33 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:48:33 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:48:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:48:33 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:48:33 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:48:33 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:48:33 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:48:33 INFO AppInfoParser: Kafka startTimeMs: 1719409713263
24/06/26 13:48:33 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:48:33 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:48:33 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:48:33 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:48:33 INFO AppInfoParser: Kafka startTimeMs: 1719409713305
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 13:48:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/sources/0/0 using temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/sources/0/.0.d02257ea-a3d6-430f-8726-36cd13228147.tmp
24/06/26 13:48:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/sources/0/0 using temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/sources/0/.0.f592d170-f44f-483c-bebf-199af4f43a36.tmp
24/06/26 13:48:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/sources/0/.0.d02257ea-a3d6-430f-8726-36cd13228147.tmp to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/sources/0/0
24/06/26 13:48:33 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":247}}
24/06/26 13:48:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/sources/0/.0.f592d170-f44f-483c-bebf-199af4f43a36.tmp to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/sources/0/0
24/06/26 13:48:33 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":247}}
24/06/26 13:48:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/offsets/0 using temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/offsets/.0.0f6ccf4f-3483-434e-ad3c-b78c10b66541.tmp
24/06/26 13:48:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/offsets/0 using temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/offsets/.0.323dc17c-81c4-4bbc-869e-0f513efa9a00.tmp
24/06/26 13:48:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/offsets/.0.323dc17c-81c4-4bbc-869e-0f513efa9a00.tmp to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/offsets/0
24/06/26 13:48:33 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719409713870,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:48:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/offsets/.0.0f6ccf4f-3483-434e-ad3c-b78c10b66541.tmp to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/offsets/0
24/06/26 13:48:33 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719409713883,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:48:34 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 13:48:34 INFO CodeGenerator: Code generated in 267.447103 ms
24/06/26 13:48:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:48:34 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a08381c]. The input RDD has 1 partitions.
24/06/26 13:48:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:48:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:48:34 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:48:34 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:48:34 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:48:34 INFO DAGScheduler: Missing parents: List()
24/06/26 13:48:34 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:48:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 13:48:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 13:48:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:39343 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:48:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 13:48:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:48:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 13:48:35 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:48:35 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:48:35 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:48:35 INFO DAGScheduler: Missing parents: List()
24/06/26 13:48:35 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:48:35 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.3 KiB, free 434.4 MiB)
24/06/26 13:48:35 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 13:48:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:39343 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:48:35 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 13:48:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:48:35 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 13:48:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:48:35 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:48:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 13:48:35 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 13:48:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:48:35 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 13:48:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:48:35 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 13:48:35 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 13:48:35 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 13:48:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 153 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:48:35 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 13:48:35 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 219 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:48:35 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 13:48:35 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.228 s
24/06/26 13:48:35 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:48:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 13:48:35 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.460754 s
24/06/26 13:48:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a08381c] is committing.
24/06/26 13:48:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a08381c] committed.
24/06/26 13:48:35 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.438 s
24/06/26 13:48:35 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:48:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 13:48:35 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.483814 s
24/06/26 13:48:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 13:48:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/commits/0 using temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/commits/.0.91a0b676-c659-4e5b-8699-aee10c078aea.tmp
+----+------+---+------+----+
|time|stream|log|offset|file|
+----+------+---+------+----+
+----+------+---+------+----+

24/06/26 13:48:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:48:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/commits/.0.91a0b676-c659-4e5b-8699-aee10c078aea.tmp to file:/tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca/commits/0
24/06/26 13:48:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/commits/0 using temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/commits/.0.600321ba-4363-4a0c-98fd-27e5b58b353c.tmp
24/06/26 13:48:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ca200421-d8d7-4818-b11d-c503f74dfd98",
  "runId" : "aefca199-5190-4012-ab24-d6bb4e7a40be",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:48:32.629Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1168,
    "commitOffsets" : 120,
    "getBatch" : 16,
    "latestOffset" : 1234,
    "queryPlanning" : 240,
    "triggerExecution" : 2919,
    "walCommit" : 109
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 13:48:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/commits/.0.600321ba-4363-4a0c-98fd-27e5b58b353c.tmp to file:/tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15/commits/0
24/06/26 13:48:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "75b14c82-b133-4034-a752-eb2ebe91d4f1",
  "runId" : "0df87843-b32b-4bee-84ab-bc80a0566765",
  "name" : null,
  "timestamp" : "2024-06-26T13:48:33.259Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1286,
    "commitOffsets" : 80,
    "getBatch" : 26,
    "latestOffset" : 603,
    "queryPlanning" : 242,
    "triggerExecution" : 2367,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@5d52ac82",
    "numOutputRows" : 0
  }
}
24/06/26 13:48:39 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:39343 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:48:39 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:39343 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:48:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:48:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:48:45] "GET / HTTP/1.1" 404 -
24/06/26 13:48:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:48:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:48:58] "GET /logs HTTP/1.1" 200 -
24/06/26 13:49:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:49:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:50:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:51:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:52:29] "GET /logs HTTP/1.1" 200 -
24/06/26 13:52:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:52:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:33 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 13:53:33 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 13:53:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:53:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:54:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:31 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 13:55:31 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 13:55:31 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 13:55:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 13:55:31 INFO MemoryStore: MemoryStore cleared
24/06/26 13:55:31 INFO BlockManager: BlockManager stopped
24/06/26 13:55:31 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 13:55:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 13:55:31 INFO SparkContext: Successfully stopped SparkContext
24/06/26 13:55:31 INFO ShutdownHookManager: Shutdown hook called
24/06/26 13:55:31 INFO ShutdownHookManager: Deleting directory /tmp/temporary-7d28d6cf-a106-421d-84c4-a1c1bb832f15
24/06/26 13:55:31 INFO ShutdownHookManager: Deleting directory /tmp/temporary-778104cb-b1ac-43d9-ab41-842a5ed221ca
24/06/26 13:55:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8/pyspark-1d6792ac-4288-43b3-9e07-4375fefd4d27
24/06/26 13:55:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-41fa8739-9b10-453b-a151-02ef7a34cdc8
24/06/26 13:55:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-eb818c60-7828-46b8-a4d7-788a7f45e453
Starting Spark at Wed Jun 26 01:55:34 PM UTC 2024
24/06/26 13:55:36 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 13:55:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-a8d16cf5-72b6-44c7-9e38-027b19d01292;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 481ms :: artifacts dl 13ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-a8d16cf5-72b6-44c7-9e38-027b19d01292
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/11ms)
24/06/26 13:55:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 13:55:38 INFO SparkContext: Running Spark version 3.5.1
24/06/26 13:55:38 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:55:38 INFO SparkContext: Java version 11.0.20.1
24/06/26 13:55:38 INFO ResourceUtils: ==============================================================
24/06/26 13:55:38 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 13:55:38 INFO ResourceUtils: ==============================================================
24/06/26 13:55:38 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 13:55:38 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 13:55:38 INFO ResourceProfile: Limiting resource is cpu
24/06/26 13:55:38 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 13:55:38 INFO SecurityManager: Changing view acls to: root
24/06/26 13:55:38 INFO SecurityManager: Changing modify acls to: root
24/06/26 13:55:38 INFO SecurityManager: Changing view acls groups to: 
24/06/26 13:55:38 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 13:55:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 13:55:39 INFO Utils: Successfully started service 'sparkDriver' on port 38065.
24/06/26 13:55:39 INFO SparkEnv: Registering MapOutputTracker
24/06/26 13:55:39 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 13:55:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 13:55:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 13:55:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 13:55:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-440fe66d-6854-4706-8247-da7d8de0295c
24/06/26 13:55:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 13:55:39 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 13:55:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 13:55:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:38065/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:38065/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:38065/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:38065/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:38065/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:38065/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:38065/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:38065/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:38065/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:38065/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:38065/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:55:39 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719410138841
24/06/26 13:55:39 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:55:40 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 13:55:40 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 13:55:40 INFO Executor: Java version 11.0.20.1
24/06/26 13:55:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 13:55:40 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@533db11c for default.
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:55:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:38065 after 37 ms (0 ms spent in bootstraps)
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp12295955945714035306.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp12295955945714035306.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp12904892898379832396.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp12904892898379832396.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp15707771605289236534.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp15707771605289236534.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp8771366434498431631.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp8771366434498431631.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp11853309703572664475.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp11853309703572664475.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp5690987375165098367.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp5690987375165098367.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp297594760019750613.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp297594760019750613.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp15134605474446922895.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp15134605474446922895.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp7044010274513738170.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp7044010274513738170.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.lz4_lz4-java-1.8.0.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp9222637877528099996.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp9222637877528099996.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/commons-logging_commons-logging-1.1.3.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 13:55:40 INFO Executor: Fetching spark://10.0.2.15:38065/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719410138841
24/06/26 13:55:40 INFO Utils: Fetching spark://10.0.2.15:38065/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp14486925190917373900.tmp
24/06/26 13:55:40 INFO Utils: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/fetchFileTemp14486925190917373900.tmp has been previously copied to /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 13:55:40 INFO Executor: Adding file:/tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 13:55:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42689.
24/06/26 13:55:40 INFO NettyBlockTransferService: Server created on 10.0.2.15:42689
24/06/26 13:55:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 13:55:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 42689, None)
24/06/26 13:55:41 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:42689 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 42689, None)
24/06/26 13:55:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 42689, None)
24/06/26 13:55:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 42689, None)
24/06/26 13:55:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 13:55:41 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 13:55:43 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 13:55:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:55:43 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f resolved to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f.
24/06/26 13:55:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:55:43 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/metadata using temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/.metadata.9b244bd0-bba9-476b-8287-78bc2eb6ffdc.tmp
24/06/26 13:55:43 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/.metadata.9b244bd0-bba9-476b-8287-78bc2eb6ffdc.tmp to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/metadata
24/06/26 13:55:43 INFO MicroBatchExecution: Starting logs_table [id = 732375a5-298c-4f0f-800a-85d3a695075c, runId = 1d9c95af-27be-4d54-8a79-ece43474ac1c]. Use file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f to store the query checkpoint.
24/06/26 13:55:43 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7003875e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3c6a52fc]
24/06/26 13:55:43 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:55:43 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:55:43 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:55:43 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:55:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 13:55:44 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa resolved to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa.
24/06/26 13:55:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 13:55:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/metadata using temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/.metadata.1dea8f54-e9b5-49d1-b48f-c78d4c2c3076.tmp
24/06/26 13:55:44 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:55:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/.metadata.1dea8f54-e9b5-49d1-b48f-c78d4c2c3076.tmp to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/metadata
24/06/26 13:55:44 INFO MicroBatchExecution: Starting [id = bb52baf1-19ae-4a4b-905d-c43e7893b105, runId = 3fc1b9d7-17cb-48da-953d-6185bac0c4fa]. Use file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa to store the query checkpoint.
24/06/26 13:55:44 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7003875e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@3c6a52fc]
24/06/26 13:55:44 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:55:44 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 13:55:44 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 13:55:44 INFO MicroBatchExecution: Stream started from {}
24/06/26 13:55:44 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 13:55:44 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:55:44 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 13:55:44 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:55:44 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:55:44 INFO AppInfoParser: Kafka startTimeMs: 1719410144409
24/06/26 13:55:44 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 13:55:44 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 13:55:44 INFO AppInfoParser: Kafka startTimeMs: 1719410144410
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 13:55:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/sources/0/0 using temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/sources/0/.0.79086f71-696a-49d4-ae3b-59531d4dd92b.tmp
24/06/26 13:55:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/sources/0/0 using temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/sources/0/.0.33c8a983-31a4-4122-9a17-a98c1e497ae1.tmp
24/06/26 13:55:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/sources/0/.0.79086f71-696a-49d4-ae3b-59531d4dd92b.tmp to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/sources/0/0
24/06/26 13:55:44 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":247}}
24/06/26 13:55:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/sources/0/.0.33c8a983-31a4-4122-9a17-a98c1e497ae1.tmp to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/sources/0/0
24/06/26 13:55:45 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":247}}
24/06/26 13:55:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/offsets/0 using temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/offsets/.0.97b60705-5ce4-45b0-992e-7e8b6fab382b.tmp
24/06/26 13:55:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/offsets/0 using temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/offsets/.0.0e2619e5-1593-48a7-87d5-f8e373fd40b1.tmp
24/06/26 13:55:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/offsets/.0.97b60705-5ce4-45b0-992e-7e8b6fab382b.tmp to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/offsets/0
24/06/26 13:55:45 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719410145023,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:55:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/offsets/.0.0e2619e5-1593-48a7-87d5-f8e373fd40b1.tmp to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/offsets/0
24/06/26 13:55:45 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719410145040,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 13:55:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 13:55:46 INFO CodeGenerator: Code generated in 376.691282 ms
24/06/26 13:55:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@96fc79c]. The input RDD has 1 partitions.
24/06/26 13:55:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 13:55:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:55:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 13:55:46 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:55:46 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:55:46 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:55:46 INFO DAGScheduler: Missing parents: List()
24/06/26 13:55:46 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:55:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 13:55:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 13:55:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:42689 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:55:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 13:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:55:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 13:55:46 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 13:55:46 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 13:55:46 INFO DAGScheduler: Parents of final stage: List()
24/06/26 13:55:46 INFO DAGScheduler: Missing parents: List()
24/06/26 13:55:46 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 13:55:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.3 KiB, free 434.4 MiB)
24/06/26 13:55:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 13:55:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:42689 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:55:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 13:55:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 13:55:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 13:55:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:55:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 13:55:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 13:55:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 13:55:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 13:55:46 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 13:55:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 13:55:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 240 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:55:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 13:55:46 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
24/06/26 13:55:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:55:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 13:55:46 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.657452 s
24/06/26 13:55:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/26 13:55:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 13:55:46 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 13:55:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 13:55:47 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 290 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 13:55:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 13:55:47 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.388 s
24/06/26 13:55:47 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 13:55:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 13:55:47 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.740273 s
24/06/26 13:55:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@96fc79c] is committing.
24/06/26 13:55:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@96fc79c] committed.
24/06/26 13:55:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/commits/0 using temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/commits/.0.0ddf008b-3b88-4590-8792-f74608964e1a.tmp
+----+------+---+------+----+
|time|stream|log|offset|file|
+----+------+---+------+----+
+----+------+---+------+----+

24/06/26 13:55:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 13:55:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/commits/0 using temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/commits/.0.78eb9216-cabc-4302-a465-09e1a7d94059.tmp
24/06/26 13:55:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/commits/.0.0ddf008b-3b88-4590-8792-f74608964e1a.tmp to file:/tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f/commits/0
24/06/26 13:55:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/commits/.0.78eb9216-cabc-4302-a465-09e1a7d94059.tmp to file:/tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa/commits/0
24/06/26 13:55:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "bb52baf1-19ae-4a4b-905d-c43e7893b105",
  "runId" : "3fc1b9d7-17cb-48da-953d-6185bac0c4fa",
  "name" : null,
  "timestamp" : "2024-06-26T13:55:44.368Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1678,
    "commitOffsets" : 64,
    "getBatch" : 38,
    "latestOffset" : 642,
    "queryPlanning" : 235,
    "triggerExecution" : 2776,
    "walCommit" : 95
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@7cb1860b",
    "numOutputRows" : 0
  }
}
24/06/26 13:55:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "732375a5-298c-4f0f-800a-85d3a695075c",
  "runId" : "1d9c95af-27be-4d54-8a79-ece43474ac1c",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T13:55:43.818Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1620,
    "commitOffsets" : 108,
    "getBatch" : 16,
    "latestOffset" : 1197,
    "queryPlanning" : 241,
    "triggerExecution" : 3313,
    "walCommit" : 100
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 13:55:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:42689 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 13:55:54 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:42689 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 13:55:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:55:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:55:58] "GET / HTTP/1.1" 404 -
127.0.0.1 - - [26/Jun/2024 13:56:03] "GET /logs HTTP/1.1" 200 -
24/06/26 13:56:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 13:56:11] "GET /logs?log_level=info HTTP/1.1" 500 -
24/06/26 13:56:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:56:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:57:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:58:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 13:59:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:44 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 14:00:44 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 14:00:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:00:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:01:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:02:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:03:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:04:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:05:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:06:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:07:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:08:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:09:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:10:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:11:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:12:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:13:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:14:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:38 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:48 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:15:58 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:08 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:18 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:28 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:16:32 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 14:16:32 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 14:16:32 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 14:16:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 14:16:32 INFO MemoryStore: MemoryStore cleared
24/06/26 14:16:32 INFO BlockManager: BlockManager stopped
24/06/26 14:16:32 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 14:16:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 14:16:32 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016. Falling back to Java IO way
java.io.IOException: Failed to delete: /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/userFiles-9063ac67-434e-42eb-be66-d0dc3385d016
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2310)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2216)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/06/26 14:16:32 INFO SparkContext: Successfully stopped SparkContext
24/06/26 14:16:32 INFO ShutdownHookManager: Shutdown hook called
24/06/26 14:16:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-a1b4f93c-38e9-464d-925b-d547d83b8783
24/06/26 14:16:32 INFO ShutdownHookManager: Deleting directory /tmp/temporary-7117c2e7-5548-43ea-9837-ce50b2bf2caa
24/06/26 14:16:32 INFO ShutdownHookManager: Deleting directory /tmp/temporary-96e9cc7f-4a8e-43ea-9d26-dae06684b83f
24/06/26 14:16:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd
24/06/26 14:16:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-00a21845-a3eb-4d85-bc1b-15ffe5291cfd/pyspark-ae6f1e0c-e3f3-419b-829b-8bd04ec99661
Starting Spark at Wed Jun 26 02:17:03 PM UTC 2024
24/06/26 14:17:05 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 14:17:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-4daac349-4018-4fc3-aa78-5c460da168cc;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 524ms :: artifacts dl 14ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-4daac349-4018-4fc3-aa78-5c460da168cc
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/9ms)
24/06/26 14:17:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 14:17:07 INFO SparkContext: Running Spark version 3.5.1
24/06/26 14:17:07 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:17:07 INFO SparkContext: Java version 11.0.20.1
24/06/26 14:17:07 INFO ResourceUtils: ==============================================================
24/06/26 14:17:07 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 14:17:07 INFO ResourceUtils: ==============================================================
24/06/26 14:17:07 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 14:17:07 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 14:17:07 INFO ResourceProfile: Limiting resource is cpu
24/06/26 14:17:07 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 14:17:07 INFO SecurityManager: Changing view acls to: root
24/06/26 14:17:07 INFO SecurityManager: Changing modify acls to: root
24/06/26 14:17:07 INFO SecurityManager: Changing view acls groups to: 
24/06/26 14:17:07 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 14:17:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 14:17:08 INFO Utils: Successfully started service 'sparkDriver' on port 44837.
24/06/26 14:17:08 INFO SparkEnv: Registering MapOutputTracker
24/06/26 14:17:08 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 14:17:08 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 14:17:08 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 14:17:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 14:17:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c2259c88-48f6-43af-ab56-7917b27e0559
24/06/26 14:17:08 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 14:17:08 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 14:17:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 14:17:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44837/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44837/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:44837/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:44837/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:44837/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:44837/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:44837/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:44837/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:44837/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:44837/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:44837/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:17:08 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:08 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:17:09 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:17:09 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 14:17:09 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:17:09 INFO Executor: Java version 11.0.20.1
24/06/26 14:17:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 14:17:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@622432cc for default.
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:17:09 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:44837 after 35 ms (0 ms spent in bootstraps)
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp5256157162909721821.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp5256157162909721821.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp17843632602816934395.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp17843632602816934395.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp6977178419043613212.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp6977178419043613212.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp16444264147280744784.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp16444264147280744784.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp17446592391358906259.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp17446592391358906259.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp11221107778492124470.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp11221107778492124470.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp2953112689923270223.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp2953112689923270223.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp12952717084842942846.tmp
24/06/26 14:17:09 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp12952717084842942846.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:17:09 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 14:17:09 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411427775
24/06/26 14:17:09 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp9482693327727703344.tmp
24/06/26 14:17:10 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp9482693327727703344.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:17:10 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 14:17:10 INFO Executor: Fetching spark://10.0.2.15:44837/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411427775
24/06/26 14:17:10 INFO Utils: Fetching spark://10.0.2.15:44837/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp9823364975157949864.tmp
24/06/26 14:17:10 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp9823364975157949864.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:17:10 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 14:17:10 INFO Executor: Fetching spark://10.0.2.15:44837/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411427775
24/06/26 14:17:10 INFO Utils: Fetching spark://10.0.2.15:44837/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp11477562432505538536.tmp
24/06/26 14:17:10 INFO Utils: /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/fetchFileTemp11477562432505538536.tmp has been previously copied to /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:17:10 INFO Executor: Adding file:/tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/userFiles-7e188328-d594-409d-ba1a-cf3b2fb23a67/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 14:17:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 32903.
24/06/26 14:17:10 INFO NettyBlockTransferService: Server created on 10.0.2.15:32903
24/06/26 14:17:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 14:17:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 32903, None)
24/06/26 14:17:10 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:32903 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 32903, None)
24/06/26 14:17:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 32903, None)
24/06/26 14:17:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 32903, None)
24/06/26 14:17:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 14:17:10 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 14:17:12 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 14:17:12 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:17:12 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae resolved to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae.
24/06/26 14:17:12 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:17:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/metadata using temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/.metadata.279a49d7-a6b0-45d1-9003-fc932d726c88.tmp
24/06/26 14:17:13 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/.metadata.279a49d7-a6b0-45d1-9003-fc932d726c88.tmp to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/metadata
24/06/26 14:17:13 INFO MicroBatchExecution: Starting logs_table [id = 2fffb5f9-58bc-4fd2-8f97-0bc91b03d4be, runId = 00cbee47-87db-4140-b724-b3f8c6c99e14]. Use file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae to store the query checkpoint.
24/06/26 14:17:13 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@69be926e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@64cf9e7]
24/06/26 14:17:13 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:17:13 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:17:13 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:17:13 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:17:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:17:13 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c resolved to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c.
24/06/26 14:17:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:17:13 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:17:13 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/metadata using temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/.metadata.40b20cba-12dc-461e-a765-b8be3b2c8be1.tmp
24/06/26 14:17:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/.metadata.40b20cba-12dc-461e-a765-b8be3b2c8be1.tmp to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/metadata
24/06/26 14:17:14 INFO MicroBatchExecution: Starting [id = 6b82a6d0-56bb-4887-bad9-75ef6b0b2e9b, runId = 930e6396-5149-4bfe-ba3f-f3c7e816c99b]. Use file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c to store the query checkpoint.
24/06/26 14:17:14 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@69be926e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@64cf9e7]
24/06/26 14:17:14 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:17:14 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:17:14 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:17:14 INFO MicroBatchExecution: Stream started from {}
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 14:17:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:17:14 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:17:14 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:17:14 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:17:14 INFO AppInfoParser: Kafka startTimeMs: 1719411434265
24/06/26 14:17:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:17:14 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:17:14 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:17:14 INFO AppInfoParser: Kafka startTimeMs: 1719411434282
24/06/26 14:17:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/sources/0/0 using temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/sources/0/.0.3d9ab46a-856a-4b5b-8260-4eb98e80e896.tmp
24/06/26 14:17:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/sources/0/0 using temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/sources/0/.0.b4936b97-cdc3-4298-bd12-4e05e490667f.tmp
24/06/26 14:17:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/sources/0/.0.3d9ab46a-856a-4b5b-8260-4eb98e80e896.tmp to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/sources/0/0
24/06/26 14:17:14 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":247}}
24/06/26 14:17:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/sources/0/.0.b4936b97-cdc3-4298-bd12-4e05e490667f.tmp to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/sources/0/0
24/06/26 14:17:14 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":247}}
24/06/26 14:17:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/0 using temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/.0.a460ccd9-4709-4c0c-9397-a2671b899cd5.tmp
24/06/26 14:17:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/0 using temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/.0.8dc18c33-a0f1-4908-b247-520807ec983c.tmp
24/06/26 14:17:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/.0.8dc18c33-a0f1-4908-b247-520807ec983c.tmp to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/0
24/06/26 14:17:15 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/.0.a460ccd9-4709-4c0c-9397-a2671b899cd5.tmp to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/0
24/06/26 14:17:15 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719411434924,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:17:15 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719411434922,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 14:17:15 INFO CodeGenerator: Code generated in 346.045099 ms
24/06/26 14:17:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:17:16 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1c5a9243]. The input RDD has 1 partitions.
24/06/26 14:17:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:17:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:17:16 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:17:16 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:17:16 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:17:16 INFO DAGScheduler: Missing parents: List()
24/06/26 14:17:16 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:17:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 14:17:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 14:17:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:32903 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:17:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 14:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:17:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 14:17:16 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:17:16 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:17:16 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:17:16 INFO DAGScheduler: Missing parents: List()
24/06/26 14:17:16 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:17:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 14:17:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 14:17:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:32903 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:17:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 14:17:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:17:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 14:17:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:17:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:17:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 14:17:16 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 14:17:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:17:16 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 14:17:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:17:16 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 14:17:16 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 14:17:16 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 14:17:16 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 216 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:17:16 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.499 s
24/06/26 14:17:16 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 14:17:16 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 165 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:17:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 14:17:16 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:17:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 14:17:16 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.581991 s
24/06/26 14:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
24/06/26 14:17:16 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.280 s
Batch: 0
24/06/26 14:17:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
-------------------------------------------
24/06/26 14:17:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 14:17:16 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.599760 s
24/06/26 14:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1c5a9243] is committing.
24/06/26 14:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@1c5a9243] committed.
24/06/26 14:17:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/0 using temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/.0.b5436c4c-78d1-4156-9a1a-59174ebcf0cd.tmp
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 14:17:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:17:16 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/0 using temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/.0.0969ff89-9e78-4acc-9f42-da8ae57350c6.tmp
24/06/26 14:17:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/.0.b5436c4c-78d1-4156-9a1a-59174ebcf0cd.tmp to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/0
24/06/26 14:17:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2fffb5f9-58bc-4fd2-8f97-0bc91b03d4be",
  "runId" : "00cbee47-87db-4140-b724-b3f8c6c99e14",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:17:13.359Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1430,
    "commitOffsets" : 108,
    "getBatch" : 24,
    "latestOffset" : 1525,
    "queryPlanning" : 223,
    "triggerExecution" : 3437,
    "walCommit" : 76
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 14:17:16 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/.0.0969ff89-9e78-4acc-9f42-da8ae57350c6.tmp to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/0
24/06/26 14:17:16 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "6b82a6d0-56bb-4887-bad9-75ef6b0b2e9b",
  "runId" : "930e6396-5149-4bfe-ba3f-f3c7e816c99b",
  "name" : null,
  "timestamp" : "2024-06-26T14:17:14.222Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1502,
    "commitOffsets" : 104,
    "getBatch" : 24,
    "latestOffset" : 698,
    "queryPlanning" : 226,
    "triggerExecution" : 2645,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@6993285c",
    "numOutputRows" : 0
  }
}
127.0.0.1 - - [26/Jun/2024 14:17:24] "GET / HTTP/1.1" 404 -
24/06/26 14:17:24 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:32903 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:17:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:32903 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:17:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:17:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 14:17:27] "GET /logs HTTP/1.1" 200 -
24/06/26 14:17:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/1 using temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/.1.eee03de6-bb21-4759-994e-10d0fdbc597a.tmp
24/06/26 14:17:33 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/1 using temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/.1.d23c5248-8ab8-4805-badd-af5d46a9a725.tmp
24/06/26 14:17:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/.1.eee03de6-bb21-4759-994e-10d0fdbc597a.tmp to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/offsets/1
24/06/26 14:17:33 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719411453061,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:17:33 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/.1.d23c5248-8ab8-4805-badd-af5d46a9a725.tmp to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/offsets/1
24/06/26 14:17:33 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719411453062,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:17:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@709a8074]. The input RDD has 1 partitions.
24/06/26 14:17:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:17:33 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:17:33 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:17:33 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:17:33 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:17:33 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:17:33 INFO DAGScheduler: Missing parents: List()
24/06/26 14:17:33 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:17:33 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 14:17:33 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 14:17:33 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:32903 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:17:33 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 14:17:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[14] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:17:33 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 14:17:33 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:17:33 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:17:33 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:17:33 INFO DAGScheduler: Missing parents: List()
24/06/26 14:17:33 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 14:17:33 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:17:33 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 14:17:33 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 14:17:33 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 14:17:33 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:32903 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 14:17:33 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 14:17:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:17:33 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 14:17:33 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/26 14:17:33 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 14:17:33 INFO CodeGenerator: Code generated in 30.833977 ms
24/06/26 14:17:33 INFO CodeGenerator: Code generated in 15.677472 ms
24/06/26 14:17:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=247 untilOffset=250, for query queryId=6b82a6d0-56bb-4887-bad9-75ef6b0b2e9b batchId=1 taskId=3 partitionId=0
24/06/26 14:17:33 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=247 untilOffset=250, for query queryId=2fffb5f9-58bc-4fd2-8f97-0bc91b03d4be batchId=1 taskId=2 partitionId=0
24/06/26 14:17:33 INFO CodeGenerator: Code generated in 17.350529 ms
24/06/26 14:17:33 INFO CodeGenerator: Code generated in 26.408381 ms
24/06/26 14:17:33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 14:17:33 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 14:17:33 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:17:33 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:17:33 INFO AppInfoParser: Kafka startTimeMs: 1719411453797
24/06/26 14:17:33 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:17:33 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:17:33 INFO AppInfoParser: Kafka startTimeMs: 1719411453798
24/06/26 14:17:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Assigned to partition(s): logs-0
24/06/26 14:17:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Assigned to partition(s): logs-0
24/06/26 14:17:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Seeking to offset 247 for partition logs-0
24/06/26 14:17:33 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Seeking to offset 247 for partition logs-0
24/06/26 14:17:33 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 14:17:33 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 14:17:33 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 14:17:33 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 14:17:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Seeking to earliest offset of partition logs-0
24/06/26 14:17:33 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Seeking to earliest offset of partition logs-0
24/06/26 14:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Seeking to latest offset of partition logs-0
24/06/26 14:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Seeking to latest offset of partition logs-0
24/06/26 14:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=250, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:17:34 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=250, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:17:34 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:17:34 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 14:17:34 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor read 3 records through 1 polls (polled  out 3 records), taking 599359800 nanos, during time span of 747915895 nanos.
24/06/26 14:17:34 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2377 bytes result sent to driver
24/06/26 14:17:34 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1210 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:17:34 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 14:17:34 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.227 s
24/06/26 14:17:34 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:17:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 14:17:34 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.324387 s
24/06/26 14:17:34 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 14:17:34 INFO CodeGenerator: Code generated in 9.867534 ms
24/06/26 14:17:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:32903 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 14:17:35 INFO CodeGenerator: Code generated in 11.937345 ms
24/06/26 14:17:35 INFO CodeGenerator: Code generated in 15.287619 ms
24/06/26 14:17:35 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:17:35 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/26 14:17:35 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor read 3 records through 1 polls (polled  out 3 records), taking 598120510 nanos, during time span of 1843303494 nanos.
24/06/26 14:17:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:17:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3345 bytes result sent to driver
24/06/26 14:17:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2341 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:17:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/1 using temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/.1.7b0f9671-573f-4bb6-ab34-aacac2c62fd9.tmp
24/06/26 14:17:35 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 2.406 s
24/06/26 14:17:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:17:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 14:17:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 14:17:35 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 2.442855 s
24/06/26 14:17:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@709a8074] is committing.
24/06/26 14:17:35 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@709a8074] committed.
24/06/26 14:17:35 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/1 using temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/.1.7364316d-ed37-4b36-91e3-5716ca09f9c7.tmp
24/06/26 14:17:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/.1.7b0f9671-573f-4bb6-ab34-aacac2c62fd9.tmp to file:/tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c/commits/1
24/06/26 14:17:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "6b82a6d0-56bb-4887-bad9-75ef6b0b2e9b",
  "runId" : "930e6396-5149-4bfe-ba3f-f3c7e816c99b",
  "name" : null,
  "timestamp" : "2024-06-26T14:17:33.048Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 1.0865628395508873,
  "durationMs" : {
    "addBatch" : 2486,
    "commitOffsets" : 142,
    "getBatch" : 0,
    "latestOffset" : 13,
    "queryPlanning" : 31,
    "triggerExecution" : 2760,
    "walCommit" : 85
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 1.0865628395508873,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@6993285c",
    "numOutputRows" : 3
  }
}
24/06/26 14:17:35 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/.1.7364316d-ed37-4b36-91e3-5716ca09f9c7.tmp to file:/tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae/commits/1
24/06/26 14:17:35 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2fffb5f9-58bc-4fd2-8f97-0bc91b03d4be",
  "runId" : "00cbee47-87db-4140-b724-b3f8c6c99e14",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:17:33.053Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 142.85714285714286,
  "processedRowsPerSecond" : 1.080691642651297,
  "durationMs" : {
    "addBatch" : 2512,
    "commitOffsets" : 137,
    "getBatch" : 1,
    "latestOffset" : 8,
    "queryPlanning" : 31,
    "triggerExecution" : 2776,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 247
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 142.85714285714286,
    "processedRowsPerSecond" : 1.080691642651297,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/26 14:17:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:32903 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:17:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:17:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:17:56 INFO CodeGenerator: Code generated in 42.319047 ms
24/06/26 14:17:56 INFO CodeGenerator: Code generated in 14.413348 ms
127.0.0.1 - - [26/Jun/2024 14:17:56] "GET /logs HTTP/1.1" 200 -
24/06/26 14:18:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 14:18:21] "GET /logs HTTP/1.1" 200 -
24/06/26 14:18:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:18:56 INFO CodeGenerator: Code generated in 47.976138 ms
24/06/26 14:18:56 INFO CodeGenerator: Code generated in 15.342045 ms
24/06/26 14:18:56 INFO SparkContext: Starting job: listTables at NativeMethodAccessorImpl.java:0
24/06/26 14:18:56 INFO DAGScheduler: Got job 4 (listTables at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:18:56 INFO DAGScheduler: Final stage: ResultStage 4 (listTables at NativeMethodAccessorImpl.java:0)
24/06/26 14:18:56 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:18:56 INFO DAGScheduler: Missing parents: List()
24/06/26 14:18:56 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[20] at listTables at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:18:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.8 KiB, free 434.4 MiB)
24/06/26 14:18:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 4.3 KiB, free 434.4 MiB)
24/06/26 14:18:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:32903 (size: 4.3 KiB, free: 434.4 MiB)
24/06/26 14:18:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 14:18:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[20] at listTables at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:18:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 14:18:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9835 bytes) 
24/06/26 14:18:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 14:18:56 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1326 bytes result sent to driver
24/06/26 14:18:56 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 40 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:18:56 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 14:18:56 INFO DAGScheduler: ResultStage 4 (listTables at NativeMethodAccessorImpl.java:0) finished in 0.054 s
24/06/26 14:18:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:18:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/26 14:18:56 INFO DAGScheduler: Job 4 finished: listTables at NativeMethodAccessorImpl.java:0, took 0.062539 s
24/06/26 14:18:57 INFO CodeGenerator: Code generated in 32.907131 ms
24/06/26 14:18:57 INFO CodeGenerator: Code generated in 21.033895 ms
24/06/26 14:18:57 INFO SparkContext: Starting job: hasNext at NativeMethodAccessorImpl.java:0
24/06/26 14:18:57 INFO DAGScheduler: Got job 5 (hasNext at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:18:57 INFO DAGScheduler: Final stage: ResultStage 5 (hasNext at NativeMethodAccessorImpl.java:0)
24/06/26 14:18:57 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:18:57 INFO DAGScheduler: Missing parents: List()
24/06/26 14:18:57 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[24] at toLocalIterator at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:18:57 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.5 KiB, free 434.4 MiB)
24/06/26 14:18:57 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 434.4 MiB)
24/06/26 14:18:57 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:32903 (size: 3.0 KiB, free: 434.4 MiB)
24/06/26 14:18:57 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 14:18:57 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:32903 in memory (size: 4.3 KiB, free: 434.4 MiB)
24/06/26 14:18:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[24] at toLocalIterator at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:18:57 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/26 14:18:57 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9883 bytes) 
24/06/26 14:18:57 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/26 14:18:57 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1337 bytes result sent to driver
24/06/26 14:18:57 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 47 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:18:57 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/26 14:18:57 INFO DAGScheduler: ResultStage 5 (hasNext at NativeMethodAccessorImpl.java:0) finished in 0.091 s
24/06/26 14:18:57 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:18:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/26 14:18:57 INFO DAGScheduler: Job 5 finished: hasNext at NativeMethodAccessorImpl.java:0, took 0.106426 s
24/06/26 14:18:57 INFO CodeGenerator: Code generated in 21.452739 ms
127.0.0.1 - - [26/Jun/2024 14:18:57] "GET /tables HTTP/1.1" 200 -
24/06/26 14:19:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:06 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:32903 in memory (size: 3.0 KiB, free: 434.4 MiB)
24/06/26 14:19:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:19:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:20:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:20:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:20:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 14:20:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1, groupId=spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 14:20:09 INFO Metrics: Metrics scheduler closed
24/06/26 14:20:09 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 14:20:09 INFO Metrics: Metrics reporters closed
24/06/26 14:20:09 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-d441db7d-a905-455b-b562-e1d2d9cbbb31--468796198-executor-1 unregistered
24/06/26 14:20:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 14:20:09 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2, groupId=spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 14:20:09 INFO Metrics: Metrics scheduler closed
24/06/26 14:20:09 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 14:20:09 INFO Metrics: Metrics reporters closed
24/06/26 14:20:09 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-acd2c029-7dfe-40a8-baaa-5af2833fb025-150148700-executor-2 unregistered
24/06/26 14:20:09 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 14:20:09 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 14:20:09 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 14:20:10 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 14:20:10 INFO MemoryStore: MemoryStore cleared
24/06/26 14:20:10 INFO BlockManager: BlockManager stopped
24/06/26 14:20:10 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 14:20:10 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 14:20:10 INFO SparkContext: Successfully stopped SparkContext
24/06/26 14:20:10 INFO ShutdownHookManager: Shutdown hook called
24/06/26 14:20:10 INFO ShutdownHookManager: Deleting directory /tmp/temporary-a7e80c1b-fcc0-414d-bf77-d907382b179c
24/06/26 14:20:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841/pyspark-f0121717-f67d-4e62-9131-a0b5245faa4f
24/06/26 14:20:10 INFO ShutdownHookManager: Deleting directory /tmp/temporary-27988c0b-cb76-4ca1-87ef-8c58c061f5ae
24/06/26 14:20:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-7eb13a5f-0547-48db-af16-dee535e028d5
24/06/26 14:20:10 INFO ShutdownHookManager: Deleting directory /tmp/spark-30522b17-1a3b-4d12-971f-628a011dc841
Starting Spark at Wed Jun 26 02:20:17 PM UTC 2024
24/06/26 14:20:19 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 14:20:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-1a4ad336-07c0-491f-a0c3-5d0f8a8cee3c;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 505ms :: artifacts dl 15ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-1a4ad336-07c0-491f-a0c3-5d0f8a8cee3c
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/9ms)
24/06/26 14:20:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 14:20:21 INFO SparkContext: Running Spark version 3.5.1
24/06/26 14:20:21 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:20:21 INFO SparkContext: Java version 11.0.20.1
24/06/26 14:20:21 INFO ResourceUtils: ==============================================================
24/06/26 14:20:21 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 14:20:21 INFO ResourceUtils: ==============================================================
24/06/26 14:20:21 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 14:20:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 14:20:21 INFO ResourceProfile: Limiting resource is cpu
24/06/26 14:20:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 14:20:21 INFO SecurityManager: Changing view acls to: root
24/06/26 14:20:21 INFO SecurityManager: Changing modify acls to: root
24/06/26 14:20:21 INFO SecurityManager: Changing view acls groups to: 
24/06/26 14:20:21 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 14:20:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 14:20:22 INFO Utils: Successfully started service 'sparkDriver' on port 34773.
24/06/26 14:20:22 INFO SparkEnv: Registering MapOutputTracker
24/06/26 14:20:22 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 14:20:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 14:20:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 14:20:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 14:20:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-512eb1d1-c770-42e6-ad31-7027e4cd2ca2
24/06/26 14:20:22 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 14:20:22 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 14:20:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 14:20:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:34773/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:34773/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:34773/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:34773/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:34773/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:34773/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:34773/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:34773/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:34773/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:34773/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:34773/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:20:22 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:20:22 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 14:20:22 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:20:22 INFO Executor: Java version 11.0.20.1
24/06/26 14:20:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 14:20:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@8eb7c73 for default.
24/06/26 14:20:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:20:22 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411621679
24/06/26 14:20:22 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:20:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:20:23 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:34773 after 36 ms (0 ms spent in bootstraps)
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp2499922695988249908.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp2499922695988249908.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp6556353275834350669.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp6556353275834350669.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp15847893209725324857.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp15847893209725324857.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp15305029490708588530.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp15305029490708588530.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp677642671274839079.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp677642671274839079.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp15936326184272212033.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp15936326184272212033.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp5643074481620846595.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp5643074481620846595.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp3196085679371864461.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp3196085679371864461.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp7018621604358734161.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp7018621604358734161.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp14694607541244771978.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp14694607541244771978.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 14:20:23 INFO Executor: Fetching spark://10.0.2.15:34773/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411621679
24/06/26 14:20:23 INFO Utils: Fetching spark://10.0.2.15:34773/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp5793144703111304190.tmp
24/06/26 14:20:23 INFO Utils: /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/fetchFileTemp5793144703111304190.tmp has been previously copied to /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:20:23 INFO Executor: Adding file:/tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/userFiles-c236ae0b-7f27-4e8e-b545-078b24eabbe6/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 14:20:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40989.
24/06/26 14:20:23 INFO NettyBlockTransferService: Server created on 10.0.2.15:40989
24/06/26 14:20:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 14:20:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 40989, None)
24/06/26 14:20:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:40989 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 40989, None)
24/06/26 14:20:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 40989, None)
24/06/26 14:20:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 40989, None)
24/06/26 14:20:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 14:20:24 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 14:20:26 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 14:20:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:20:26 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd resolved to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd.
24/06/26 14:20:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:20:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/metadata using temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/.metadata.b1ba993a-1f2e-462e-baeb-7c60d4a65c50.tmp
24/06/26 14:20:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/.metadata.b1ba993a-1f2e-462e-baeb-7c60d4a65c50.tmp to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/metadata
24/06/26 14:20:26 INFO MicroBatchExecution: Starting logs_table [id = 707353a2-5882-4a06-a347-77fecc199777, runId = a3ca8fcb-18b1-4bfc-81ca-9ab839935a07]. Use file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd to store the query checkpoint.
24/06/26 14:20:26 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7510ea4e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4f9e20bd]
24/06/26 14:20:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:20:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:20:26 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:20:26 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:20:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:20:27 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad resolved to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad.
24/06/26 14:20:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:20:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/metadata using temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/.metadata.e04b040b-506f-44fb-a982-414e0a60c402.tmp
24/06/26 14:20:27 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:20:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/.metadata.e04b040b-506f-44fb-a982-414e0a60c402.tmp to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/metadata
24/06/26 14:20:27 INFO MicroBatchExecution: Starting [id = 8cfc192b-6d45-4d55-affe-62369b6ae795, runId = 07ca7b12-2053-4378-be3f-d65c99f88bc7]. Use file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad to store the query checkpoint.
24/06/26 14:20:27 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@7510ea4e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4f9e20bd]
24/06/26 14:20:27 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:20:27 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:20:27 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:20:27 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:20:27 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:20:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:20:27 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:20:27 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:20:27 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:20:27 INFO AppInfoParser: Kafka startTimeMs: 1719411627383
24/06/26 14:20:27 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:20:27 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:20:27 INFO AppInfoParser: Kafka startTimeMs: 1719411627383
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 14:20:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/sources/0/0 using temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/sources/0/.0.db92c9fd-c9cc-4c58-ac49-c7ac342ac86a.tmp
24/06/26 14:20:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/sources/0/0 using temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/sources/0/.0.c2a137aa-9190-475a-9deb-ea42ff4cc354.tmp
24/06/26 14:20:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/sources/0/.0.db92c9fd-c9cc-4c58-ac49-c7ac342ac86a.tmp to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/sources/0/0
24/06/26 14:20:27 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":250}}
24/06/26 14:20:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/sources/0/.0.c2a137aa-9190-475a-9deb-ea42ff4cc354.tmp to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/sources/0/0
24/06/26 14:20:27 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":250}}
24/06/26 14:20:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/offsets/0 using temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/offsets/.0.bf7ce468-2713-4668-a01c-cc4f413b2e91.tmp
24/06/26 14:20:28 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/offsets/0 using temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/offsets/.0.2be3e60e-3a44-4f3e-b4dd-ddc86526dddb.tmp
24/06/26 14:20:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/offsets/.0.bf7ce468-2713-4668-a01c-cc4f413b2e91.tmp to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/offsets/0
24/06/26 14:20:28 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719411628005,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:20:28 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/offsets/.0.2be3e60e-3a44-4f3e-b4dd-ddc86526dddb.tmp to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/offsets/0
24/06/26 14:20:28 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719411628005,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:20:29 INFO CodeGenerator: Code generated in 237.758165 ms
24/06/26 14:20:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6592c537]. The input RDD has 1 partitions.
24/06/26 14:20:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:20:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:20:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:20:29 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:20:29 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:20:29 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:20:29 INFO DAGScheduler: Missing parents: List()
24/06/26 14:20:29 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:20:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 14:20:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 14:20:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:40989 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:20:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 14:20:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:20:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 14:20:29 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:20:29 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:20:29 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:20:29 INFO DAGScheduler: Missing parents: List()
24/06/26 14:20:29 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:20:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 14:20:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 14:20:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:40989 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:20:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 14:20:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:20:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 14:20:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:20:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:20:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 14:20:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 14:20:30 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:20:30 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 14:20:30 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:20:30 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 14:20:30 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 14:20:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 14:20:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 235 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:20:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 14:20:30 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.579 s
24/06/26 14:20:30 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 186 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:20:30 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 14:20:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:20:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 14:20:30 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.662243 s
24/06/26 14:20:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 14:20:30 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.300 s
24/06/26 14:20:30 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:20:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 14:20:30 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.672077 s
24/06/26 14:20:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6592c537] is committing.
24/06/26 14:20:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6592c537] committed.
24/06/26 14:20:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/commits/0 using temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/commits/.0.27d69a8e-773b-487d-9c5d-0c19a9286a8b.tmp
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 14:20:30 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:20:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/commits/.0.27d69a8e-773b-487d-9c5d-0c19a9286a8b.tmp to file:/tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd/commits/0
24/06/26 14:20:30 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/commits/0 using temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/commits/.0.2b601023-cc9d-471f-8e7d-acc191a767f3.tmp
24/06/26 14:20:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "707353a2-5882-4a06-a347-77fecc199777",
  "runId" : "a3ca8fcb-18b1-4bfc-81ca-9ab839935a07",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:20:26.734Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1353,
    "commitOffsets" : 99,
    "getBatch" : 13,
    "latestOffset" : 1250,
    "queryPlanning" : 628,
    "triggerExecution" : 3468,
    "walCommit" : 95
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 14:20:30 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/commits/.0.2b601023-cc9d-471f-8e7d-acc191a767f3.tmp to file:/tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad/commits/0
24/06/26 14:20:30 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "8cfc192b-6d45-4d55-affe-62369b6ae795",
  "runId" : "07ca7b12-2053-4378-be3f-d65c99f88bc7",
  "name" : null,
  "timestamp" : "2024-06-26T14:20:27.287Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1451,
    "commitOffsets" : 71,
    "getBatch" : 27,
    "latestOffset" : 707,
    "queryPlanning" : 628,
    "triggerExecution" : 2986,
    "walCommit" : 79
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@71c128b0",
    "numOutputRows" : 0
  }
}
DataFrame[timestamp: string, log_level: string, message: string]
127.0.0.1 - - [26/Jun/2024 14:20:34] "GET /logs HTTP/1.1" 200 -
24/06/26 14:20:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:40989 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:20:34 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:40989 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:20:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:20:50 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:00 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:10 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:20 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:30 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:40 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:21:44 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 14:21:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 14:21:44 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 14:21:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 14:21:44 INFO MemoryStore: MemoryStore cleared
24/06/26 14:21:44 INFO BlockManager: BlockManager stopped
24/06/26 14:21:44 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 14:21:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 14:21:44 INFO SparkContext: Successfully stopped SparkContext
24/06/26 14:21:44 INFO ShutdownHookManager: Shutdown hook called
24/06/26 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f
24/06/26 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/temporary-dbae5b9c-c83a-4389-8c15-a92ea7de6aad
24/06/26 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-20bf7c9e-7f93-4c6c-92a2-09cca73cff5f/pyspark-31adac47-f7e4-45d7-aee3-aea3ad1f4d37
24/06/26 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-3f844418-85ab-4e26-869b-c2cd8e03522e
24/06/26 14:21:44 INFO ShutdownHookManager: Deleting directory /tmp/temporary-7883ddef-27b5-40e6-a4e0-793596a24ddd
Starting Spark at Wed Jun 26 02:21:52 PM UTC 2024
24/06/26 14:21:54 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 14:21:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-43b43c78-697a-465e-88f7-d344e28cfb2b;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 509ms :: artifacts dl 17ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-43b43c78-697a-465e-88f7-d344e28cfb2b
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/9ms)
24/06/26 14:21:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 14:21:57 INFO SparkContext: Running Spark version 3.5.1
24/06/26 14:21:57 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:21:57 INFO SparkContext: Java version 11.0.20.1
24/06/26 14:21:57 INFO ResourceUtils: ==============================================================
24/06/26 14:21:57 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 14:21:57 INFO ResourceUtils: ==============================================================
24/06/26 14:21:57 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 14:21:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 14:21:57 INFO ResourceProfile: Limiting resource is cpu
24/06/26 14:21:57 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 14:21:57 INFO SecurityManager: Changing view acls to: root
24/06/26 14:21:57 INFO SecurityManager: Changing modify acls to: root
24/06/26 14:21:57 INFO SecurityManager: Changing view acls groups to: 
24/06/26 14:21:57 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 14:21:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 14:21:57 INFO Utils: Successfully started service 'sparkDriver' on port 34199.
24/06/26 14:21:57 INFO SparkEnv: Registering MapOutputTracker
24/06/26 14:21:57 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 14:21:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 14:21:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 14:21:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 14:21:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f04354a4-a8b7-4037-93b9-674b3155e12d
24/06/26 14:21:57 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 14:21:57 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 14:21:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 14:21:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:34199/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:34199/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:34199/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:34199/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:34199/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:34199/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:34199/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:34199/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:34199/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:34199/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:34199/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:21:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:21:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:21:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411716992
24/06/26 14:21:57 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:21:58 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:21:58 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 14:21:58 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:21:58 INFO Executor: Java version 11.0.20.1
24/06/26 14:21:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 14:21:58 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@661c8dfa for default.
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:21:58 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:21:58 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:34199 after 31 ms (0 ms spent in bootstraps)
24/06/26 14:21:58 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp565002875469718561.tmp
24/06/26 14:21:58 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp565002875469718561.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:21:58 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 14:21:58 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp16712750978782096538.tmp
24/06/26 14:21:58 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp16712750978782096538.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:21:58 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 14:21:58 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp8358072573379648734.tmp
24/06/26 14:21:58 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp8358072573379648734.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:21:58 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 14:21:58 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719411716992
24/06/26 14:21:58 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp18365580530714507379.tmp
24/06/26 14:21:58 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp18365580530714507379.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:21:58 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 14:21:58 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp4864012727132429857.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp4864012727132429857.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 14:21:59 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp2036059599287490047.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp2036059599287490047.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:21:59 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp4065799281005089571.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp4065799281005089571.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 14:21:59 INFO Executor: Fetching spark://10.0.2.15:34199/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp11935563380840668665.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp11935563380840668665.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 14:21:59 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp10260384455688279358.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp10260384455688279358.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 14:21:59 INFO Executor: Fetching spark://10.0.2.15:34199/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp10993444439498768880.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp10993444439498768880.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:21:59 INFO Executor: Fetching spark://10.0.2.15:34199/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719411716992
24/06/26 14:21:59 INFO Utils: Fetching spark://10.0.2.15:34199/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp2980938064915786292.tmp
24/06/26 14:21:59 INFO Utils: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/fetchFileTemp2980938064915786292.tmp has been previously copied to /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:21:59 INFO Executor: Adding file:/tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 14:21:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45627.
24/06/26 14:21:59 INFO NettyBlockTransferService: Server created on 10.0.2.15:45627
24/06/26 14:21:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 14:21:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 45627, None)
24/06/26 14:21:59 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:45627 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 45627, None)
24/06/26 14:21:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 45627, None)
24/06/26 14:21:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 45627, None)
24/06/26 14:21:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 14:21:59 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 14:22:01 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 14:22:01 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:22:01 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0 resolved to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0.
24/06/26 14:22:01 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:22:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/metadata using temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/.metadata.08bd0ef8-ca98-4b37-80b6-f3e62c42356e.tmp
24/06/26 14:22:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/.metadata.08bd0ef8-ca98-4b37-80b6-f3e62c42356e.tmp to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/metadata
24/06/26 14:22:01 INFO MicroBatchExecution: Starting logs_table [id = 303133e7-9a5a-43cb-a5aa-bc1b44cbeca8, runId = 7cfa6cee-4eb6-4305-96e9-edabe277ba74]. Use file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0 to store the query checkpoint.
24/06/26 14:22:02 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5e5c15f9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@22516c25]
24/06/26 14:22:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:22:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:22:02 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:22:02 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:22:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:22:02 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6 resolved to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6.
24/06/26 14:22:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:22:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/metadata using temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/.metadata.4e6d04b4-b084-490a-a32d-ef23fa60338f.tmp
24/06/26 14:22:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/.metadata.4e6d04b4-b084-490a-a32d-ef23fa60338f.tmp to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/metadata
24/06/26 14:22:02 INFO MicroBatchExecution: Starting [id = a0c7d6cb-ed04-43fd-bcac-836f4df50078, runId = b3751b82-3c51-41db-87ac-14ffddd90baa]. Use file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6 to store the query checkpoint.
24/06/26 14:22:02 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:22:02 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@5e5c15f9] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@22516c25]
24/06/26 14:22:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:22:02 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:22:02 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:22:02 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:22:02 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:22:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:22:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:22:02 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:22:02 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:22:02 INFO AppInfoParser: Kafka startTimeMs: 1719411722786
24/06/26 14:22:02 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:22:02 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:22:02 INFO AppInfoParser: Kafka startTimeMs: 1719411722789
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 14:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/sources/0/0 using temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/sources/0/.0.fe3f6306-6035-4a2e-98de-33cc7f02cc64.tmp
24/06/26 14:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/sources/0/0 using temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/sources/0/.0.ce31c349-f768-485a-b768-81860b373830.tmp
24/06/26 14:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/sources/0/.0.fe3f6306-6035-4a2e-98de-33cc7f02cc64.tmp to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/sources/0/0
24/06/26 14:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/sources/0/.0.ce31c349-f768-485a-b768-81860b373830.tmp to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/sources/0/0
24/06/26 14:22:03 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":250}}
24/06/26 14:22:03 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":250}}
24/06/26 14:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/offsets/0 using temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/offsets/.0.4ec8c006-05b2-43e0-a491-7ae3d2475b15.tmp
24/06/26 14:22:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/offsets/0 using temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/offsets/.0.4edd3b3f-b01c-4b02-ad2e-540fb637a323.tmp
24/06/26 14:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/offsets/.0.4edd3b3f-b01c-4b02-ad2e-540fb637a323.tmp to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/offsets/0
24/06/26 14:22:03 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719411723532,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:22:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/offsets/.0.4ec8c006-05b2-43e0-a491-7ae3d2475b15.tmp to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/offsets/0
24/06/26 14:22:03 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719411723532,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:22:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 14:22:04 INFO CodeGenerator: Code generated in 276.316583 ms
24/06/26 14:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:22:04 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@38f2f767]. The input RDD has 1 partitions.
24/06/26 14:22:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:22:04 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:22:04 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:22:04 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:22:04 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:22:04 INFO DAGScheduler: Missing parents: List()
24/06/26 14:22:04 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:22:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 14:22:04 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 14:22:04 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:45627 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:22:04 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 14:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:22:04 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 14:22:04 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:22:04 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:22:04 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:22:04 INFO DAGScheduler: Missing parents: List()
24/06/26 14:22:04 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:22:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 14:22:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 14:22:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:45627 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:22:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 14:22:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:22:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 14:22:04 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:22:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:22:04 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 14:22:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 14:22:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:22:05 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 14:22:05 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:22:05 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 14:22:05 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/26 14:22:05 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1252 bytes result sent to driver
24/06/26 14:22:05 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 160 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:22:05 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 14:22:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 224 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:22:05 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 14:22:05 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.258 s
24/06/26 14:22:05 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 14:22:05 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.570600 s
24/06/26 14:22:05 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.535 s
24/06/26 14:22:05 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/26 14:22:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 14:22:05 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.579759 s
-------------------------------------------
24/06/26 14:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@38f2f767] is committing.
Batch: 0
-------------------------------------------
24/06/26 14:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@38f2f767] committed.
24/06/26 14:22:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/commits/0 using temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/commits/.0.45c0b646-8df8-4dfc-8f3c-a46838fa4541.tmp
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 14:22:05 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:22:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/commits/.0.45c0b646-8df8-4dfc-8f3c-a46838fa4541.tmp to file:/tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0/commits/0
24/06/26 14:22:05 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/commits/0 using temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/commits/.0.e43de622-177f-4ede-b6ad-b4e5cf8d2662.tmp
24/06/26 14:22:05 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/commits/.0.e43de622-177f-4ede-b6ad-b4e5cf8d2662.tmp to file:/tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6/commits/0
24/06/26 14:22:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "a0c7d6cb-ed04-43fd-bcac-836f4df50078",
  "runId" : "b3751b82-3c51-41db-87ac-14ffddd90baa",
  "name" : null,
  "timestamp" : "2024-06-26T14:22:02.626Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1368,
    "commitOffsets" : 67,
    "getBatch" : 26,
    "latestOffset" : 890,
    "queryPlanning" : 218,
    "triggerExecution" : 2677,
    "walCommit" : 81
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@5ca003cc",
    "numOutputRows" : 0
  }
}
24/06/26 14:22:05 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "303133e7-9a5a-43cb-a5aa-bc1b44cbeca8",
  "runId" : "7cfa6cee-4eb6-4305-96e9-edabe277ba74",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:22:02.073Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1270,
    "commitOffsets" : 111,
    "getBatch" : 2,
    "latestOffset" : 1431,
    "queryPlanning" : 214,
    "triggerExecution" : 3179,
    "walCommit" : 108
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 14:22:10 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:45627 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:22:10 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:45627 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:22:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[]
[]
127.0.0.1 - - [26/Jun/2024 14:22:48] "GET /logs HTTP/1.1" 200 -
24/06/26 14:22:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:22:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:23:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:24:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:25:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:26:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:03 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 14:27:03 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 14:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:27:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:28:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:29:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:30:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:31:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:32:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:05 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:15 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:25 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:35 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:45 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:33:55 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:34:01 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 14:34:01 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 14:34:01 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 14:34:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 14:34:01 INFO MemoryStore: MemoryStore cleared
24/06/26 14:34:01 INFO BlockManager: BlockManager stopped
24/06/26 14:34:01 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 14:34:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 14:34:01 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c. Falling back to Java IO way
java.io.IOException: Failed to delete: /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/userFiles-06ad19d0-5335-4c70-a799-f29020e2b30c
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:108)
	at org.apache.spark.SparkContext.$anonfun$stop$25(SparkContext.scala:2310)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2310)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2216)
	at org.apache.spark.SparkContext.$anonfun$new$34(SparkContext.scala:686)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/06/26 14:34:01 INFO SparkContext: Successfully stopped SparkContext
24/06/26 14:34:01 INFO ShutdownHookManager: Shutdown hook called
24/06/26 14:34:02 INFO ShutdownHookManager: Deleting directory /tmp/temporary-1e711313-9b2a-4f0c-9606-5c118a4befc0
24/06/26 14:34:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8
24/06/26 14:34:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-60dae20d-ab5d-4377-b25e-abe1d2a21e7e
24/06/26 14:34:02 INFO ShutdownHookManager: Deleting directory /tmp/temporary-3a93faed-fa14-42c8-a83e-7f4140a896b6
24/06/26 14:34:02 INFO ShutdownHookManager: Deleting directory /tmp/spark-018fcc46-f660-4318-9fab-d47f3ad791a8/pyspark-009f2638-3f59-47fb-9c77-1ea1bd3cf320
Starting Spark at Wed Jun 26 02:36:48 PM UTC 2024
24/06/26 14:36:50 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 14:36:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-530bce0a-a873-410e-b052-f97bce666ae9;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 503ms :: artifacts dl 13ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-530bce0a-a873-410e-b052-f97bce666ae9
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/12ms)
24/06/26 14:36:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 14:36:53 INFO SparkContext: Running Spark version 3.5.1
24/06/26 14:36:53 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:36:53 INFO SparkContext: Java version 11.0.20.1
24/06/26 14:36:53 INFO ResourceUtils: ==============================================================
24/06/26 14:36:53 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 14:36:53 INFO ResourceUtils: ==============================================================
24/06/26 14:36:53 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 14:36:53 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 14:36:53 INFO ResourceProfile: Limiting resource is cpu
24/06/26 14:36:53 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 14:36:53 INFO SecurityManager: Changing view acls to: root
24/06/26 14:36:53 INFO SecurityManager: Changing modify acls to: root
24/06/26 14:36:53 INFO SecurityManager: Changing view acls groups to: 
24/06/26 14:36:53 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 14:36:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 14:36:53 INFO Utils: Successfully started service 'sparkDriver' on port 44571.
24/06/26 14:36:53 INFO SparkEnv: Registering MapOutputTracker
24/06/26 14:36:53 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 14:36:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 14:36:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 14:36:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 14:36:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-496d5a85-af5c-4f7f-aac6-8d3f74c29202
24/06/26 14:36:53 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 14:36:53 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 14:36:54 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 14:36:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44571/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:44571/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:44571/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:44571/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:44571/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:44571/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:44571/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:44571/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:44571/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:44571/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:44571/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:36:54 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:36:54 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 14:36:54 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:36:54 INFO Executor: Java version 11.0.20.1
24/06/26 14:36:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 14:36:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4c83b5fa for default.
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:36:54 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:36:54 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:44571 after 31 ms (0 ms spent in bootstraps)
24/06/26 14:36:54 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp9933529927525618459.tmp
24/06/26 14:36:54 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp9933529927525618459.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:36:54 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 14:36:54 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp3356162316390419196.tmp
24/06/26 14:36:54 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp3356162316390419196.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:36:54 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 14:36:54 INFO Executor: Fetching spark://10.0.2.15:44571/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Fetching spark://10.0.2.15:44571/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp8157888193027177851.tmp
24/06/26 14:36:54 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp8157888193027177851.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:36:54 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 14:36:54 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp5527139449400878529.tmp
24/06/26 14:36:54 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp5527139449400878529.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:36:54 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 14:36:54 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719412613111
24/06/26 14:36:54 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp254714322139189476.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp254714322139189476.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 14:36:55 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:55 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp15820075031175269168.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp15820075031175269168.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:36:55 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719412613111
24/06/26 14:36:55 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp15727768802517008482.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp15727768802517008482.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 14:36:55 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719412613111
24/06/26 14:36:55 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp3573963607218213900.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp3573963607218213900.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:36:55 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719412613111
24/06/26 14:36:55 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp12947493808461044221.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp12947493808461044221.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 14:36:55 INFO Executor: Fetching spark://10.0.2.15:44571/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719412613111
24/06/26 14:36:55 INFO Utils: Fetching spark://10.0.2.15:44571/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp17221601733723158548.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp17221601733723158548.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 14:36:55 INFO Executor: Fetching spark://10.0.2.15:44571/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719412613111
24/06/26 14:36:55 INFO Utils: Fetching spark://10.0.2.15:44571/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp9836513684984072279.tmp
24/06/26 14:36:55 INFO Utils: /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/fetchFileTemp9836513684984072279.tmp has been previously copied to /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:36:55 INFO Executor: Adding file:/tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/userFiles-b493b017-f50d-4f7a-b59b-b9d2d5e0bae1/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 14:36:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44679.
24/06/26 14:36:55 INFO NettyBlockTransferService: Server created on 10.0.2.15:44679
24/06/26 14:36:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 14:36:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 44679, None)
24/06/26 14:36:55 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:44679 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 44679, None)
24/06/26 14:36:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 44679, None)
24/06/26 14:36:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 44679, None)
24/06/26 14:36:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 14:36:55 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 14:36:57 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 14:36:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:36:58 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd resolved to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd.
24/06/26 14:36:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:36:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/metadata using temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/.metadata.4fb3c8c1-830f-493e-ae34-bf26337127d2.tmp
24/06/26 14:36:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/.metadata.4fb3c8c1-830f-493e-ae34-bf26337127d2.tmp to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/metadata
24/06/26 14:36:58 INFO MicroBatchExecution: Starting logs_table [id = d1535517-1fb4-49a6-812a-60a186b51442, runId = 62776273-385d-4aa7-8dd6-01484d21d1b7]. Use file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd to store the query checkpoint.
24/06/26 14:36:58 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@644bec74] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@71c53a5c]
24/06/26 14:36:58 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:36:58 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:36:58 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:36:58 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:36:58 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:36:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:36:58 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214 resolved to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214.
24/06/26 14:36:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:36:58 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/metadata using temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/.metadata.a1864732-50ff-4a93-9375-17ae43c27d7f.tmp
24/06/26 14:36:58 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/.metadata.a1864732-50ff-4a93-9375-17ae43c27d7f.tmp to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/metadata
24/06/26 14:36:58 INFO MicroBatchExecution: Starting [id = e83a9a52-73aa-4842-a2dd-46c535a7d5bc, runId = f5a279a9-50fe-4b8e-aece-e4443c21398d]. Use file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214 to store the query checkpoint.
24/06/26 14:36:58 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@644bec74] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@71c53a5c]
24/06/26 14:36:58 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:36:58 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:36:58 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:36:58 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:36:59 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:36:59 INFO AppInfoParser: Kafka version: 3.4.1
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 14:36:59 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:36:59 INFO AppInfoParser: Kafka startTimeMs: 1719412619070
24/06/26 14:36:59 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:36:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:36:59 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:36:59 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:36:59 INFO AppInfoParser: Kafka startTimeMs: 1719412619131
24/06/26 14:36:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/sources/0/0 using temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/sources/0/.0.e9e8bd69-48d8-4aa8-90c9-3e529163850e.tmp
24/06/26 14:36:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/sources/0/0 using temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/sources/0/.0.207f3c1b-2c45-4340-b129-74a139eac39b.tmp
24/06/26 14:36:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/sources/0/.0.e9e8bd69-48d8-4aa8-90c9-3e529163850e.tmp to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/sources/0/0
24/06/26 14:36:59 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":250}}
24/06/26 14:36:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/sources/0/.0.207f3c1b-2c45-4340-b129-74a139eac39b.tmp to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/sources/0/0
24/06/26 14:36:59 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":250}}
24/06/26 14:36:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/0 using temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/.0.3842c693-d46d-4120-bf11-a87ce1813d00.tmp
24/06/26 14:36:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/0 using temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/.0.8a700766-3a80-48db-8b80-430ae67af8c0.tmp
24/06/26 14:36:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/.0.8a700766-3a80-48db-8b80-430ae67af8c0.tmp to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/0
24/06/26 14:36:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/.0.3842c693-d46d-4120-bf11-a87ce1813d00.tmp to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/0
24/06/26 14:36:59 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719412619736,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:36:59 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719412619753,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
127.0.0.1 - - [26/Jun/2024 14:37:00] "GET / HTTP/1.1" 404 -
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:37:00 INFO CodeGenerator: Code generated in 244.020391 ms
24/06/26 14:37:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:37:00 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4947be45]. The input RDD has 1 partitions.
24/06/26 14:37:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:37:00 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:37:00 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:37:00 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:37:00 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:37:00 INFO DAGScheduler: Missing parents: List()
24/06/26 14:37:00 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:37:01 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 14:37:01 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 14:37:01 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:44679 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:37:01 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 14:37:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:37:01 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 14:37:01 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:37:01 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:37:01 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:37:01 INFO DAGScheduler: Missing parents: List()
24/06/26 14:37:01 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:37:01 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 14:37:01 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 14:37:01 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:44679 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:37:01 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 14:37:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:37:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 14:37:01 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:37:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:37:01 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 14:37:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 14:37:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:37:01 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 14:37:01 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:37:01 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 14:37:01 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/26 14:37:01 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1252 bytes result sent to driver
24/06/26 14:37:01 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 134 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:37:01 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 14:37:01 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.214 s
24/06/26 14:37:01 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 196 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:37:01 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 14:37:01 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:37:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 14:37:01 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.486763 s
24/06/26 14:37:01 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.434 s
24/06/26 14:37:01 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:37:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/26 14:37:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 14:37:01 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.486076 s
24/06/26 14:37:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4947be45] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------24/06/26 14:37:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4947be45] committed.

24/06/26 14:37:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/0 using temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/.0.07240f68-9118-458e-a242-a5476aa3bc61.tmp
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 14:37:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:37:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/.0.07240f68-9118-458e-a242-a5476aa3bc61.tmp to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/0
24/06/26 14:37:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/0 using temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/.0.ef90789f-2a58-4eac-ad06-d61935ab9052.tmp
24/06/26 14:37:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "d1535517-1fb4-49a6-812a-60a186b51442",
  "runId" : "62776273-385d-4aa7-8dd6-01484d21d1b7",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:36:58.376Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1129,
    "commitOffsets" : 121,
    "getBatch" : 24,
    "latestOffset" : 1358,
    "queryPlanning" : 423,
    "triggerExecution" : 3166,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 14:37:01 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/.0.ef90789f-2a58-4eac-ad06-d61935ab9052.tmp to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/0
24/06/26 14:37:01 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "e83a9a52-73aa-4842-a2dd-46c535a7d5bc",
  "runId" : "f5a279a9-50fe-4b8e-aece-e4443c21398d",
  "name" : null,
  "timestamp" : "2024-06-26T14:36:58.920Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1252,
    "commitOffsets" : 75,
    "getBatch" : 24,
    "latestOffset" : 663,
    "queryPlanning" : 423,
    "triggerExecution" : 2697,
    "walCommit" : 98
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@66d8aa8d",
    "numOutputRows" : 0
  }
}
24/06/26 14:37:06 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:44679 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:37:06 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:44679 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:37:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:38:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:39:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:40:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/1 using temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/.1.1908a13b-3509-4c22-853c-b7446586a645.tmp
24/06/26 14:41:14 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/1 using temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/.1.6bcd3ed6-11bd-460e-a069-3fd6d3603e61.tmp
24/06/26 14:41:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/.1.6bcd3ed6-11bd-460e-a069-3fd6d3603e61.tmp to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/offsets/1
24/06/26 14:41:14 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719412874737,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:41:14 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/.1.1908a13b-3509-4c22-853c-b7446586a645.tmp to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/offsets/1
24/06/26 14:41:14 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719412874743,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:41:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6243d527]. The input RDD has 1 partitions.
24/06/26 14:41:14 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:41:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:41:14 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:41:14 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:41:14 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:41:14 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:41:14 INFO DAGScheduler: Missing parents: List()
24/06/26 14:41:14 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:41:15 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 14:41:15 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 14:41:15 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:44679 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:41:15 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 14:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[16] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:41:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 14:41:15 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:41:15 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:41:15 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:41:15 INFO DAGScheduler: Missing parents: List()
24/06/26 14:41:15 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:41:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/26 14:41:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 14:41:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 14:41:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:44679 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 14:41:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 14:41:15 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 14:41:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:41:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 14:41:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10569 bytes) 
24/06/26 14:41:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 14:41:15 INFO CodeGenerator: Code generated in 93.246452 ms
24/06/26 14:41:15 INFO CodeGenerator: Code generated in 13.982248 ms
24/06/26 14:41:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=250 untilOffset=253, for query queryId=e83a9a52-73aa-4842-a2dd-46c535a7d5bc batchId=1 taskId=3 partitionId=0
24/06/26 14:41:15 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=250 untilOffset=253, for query queryId=d1535517-1fb4-49a6-812a-60a186b51442 batchId=1 taskId=2 partitionId=0
24/06/26 14:41:15 INFO CodeGenerator: Code generated in 12.617938 ms
24/06/26 14:41:15 INFO CodeGenerator: Code generated in 27.551821 ms
24/06/26 14:41:15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 14:41:15 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 14:41:15 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:41:15 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:41:15 INFO AppInfoParser: Kafka startTimeMs: 1719412875524
24/06/26 14:41:15 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:41:15 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:41:15 INFO AppInfoParser: Kafka startTimeMs: 1719412875524
24/06/26 14:41:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Assigned to partition(s): logs-0
24/06/26 14:41:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Assigned to partition(s): logs-0
24/06/26 14:41:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Seeking to offset 250 for partition logs-0
24/06/26 14:41:15 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Seeking to offset 250 for partition logs-0
24/06/26 14:41:15 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 14:41:15 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 14:41:15 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 14:41:15 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 14:41:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Seeking to earliest offset of partition logs-0
24/06/26 14:41:15 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Seeking to earliest offset of partition logs-0
24/06/26 14:41:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:41:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:41:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Seeking to latest offset of partition logs-0
24/06/26 14:41:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Seeking to latest offset of partition logs-0
24/06/26 14:41:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=253, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:41:16 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=253, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:41:16 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:41:16 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 14:41:16 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor read 3 records through 1 polls (polled  out 3 records), taking 646733769 nanos, during time span of 793796705 nanos.
24/06/26 14:41:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2420 bytes result sent to driver
24/06/26 14:41:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1298 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:41:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 14:41:16 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.317 s
24/06/26 14:41:16 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:41:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 14:41:16 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.396005 s
24/06/26 14:41:16 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 14:41:16 INFO CodeGenerator: Code generated in 6.984558 ms
24/06/26 14:41:16 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:44679 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 14:41:17 INFO CodeGenerator: Code generated in 12.377562 ms
24/06/26 14:41:17 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:41:17 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 14:41:17 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor read 3 records through 1 polls (polled  out 3 records), taking 645395032 nanos, during time span of 1833359994 nanos.
24/06/26 14:41:17 INFO CodeGenerator: Code generated in 24.218199 ms
24/06/26 14:41:17 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3345 bytes result sent to driver
+-------------------+---------+--------------------+
|          timestamp|log_level|             message|
+-------------------+---------+--------------------+
|2023-06-25T12:00:00|    ERROR|     Service started|
|2023-06-25T12:01:00|  WARNING|High memory usage...|
|2023-06-25T12:02:00|    ERROR|Failed to connect...|
+-------------------+---------+--------------------+

24/06/26 14:41:17 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2347 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:41:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:41:17 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 14:41:17 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 2.404 s
24/06/26 14:41:17 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:41:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 14:41:17 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 2.421131 s
24/06/26 14:41:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6243d527] is committing.
24/06/26 14:41:17 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6243d527] committed.
24/06/26 14:41:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/1 using temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/.1.0eecb10e-6a5a-479d-89c4-36bb444e5604.tmp
24/06/26 14:41:17 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/1 using temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/.1.1f8d487c-de29-4d71-a046-f272a428d4cb.tmp
24/06/26 14:41:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/.1.0eecb10e-6a5a-479d-89c4-36bb444e5604.tmp to file:/tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214/commits/1
24/06/26 14:41:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "e83a9a52-73aa-4842-a2dd-46c535a7d5bc",
  "runId" : "f5a279a9-50fe-4b8e-aece-e4443c21398d",
  "name" : null,
  "timestamp" : "2024-06-26T14:41:14.734Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 187.5,
  "processedRowsPerSecond" : 1.102941176470588,
  "durationMs" : {
    "addBatch" : 2484,
    "commitOffsets" : 68,
    "getBatch" : 0,
    "latestOffset" : 9,
    "queryPlanning" : 20,
    "triggerExecution" : 2720,
    "walCommit" : 136
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 187.5,
    "processedRowsPerSecond" : 1.102941176470588,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@66d8aa8d",
    "numOutputRows" : 3
  }
}
24/06/26 14:41:17 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/.1.1f8d487c-de29-4d71-a046-f272a428d4cb.tmp to file:/tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd/commits/1
24/06/26 14:41:17 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "d1535517-1fb4-49a6-812a-60a186b51442",
  "runId" : "62776273-385d-4aa7-8dd6-01484d21d1b7",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:41:14.732Z",
  "batchId" : 1,
  "numInputRows" : 3,
  "inputRowsPerSecond" : 166.66666666666669,
  "processedRowsPerSecond" : 1.094890510948905,
  "durationMs" : {
    "addBatch" : 2490,
    "commitOffsets" : 80,
    "getBatch" : 1,
    "latestOffset" : 5,
    "queryPlanning" : 28,
    "triggerExecution" : 2740,
    "walCommit" : 135
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 250
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "numInputRows" : 3,
    "inputRowsPerSecond" : 166.66666666666669,
    "processedRowsPerSecond" : 1.094890510948905,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 3
  }
}
24/06/26 14:41:23 INFO CodeGenerator: Code generated in 42.01827 ms
24/06/26 14:41:23 INFO CodeGenerator: Code generated in 10.374579 ms
[Row(timestamp='2023-06-25T12:00:00', log_level='ERROR', message='Service started'), Row(timestamp='2023-06-25T12:01:00', log_level='WARNING', message='High memory usage detected'), Row(timestamp='2023-06-25T12:02:00', log_level='ERROR', message='Failed to connect to database')]
[{'timestamp': '2023-06-25T12:00:00', 'log_level': 'ERROR', 'message': 'Service started'}, {'timestamp': '2023-06-25T12:01:00', 'log_level': 'WARNING', 'message': 'High memory usage detected'}, {'timestamp': '2023-06-25T12:02:00', 'log_level': 'ERROR', 'message': 'Failed to connect to database'}]
127.0.0.1 - - [26/Jun/2024 14:41:23] "GET /logs HTTP/1.1" 200 -
24/06/26 14:41:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:41:59 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 14:41:59 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
[Row(timestamp='2023-06-25T12:00:00', log_level='ERROR', message='Service started'), Row(timestamp='2023-06-25T12:01:00', log_level='WARNING', message='High memory usage detected'), Row(timestamp='2023-06-25T12:02:00', log_level='ERROR', message='Failed to connect to database')]
[{'timestamp': '2023-06-25T12:00:00', 'log_level': 'ERROR', 'message': 'Service started'}, {'timestamp': '2023-06-25T12:01:00', 'log_level': 'WARNING', 'message': 'High memory usage detected'}, {'timestamp': '2023-06-25T12:02:00', 'log_level': 'ERROR', 'message': 'Failed to connect to database'}]
127.0.0.1 - - [26/Jun/2024 14:42:01] "GET /logs HTTP/1.1" 200 -
24/06/26 14:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[Row(timestamp='2023-06-25T12:00:00', log_level='ERROR', message='Service started'), Row(timestamp='2023-06-25T12:01:00', log_level='WARNING', message='High memory usage detected'), Row(timestamp='2023-06-25T12:02:00', log_level='ERROR', message='Failed to connect to database')]
[{'timestamp': '2023-06-25T12:00:00', 'log_level': 'ERROR', 'message': 'Service started'}, {'timestamp': '2023-06-25T12:01:00', 'log_level': 'WARNING', 'message': 'High memory usage detected'}, {'timestamp': '2023-06-25T12:02:00', 'log_level': 'ERROR', 'message': 'Failed to connect to database'}]
127.0.0.1 - - [26/Jun/2024 14:42:29] "GET /logs HTTP/1.1" 200 -
24/06/26 14:42:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:42:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:43:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:00 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:44679 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:44:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:44:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:45:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
[Row(timestamp='2023-06-25T12:00:00', log_level='ERROR', message='Service started'), Row(timestamp='2023-06-25T12:01:00', log_level='WARNING', message='High memory usage detected'), Row(timestamp='2023-06-25T12:02:00', log_level='ERROR', message='Failed to connect to database')]
[{'timestamp': '2023-06-25T12:00:00', 'log_level': 'ERROR', 'message': 'Service started'}, {'timestamp': '2023-06-25T12:01:00', 'log_level': 'WARNING', 'message': 'High memory usage detected'}, {'timestamp': '2023-06-25T12:02:00', 'log_level': 'ERROR', 'message': 'Failed to connect to database'}]
127.0.0.1 - - [26/Jun/2024 14:46:23] "GET /logs HTTP/1.1" 200 -
24/06/26 14:46:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:46:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 14:47:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2, groupId=spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 14:47:15 INFO Metrics: Metrics scheduler closed
24/06/26 14:47:15 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 14:47:15 INFO Metrics: Metrics reporters closed
24/06/26 14:47:15 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-565a50fe-5346-41b8-be77-6d51eced0012-280110568-executor-2 unregistered
24/06/26 14:47:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 14:47:15 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1, groupId=spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 14:47:15 INFO Metrics: Metrics scheduler closed
24/06/26 14:47:15 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 14:47:15 INFO Metrics: Metrics reporters closed
24/06/26 14:47:15 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-fe666e14-31d7-455b-9ac4-41a756d0b648-595077049-executor-1 unregistered
24/06/26 14:47:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:47:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:37 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:47 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:48:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:27 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:49:31 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 14:49:31 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 14:49:31 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 14:49:31 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 14:49:31 INFO MemoryStore: MemoryStore cleared
24/06/26 14:49:31 INFO BlockManager: BlockManager stopped
24/06/26 14:49:31 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 14:49:31 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 14:49:31 INFO SparkContext: Successfully stopped SparkContext
24/06/26 14:49:31 INFO ShutdownHookManager: Shutdown hook called
24/06/26 14:49:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394
24/06/26 14:49:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-da2f7fcb-31a4-45e1-8cbe-60c7b44d4394/pyspark-6157e8c0-778f-462f-97bd-34e1ecfa622b
24/06/26 14:49:31 INFO ShutdownHookManager: Deleting directory /tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214
24/06/26 14:49:31 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214. Falling back to Java IO way
java.io.IOException: Failed to delete: /tmp/temporary-324d6423-26d3-487c-ae90-79b9df5ef214
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:173)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)
	at org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
24/06/26 14:49:31 INFO ShutdownHookManager: Deleting directory /tmp/temporary-2c2f5e75-9bb2-4c9b-bb17-17d4a5ce23dd
24/06/26 14:49:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-7134ac99-b246-4a67-96fa-4a4e330702b9
Starting Spark at Wed Jun 26 02:51:59 PM UTC 2024
24/06/26 14:52:01 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 14:52:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-a0a21709-fdda-49c8-af08-b3a63a9f5af9;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 537ms :: artifacts dl 27ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-a0a21709-fdda-49c8-af08-b3a63a9f5af9
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/11ms)
24/06/26 14:52:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 14:52:04 INFO SparkContext: Running Spark version 3.5.1
24/06/26 14:52:04 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:52:04 INFO SparkContext: Java version 11.0.20.1
24/06/26 14:52:04 INFO ResourceUtils: ==============================================================
24/06/26 14:52:04 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 14:52:04 INFO ResourceUtils: ==============================================================
24/06/26 14:52:04 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 14:52:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 14:52:04 INFO ResourceProfile: Limiting resource is cpu
24/06/26 14:52:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 14:52:04 INFO SecurityManager: Changing view acls to: root
24/06/26 14:52:04 INFO SecurityManager: Changing modify acls to: root
24/06/26 14:52:04 INFO SecurityManager: Changing view acls groups to: 
24/06/26 14:52:04 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 14:52:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 14:52:04 INFO Utils: Successfully started service 'sparkDriver' on port 37067.
24/06/26 14:52:04 INFO SparkEnv: Registering MapOutputTracker
24/06/26 14:52:04 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 14:52:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 14:52:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 14:52:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 14:52:04 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b07871e9-81d9-478c-af74-cc0a9a794528
24/06/26 14:52:04 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 14:52:04 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 14:52:04 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 14:52:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:37067/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:37067/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:37067/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:37067/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:37067/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:37067/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:37067/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:37067/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:37067/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:37067/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:37067/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:52:05 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:52:05 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 14:52:05 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 14:52:05 INFO Executor: Java version 11.0.20.1
24/06/26 14:52:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 14:52:05 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2cadf1b2 for default.
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:52:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:37067 after 33 ms (0 ms spent in bootstraps)
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp17219212084520086720.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp17219212084520086720.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp7495658500568347507.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp7495658500568347507.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp9288146486831601760.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp9288146486831601760.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp17379677651275229673.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp17379677651275229673.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp5685927173295735524.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp5685927173295735524.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp7215682114504679995.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp7215682114504679995.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp8878472251698985563.tmp
24/06/26 14:52:05 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp8878472251698985563.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.lz4_lz4-java-1.8.0.jar
24/06/26 14:52:05 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 14:52:05 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719413524057
24/06/26 14:52:05 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp4400915062493310764.tmp
24/06/26 14:52:06 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp4400915062493310764.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 14:52:06 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 14:52:06 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719413524057
24/06/26 14:52:06 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp3529617209827098248.tmp
24/06/26 14:52:06 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp3529617209827098248.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 14:52:06 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 14:52:06 INFO Executor: Fetching spark://10.0.2.15:37067/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719413524057
24/06/26 14:52:06 INFO Utils: Fetching spark://10.0.2.15:37067/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp17397877801825880255.tmp
24/06/26 14:52:06 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp17397877801825880255.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/commons-logging_commons-logging-1.1.3.jar
24/06/26 14:52:06 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 14:52:06 INFO Executor: Fetching spark://10.0.2.15:37067/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719413524057
24/06/26 14:52:06 INFO Utils: Fetching spark://10.0.2.15:37067/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp214274204061813431.tmp
24/06/26 14:52:06 INFO Utils: /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/fetchFileTemp214274204061813431.tmp has been previously copied to /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 14:52:06 INFO Executor: Adding file:/tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/userFiles-f0e72d8a-4f70-4c46-9db8-e55208dcfb57/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 14:52:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43025.
24/06/26 14:52:06 INFO NettyBlockTransferService: Server created on 10.0.2.15:43025
24/06/26 14:52:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 14:52:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 43025, None)
24/06/26 14:52:06 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:43025 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 43025, None)
24/06/26 14:52:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 43025, None)
24/06/26 14:52:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 43025, None)
24/06/26 14:52:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 14:52:06 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 14:52:08 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 14:52:09 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:52:09 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3 resolved to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3.
24/06/26 14:52:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:52:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/metadata using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/.metadata.a4238eee-766a-4b42-92f2-1db1b2624918.tmp
24/06/26 14:52:09 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/.metadata.a4238eee-766a-4b42-92f2-1db1b2624918.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/metadata
24/06/26 14:52:09 INFO MicroBatchExecution: Starting logs_table [id = ecad1c35-6842-4a93-acfd-d8c042165d5d, runId = d486de34-5908-4a59-971f-8aeba1a52e9f]. Use file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3 to store the query checkpoint.
24/06/26 14:52:09 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@661a5112] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@249f0545]
24/06/26 14:52:09 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:52:09 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:52:09 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:52:09 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:52:09 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 14:52:09 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd resolved to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd.
24/06/26 14:52:09 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 14:52:09 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/metadata using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/.metadata.84535ee4-4f08-4e1f-a3dc-d848d71f3dc0.tmp
24/06/26 14:52:09 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:52:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/.metadata.84535ee4-4f08-4e1f-a3dc-d848d71f3dc0.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/metadata
24/06/26 14:52:10 INFO MicroBatchExecution: Starting [id = 2e38d082-5107-42c5-a4ff-1dc6f6390043, runId = 0a3fd71d-9ed6-491e-b4ae-b3bb814a3930]. Use file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd to store the query checkpoint.
24/06/26 14:52:10 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@661a5112] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@249f0545]
24/06/26 14:52:10 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:52:10 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 14:52:10 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 14:52:10 INFO MicroBatchExecution: Stream started from {}
24/06/26 14:52:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:52:10 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:52:10 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:52:10 INFO AppInfoParser: Kafka startTimeMs: 1719413530143
24/06/26 14:52:10 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 14:52:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 14:52:10 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:52:10 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:52:10 INFO AppInfoParser: Kafka startTimeMs: 1719413530166
 * Serving Flask app 'spark_kafka_consumer'
 * Debug mode: off
24/06/26 14:52:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/sources/0/0 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/sources/0/.0.ef51af67-839e-4127-8ef8-62a1cbe2c41c.tmp
24/06/26 14:52:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/sources/0/0 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/sources/0/.0.819809a3-1d62-4939-8105-0eb8ddcd99ce.tmp
24/06/26 14:52:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/sources/0/.0.819809a3-1d62-4939-8105-0eb8ddcd99ce.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/sources/0/0
24/06/26 14:52:10 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":253}}
24/06/26 14:52:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/0 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.0.cc6ba850-86ba-43a0-b795-d76f40029044.tmp
24/06/26 14:52:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/sources/0/.0.ef51af67-839e-4127-8ef8-62a1cbe2c41c.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/sources/0/0
24/06/26 14:52:10 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":253}}
24/06/26 14:52:10 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/0 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.0.f975cb50-28e6-4d63-aea8-51715f1218cf.tmp
24/06/26 14:52:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.0.cc6ba850-86ba-43a0-b795-d76f40029044.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/0
24/06/26 14:52:10 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719413530782,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:52:10 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.0.f975cb50-28e6-4d63-aea8-51715f1218cf.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/0
24/06/26 14:52:10 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719413530831,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 14:52:11 INFO CodeGenerator: Code generated in 304.004676 ms
24/06/26 14:52:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:52:11 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a5a012a]. The input RDD has 1 partitions.
24/06/26 14:52:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:52:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:52:11 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:52:11 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:52:11 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:52:11 INFO DAGScheduler: Missing parents: List()
24/06/26 14:52:11 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:52:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 14:52:12 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 14:52:12 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:43025 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:52:12 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 14:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:52:12 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 14:52:12 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:52:12 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:52:12 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:52:12 INFO DAGScheduler: Missing parents: List()
24/06/26 14:52:12 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:52:12 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 14:52:12 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 14:52:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:43025 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:52:12 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 14:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:52:12 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 14:52:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:52:12 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 14:52:12 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 14:52:12 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 14:52:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:52:12 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 14:52:12 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:52:12 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 14:52:12 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1396 bytes result sent to driver
24/06/26 14:52:12 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1295 bytes result sent to driver
24/06/26 14:52:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 228 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:52:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 14:52:12 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 290 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:52:12 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 14:52:12 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.324 s
24/06/26 14:52:12 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:52:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 14:52:12 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.672035 s
24/06/26 14:52:12 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.614 s
24/06/26 14:52:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:52:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a5a012a] is committing.
24/06/26 14:52:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6a5a012a] committed.
24/06/26 14:52:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 14:52:12 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.687056 s
24/06/26 14:52:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 14:52:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/0 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.0.2d6a8376-f018-4947-902b-6986a78e9e86.tmp
24/06/26 14:52:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.0.2d6a8376-f018-4947-902b-6986a78e9e86.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/0
+---------+---------+-------+
|timestamp|log_level|message|
+---------+---------+-------+
+---------+---------+-------+

24/06/26 14:52:12 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:52:12 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/0 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.0.a7a29bc3-f32d-45d2-87d5-c0ceac9033b7.tmp
24/06/26 14:52:12 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.0.a7a29bc3-f32d-45d2-87d5-c0ceac9033b7.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/0
24/06/26 14:52:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2e38d082-5107-42c5-a4ff-1dc6f6390043",
  "runId" : "0a3fd71d-9ed6-491e-b4ae-b3bb814a3930",
  "name" : null,
  "timestamp" : "2024-06-26T14:52:10.121Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1556,
    "commitOffsets" : 62,
    "getBatch" : 2,
    "latestOffset" : 702,
    "queryPlanning" : 233,
    "triggerExecution" : 2678,
    "walCommit" : 111
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@660b95d2",
    "numOutputRows" : 0
  }
}
24/06/26 14:52:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ecad1c35-6842-4a93-acfd-d8c042165d5d",
  "runId" : "d486de34-5908-4a59-971f-8aeba1a52e9f",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:52:09.406Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1450,
    "commitOffsets" : 94,
    "getBatch" : 31,
    "latestOffset" : 1343,
    "queryPlanning" : 263,
    "triggerExecution" : 3325,
    "walCommit" : 89
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 14:52:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:43025 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 14:52:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:43025 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 14:52:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:52:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 14:52:31] "GET / HTTP/1.1" 404 -
24/06/26 14:52:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:52:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 14:52:34] "GET /logs HTTP/1.1" 200 -
24/06/26 14:52:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:52:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:52:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:52:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:52:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/1 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.1.17a042ee-1ebe-4323-b245-1b329f57667b.tmp
24/06/26 14:52:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/1 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.1.c69c8a8b-8cf8-400a-bc51-80b2c3f7e419.tmp
24/06/26 14:52:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.1.17a042ee-1ebe-4323-b245-1b329f57667b.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/1
24/06/26 14:52:53 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719413573549,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:52:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.1.c69c8a8b-8cf8-400a-bc51-80b2c3f7e419.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/1
24/06/26 14:52:53 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719413573554,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@eb8e980]. The input RDD has 1 partitions.
24/06/26 14:52:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:52:53 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:52:53 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:52:53 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:52:53 INFO DAGScheduler: Missing parents: List()
24/06/26 14:52:53 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:52:53 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 14:52:53 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:52:53 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 14:52:53 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 14:52:53 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:43025 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:52:53 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 14:52:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:52:53 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 14:52:53 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:52:53 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:52:53 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:52:53 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 14:52:53 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 14:52:53 INFO DAGScheduler: Missing parents: List()
24/06/26 14:52:53 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:52:53 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 14:52:53 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 14:52:53 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:43025 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 14:52:53 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 14:52:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:52:53 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 14:52:53 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 14:52:53 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 14:52:53 INFO CodeGenerator: Code generated in 48.833752 ms
24/06/26 14:52:53 INFO CodeGenerator: Code generated in 13.74058 ms
24/06/26 14:52:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=253 untilOffset=272, for query queryId=2e38d082-5107-42c5-a4ff-1dc6f6390043 batchId=1 taskId=3 partitionId=0
24/06/26 14:52:53 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=253 untilOffset=254, for query queryId=ecad1c35-6842-4a93-acfd-d8c042165d5d batchId=1 taskId=2 partitionId=0
24/06/26 14:52:54 INFO CodeGenerator: Code generated in 20.464697 ms
24/06/26 14:52:54 INFO CodeGenerator: Code generated in 22.813206 ms
24/06/26 14:52:54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 14:52:54 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 14:52:54 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:52:54 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:52:54 INFO AppInfoParser: Kafka startTimeMs: 1719413574161
24/06/26 14:52:54 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 14:52:54 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 14:52:54 INFO AppInfoParser: Kafka startTimeMs: 1719413574161
24/06/26 14:52:54 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Assigned to partition(s): logs-0
24/06/26 14:52:54 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Assigned to partition(s): logs-0
24/06/26 14:52:54 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to offset 253 for partition logs-0
24/06/26 14:52:54 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to offset 253 for partition logs-0
24/06/26 14:52:54 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 14:52:54 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 14:52:54 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 14:52:54 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to earliest offset of partition logs-0
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to earliest offset of partition logs-0
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to latest offset of partition logs-0
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to latest offset of partition logs-0
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=272, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:52:54 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=272, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 14:52:54 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:52:54 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 14:52:54 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor read 19 records through 1 polls (polled  out 19 records), taking 594822274 nanos, during time span of 741444593 nanos.
24/06/26 14:52:54 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 4470 bytes result sent to driver
24/06/26 14:52:54 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1136 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:52:54 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 14:52:54 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.180 s
24/06/26 14:52:54 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:52:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 14:52:54 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.220507 s
24/06/26 14:52:54 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 14:52:55 INFO CodeGenerator: Code generated in 18.716965 ms
24/06/26 14:52:55 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:43025 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 14:52:55 INFO CodeGenerator: Code generated in 12.390718 ms
24/06/26 14:52:55 INFO CodeGenerator: Code generated in 18.664239 ms
24/06/26 14:52:55 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:52:55 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
|     NULL|     NULL|2024/06/26 14:52:...|
+---------+---------+--------------------+
24/06/26 14:52:55 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor read 1 records through 1 polls (polled  out 19 records), taking 592416374 nanos, during time span of 1803162460 nanos.

24/06/26 14:52:55 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 14:52:56 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3235 bytes result sent to driver
24/06/26 14:52:56 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2308 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:52:56 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 14:52:56 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 2.382 s
24/06/26 14:52:56 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:52:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 14:52:56 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 2.398946 s
24/06/26 14:52:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@eb8e980] is committing.
24/06/26 14:52:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@eb8e980] committed.
24/06/26 14:52:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/1 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.1.10844a86-b0db-4867-a9d0-6c7a74c87500.tmp
24/06/26 14:52:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/1 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.1.a1af8690-d1f0-49dd-a0b2-8cb06ea29ed5.tmp
24/06/26 14:52:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.1.10844a86-b0db-4867-a9d0-6c7a74c87500.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/1
24/06/26 14:52:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2e38d082-5107-42c5-a4ff-1dc6f6390043",
  "runId" : "0a3fd71d-9ed6-491e-b4ae-b3bb814a3930",
  "name" : null,
  "timestamp" : "2024-06-26T14:52:53.548Z",
  "batchId" : 1,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 950.0,
  "processedRowsPerSecond" : 7.276905400229796,
  "durationMs" : {
    "addBatch" : 2341,
    "commitOffsets" : 169,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 26,
    "triggerExecution" : 2611,
    "walCommit" : 69
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 272
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 272
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 950.0,
    "processedRowsPerSecond" : 7.276905400229796,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@660b95d2",
    "numOutputRows" : 19
  }
}
24/06/26 14:52:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.1.a1af8690-d1f0-49dd-a0b2-8cb06ea29ed5.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/1
24/06/26 14:52:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ecad1c35-6842-4a93-acfd-d8c042165d5d",
  "runId" : "d486de34-5908-4a59-971f-8aeba1a52e9f",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:52:53.529Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 55.55555555555556,
  "processedRowsPerSecond" : 0.3779289493575208,
  "durationMs" : {
    "addBatch" : 2447,
    "commitOffsets" : 81,
    "getBatch" : 1,
    "latestOffset" : 20,
    "queryPlanning" : 23,
    "triggerExecution" : 2646,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 253
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 254
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 254
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 55.55555555555556,
    "processedRowsPerSecond" : 0.3779289493575208,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 14:52:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/2 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.2.f9f66953-c031-4c5f-9e7c-090dce4424a1.tmp
24/06/26 14:52:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.2.f9f66953-c031-4c5f-9e7c-090dce4424a1.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/2
24/06/26 14:52:56 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719413576186,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 14:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:56 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 14:52:56 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@38f055c7]. The input RDD has 1 partitions.
24/06/26 14:52:56 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 14:52:56 INFO DAGScheduler: Got job 4 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 14:52:56 INFO DAGScheduler: Final stage: ResultStage 4 (start at NativeMethodAccessorImpl.java:0)
24/06/26 14:52:56 INFO DAGScheduler: Parents of final stage: List()
24/06/26 14:52:56 INFO DAGScheduler: Missing parents: List()
24/06/26 14:52:56 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 14:52:56 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/26 14:52:56 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/26 14:52:56 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:43025 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:52:56 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 14:52:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 14:52:56 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 14:52:56 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 14:52:56 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 14:52:56 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=254 untilOffset=272, for query queryId=ecad1c35-6842-4a93-acfd-d8c042165d5d batchId=2 taskId=4 partitionId=0
24/06/26 14:52:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 14:52:56 INFO DataWritingSparkTask: Committed partition 0 (task 4, attempt 0, stage 4.0)
24/06/26 14:52:56 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor read 18 records through 0 polls (polled  out 0 records), taking 0 nanos, during time span of 24142387 nanos.
24/06/26 14:52:56 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 4754 bytes result sent to driver
24/06/26 14:52:56 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 84 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 14:52:56 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 14:52:56 INFO DAGScheduler: ResultStage 4 (start at NativeMethodAccessorImpl.java:0) finished in 0.112 s
24/06/26 14:52:56 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 14:52:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/26 14:52:56 INFO DAGScheduler: Job 4 finished: start at NativeMethodAccessorImpl.java:0, took 0.126818 s
24/06/26 14:52:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@38f055c7] is committing.
24/06/26 14:52:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@38f055c7] committed.
24/06/26 14:52:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/2 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.2.a1a96bbd-ad04-4c16-a109-25dc64ffb278.tmp
24/06/26 14:52:56 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:43025 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:52:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.2.a1a96bbd-ad04-4c16-a109-25dc64ffb278.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/2
24/06/26 14:52:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ecad1c35-6842-4a93-acfd-d8c042165d5d",
  "runId" : "d486de34-5908-4a59-971f-8aeba1a52e9f",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T14:52:56.178Z",
  "batchId" : 2,
  "numInputRows" : 18,
  "inputRowsPerSecond" : 6.795016987542469,
  "processedRowsPerSecond" : 46.51162790697674,
  "durationMs" : {
    "addBatch" : 191,
    "commitOffsets" : 104,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 20,
    "triggerExecution" : 386,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 254
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 272
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 272
      }
    },
    "numInputRows" : 18,
    "inputRowsPerSecond" : 6.795016987542469,
    "processedRowsPerSecond" : 46.51162790697674,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 18
  }
}
24/06/26 14:53:02 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:43025 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 14:53:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:53:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:54:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:55:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:56:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:10 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 14:57:10 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 14:57:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:57:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 14:58:53 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 14:58:53 INFO Metrics: Metrics scheduler closed
24/06/26 14:58:53 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 14:58:53 INFO Metrics: Metrics reporters closed
24/06/26 14:58:54 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-2 unregistered
24/06/26 14:58:54 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 14:58:54 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 14:58:54 INFO Metrics: Metrics scheduler closed
24/06/26 14:58:54 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 14:58:54 INFO Metrics: Metrics reporters closed
24/06/26 14:58:54 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-1 unregistered
24/06/26 14:58:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:58:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 14:59:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:36 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:46 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:00:56 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/3 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.3.cda7e74b-5586-4168-a759-cff146335665.tmp
24/06/26 15:01:03 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/2 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.2.ecbdd627-cf8b-4517-b0bd-578856b6c9d9.tmp
24/06/26 15:01:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.3.cda7e74b-5586-4168-a759-cff146335665.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/3
24/06/26 15:01:03 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719414063619,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:01:03 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.2.ecbdd627-cf8b-4517-b0bd-578856b6c9d9.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/2
24/06/26 15:01:03 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1719414063620,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:01:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:01:03 INFO DAGScheduler: Got job 5 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:01:03 INFO DAGScheduler: Final stage: ResultStage 5 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:01:03 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:01:03 INFO DAGScheduler: Missing parents: List()
24/06/26 15:01:03 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:01:03 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 22.4 KiB, free 434.4 MiB)
24/06/26 15:01:03 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.4 MiB)
24/06/26 15:01:03 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:43025 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:01:03 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 15:01:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:01:03 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/26 15:01:03 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6004fe33]. The input RDD has 1 partitions.
24/06/26 15:01:03 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:01:03 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:01:03 INFO DAGScheduler: Got job 6 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:01:03 INFO DAGScheduler: Final stage: ResultStage 6 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:01:03 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:01:03 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/26 15:01:03 INFO DAGScheduler: Missing parents: List()
24/06/26 15:01:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:01:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 22.5 KiB, free 434.3 MiB)
24/06/26 15:01:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=272 untilOffset=291, for query queryId=2e38d082-5107-42c5-a4ff-1dc6f6390043 batchId=2 taskId=5 partitionId=0
24/06/26 15:01:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.3 MiB)
24/06/26 15:01:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:43025 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:01:03 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
24/06/26 15:01:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[29] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:01:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/26 15:01:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:01:03 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:01:03 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:01:03 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:01:03 INFO AppInfoParser: Kafka startTimeMs: 1719414063876
24/06/26 15:01:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Assigned to partition(s): logs-0
24/06/26 15:01:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to offset 272 for partition logs-0
24/06/26 15:01:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
24/06/26 15:01:03 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:01:03 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:01:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:01:03 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=272 untilOffset=291, for query queryId=ecad1c35-6842-4a93-acfd-d8c042165d5d batchId=3 taskId=6 partitionId=0
24/06/26 15:01:03 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:01:03 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:01:03 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:01:03 INFO AppInfoParser: Kafka startTimeMs: 1719414063943
24/06/26 15:01:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Assigned to partition(s): logs-0
24/06/26 15:01:03 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to offset 272 for partition logs-0
24/06/26 15:01:03 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:01:03 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:01:03 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:01:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to latest offset of partition logs-0
24/06/26 15:01:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=291, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:01:04 INFO DataWritingSparkTask: Committed partition 0 (task 5, attempt 0, stage 5.0)
24/06/26 15:01:04 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor read 19 records through 1 polls (polled  out 19 records), taking 537119780 nanos, during time span of 544590275 nanos.
24/06/26 15:01:04 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 4470 bytes result sent to driver
24/06/26 15:01:04 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 643 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:01:04 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/26 15:01:04 INFO DAGScheduler: ResultStage 5 (start at NativeMethodAccessorImpl.java:0) finished in 0.678 s
24/06/26 15:01:04 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:01:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
24/06/26 15:01:04 INFO DAGScheduler: Job 5 finished: start at NativeMethodAccessorImpl.java:0, took 0.691167 s
24/06/26 15:01:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 2
-------------------------------------------
24/06/26 15:01:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to latest offset of partition logs-0
24/06/26 15:01:04 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=291, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:04 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:01:04 INFO DataWritingSparkTask: Committed partition 0 (task 6, attempt 0, stage 6.0)
24/06/26 15:01:04 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor read 19 records through 1 polls (polled  out 19 records), taking 522964266 nanos, during time span of 537109257 nanos.
24/06/26 15:01:04 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 4921 bytes result sent to driver
24/06/26 15:01:04 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 625 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:01:04 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/26 15:01:04 INFO DAGScheduler: ResultStage 6 (start at NativeMethodAccessorImpl.java:0) finished in 0.671 s
24/06/26 15:01:04 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:01:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/26 15:01:04 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:0, took 0.694411 s
24/06/26 15:01:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6004fe33] is committing.
24/06/26 15:01:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6004fe33] committed.
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|10-listen-on-ipv6...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|/docker-entrypoin...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
|     NULL|     NULL|2024/06/26 15:01:...|
+---------+---------+--------------------+

24/06/26 15:01:04 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:01:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/2 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.2.e5db2eea-d0a9-400c-9dcc-ee1ec13dc537.tmp
24/06/26 15:01:04 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/3 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.3.2edff3bb-92ea-4a7f-bb08-262879475319.tmp
24/06/26 15:01:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.2.e5db2eea-d0a9-400c-9dcc-ee1ec13dc537.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/2
24/06/26 15:01:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2e38d082-5107-42c5-a4ff-1dc6f6390043",
  "runId" : "0a3fd71d-9ed6-491e-b4ae-b3bb814a3930",
  "name" : null,
  "timestamp" : "2024-06-26T15:01:03.613Z",
  "batchId" : 2,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 1357.142857142857,
  "processedRowsPerSecond" : 19.40755873340143,
  "durationMs" : {
    "addBatch" : 817,
    "commitOffsets" : 66,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 16,
    "triggerExecution" : 979,
    "walCommit" : 73
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 272
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 291
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 291
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 1357.142857142857,
    "processedRowsPerSecond" : 19.40755873340143,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@660b95d2",
    "numOutputRows" : 19
  }
}
24/06/26 15:01:04 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.3.2edff3bb-92ea-4a7f-bb08-262879475319.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/3
24/06/26 15:01:04 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ecad1c35-6842-4a93-acfd-d8c042165d5d",
  "runId" : "d486de34-5908-4a59-971f-8aeba1a52e9f",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:01:03.613Z",
  "batchId" : 3,
  "numInputRows" : 19,
  "inputRowsPerSecond" : 1357.142857142857,
  "processedRowsPerSecond" : 19.34826883910387,
  "durationMs" : {
    "addBatch" : 786,
    "commitOffsets" : 103,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 20,
    "triggerExecution" : 982,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 272
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 291
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 291
      }
    },
    "numInputRows" : 19,
    "inputRowsPerSecond" : 1357.142857142857,
    "processedRowsPerSecond" : 19.34826883910387,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 19
  }
}
24/06/26 15:01:10 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:43025 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:01:10 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:43025 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:01:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/3 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.3.66343300-ecb6-4830-b0d1-2b6cdb1ad52d.tmp
24/06/26 15:01:48 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/4 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.4.eaf321a5-c040-4119-9b1c-7bdc3e2800eb.tmp
24/06/26 15:01:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.3.66343300-ecb6-4830-b0d1-2b6cdb1ad52d.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/3
24/06/26 15:01:48 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1719414108625,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:01:48 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.4.eaf321a5-c040-4119-9b1c-7bdc3e2800eb.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/4
24/06/26 15:01:48 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719414108630,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:01:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@79afdff]. The input RDD has 1 partitions.
24/06/26 15:01:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:01:48 INFO DAGScheduler: Got job 7 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:01:48 INFO DAGScheduler: Final stage: ResultStage 7 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:01:48 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:01:48 INFO DAGScheduler: Missing parents: List()
24/06/26 15:01:48 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[33] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:01:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 15:01:48 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:01:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 15:01:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:43025 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:01:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
24/06/26 15:01:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[33] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:01:48 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/06/26 15:01:48 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:01:48 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:01:48 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
24/06/26 15:01:48 INFO DAGScheduler: Got job 8 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:01:48 INFO DAGScheduler: Final stage: ResultStage 8 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:01:48 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:01:48 INFO DAGScheduler: Missing parents: List()
24/06/26 15:01:48 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:01:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 15:01:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 15:01:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.2.15:43025 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:01:48 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
24/06/26 15:01:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:01:48 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
24/06/26 15:01:48 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:01:48 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
24/06/26 15:01:48 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=291 untilOffset=292, for query queryId=ecad1c35-6842-4a93-acfd-d8c042165d5d batchId=4 taskId=7 partitionId=0
24/06/26 15:01:48 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=291 untilOffset=292, for query queryId=2e38d082-5107-42c5-a4ff-1dc6f6390043 batchId=3 taskId=8 partitionId=0
24/06/26 15:01:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to offset 291 for partition logs-0
24/06/26 15:01:48 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to offset 291 for partition logs-0
24/06/26 15:01:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:01:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:01:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to latest offset of partition logs-0
24/06/26 15:01:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to latest offset of partition logs-0
24/06/26 15:01:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=292, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:49 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=292, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:01:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:01:49 INFO DataWritingSparkTask: Committed partition 0 (task 7, attempt 0, stage 7.0)
24/06/26 15:01:49 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor read 1 records through 1 polls (polled  out 1 records), taking 513029305 nanos, during time span of 519080125 nanos.
24/06/26 15:01:49 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:01:49 INFO DataWritingSparkTask: Committed partition 0 (task 8, attempt 0, stage 8.0)
24/06/26 15:01:49 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor read 1 records through 1 polls (polled  out 1 records), taking 516682093 nanos, during time span of 534054465 nanos.
24/06/26 15:01:49 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 3229 bytes result sent to driver
24/06/26 15:01:49 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 608 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:01:49 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/06/26 15:01:49 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2199 bytes result sent to driver
24/06/26 15:01:49 INFO DAGScheduler: ResultStage 7 (start at NativeMethodAccessorImpl.java:0) finished in 0.636 s
24/06/26 15:01:49 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:01:49 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 560 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:01:49 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
24/06/26 15:01:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/06/26 15:01:49 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:0) finished in 0.587 s
24/06/26 15:01:49 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:01:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
24/06/26 15:01:49 INFO DAGScheduler: Job 7 finished: start at NativeMethodAccessorImpl.java:0, took 0.653167 s
24/06/26 15:01:49 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:0, took 0.611227 s
24/06/26 15:01:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@79afdff] is committing.
24/06/26 15:01:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@79afdff] committed.
24/06/26 15:01:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 3
-------------------------------------------
24/06/26 15:01:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/4 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.4.2eba6172-d8d0-49e4-963f-7010efb73928.tmp
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|172.17.0.1 - - [2...|
+---------+---------+--------------------+

24/06/26 15:01:49 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:01:49 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/3 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.3.23c21faf-bde7-4794-b286-097eca1c589a.tmp
24/06/26 15:01:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.4.2eba6172-d8d0-49e4-963f-7010efb73928.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/4
24/06/26 15:01:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ecad1c35-6842-4a93-acfd-d8c042165d5d",
  "runId" : "d486de34-5908-4a59-971f-8aeba1a52e9f",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:01:48.624Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 1.1148272017837235,
  "durationMs" : {
    "addBatch" : 719,
    "commitOffsets" : 93,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 13,
    "triggerExecution" : 897,
    "walCommit" : 64
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 291
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 292
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 292
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 1.1148272017837235,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 15:01:49 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.3.23c21faf-bde7-4794-b286-097eca1c589a.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/3
24/06/26 15:01:49 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2e38d082-5107-42c5-a4ff-1dc6f6390043",
  "runId" : "0a3fd71d-9ed6-491e-b4ae-b3bb814a3930",
  "name" : null,
  "timestamp" : "2024-06-26T15:01:48.619Z",
  "batchId" : 3,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 76.92307692307692,
  "processedRowsPerSecond" : 1.0309278350515465,
  "durationMs" : {
    "addBatch" : 775,
    "commitOffsets" : 108,
    "getBatch" : 0,
    "latestOffset" : 6,
    "queryPlanning" : 17,
    "triggerExecution" : 970,
    "walCommit" : 63
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 291
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 292
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 292
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 76.92307692307692,
    "processedRowsPerSecond" : 1.0309278350515465,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@660b95d2",
    "numOutputRows" : 1
  }
}
24/06/26 15:01:53 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.2.15:43025 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:01:53 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:43025 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:01:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:01:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:02:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:03:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:04:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:18 INFO CodeGenerator: Code generated in 92.873265 ms
24/06/26 15:05:18 INFO CodeGenerator: Code generated in 11.31508 ms
127.0.0.1 - - [26/Jun/2024 15:05:18] "GET /logs HTTP/1.1" 200 -
24/06/26 15:05:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:05:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/4 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.4.e358ae9c-3ce7-4741-8b72-77bb2d1baa90.tmp
24/06/26 15:06:18 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/5 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.5.47dfcfa4-ec95-4e6d-81de-3a43a1d3fc3e.tmp
24/06/26 15:06:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/.5.47dfcfa4-ec95-4e6d-81de-3a43a1d3fc3e.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/offsets/5
24/06/26 15:06:18 INFO MicroBatchExecution: Committed offsets for batch 5. Metadata OffsetSeqMetadata(0,1719414378468,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:06:18 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/.4.e358ae9c-3ce7-4741-8b72-77bb2d1baa90.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/offsets/4
24/06/26 15:06:18 INFO MicroBatchExecution: Committed offsets for batch 4. Metadata OffsetSeqMetadata(0,1719414378467,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:06:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@9ccd4a6]. The input RDD has 1 partitions.
24/06/26 15:06:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:06:18 INFO DAGScheduler: Got job 9 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:06:18 INFO DAGScheduler: Final stage: ResultStage 9 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:06:18 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:06:18 INFO DAGScheduler: Missing parents: List()
24/06/26 15:06:18 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:06:18 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 15:06:18 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:06:18 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 15:06:18 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:06:18 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.2.15:43025 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:06:18 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
24/06/26 15:06:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[41] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:06:18 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/06/26 15:06:18 INFO DAGScheduler: Got job 10 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:06:18 INFO DAGScheduler: Final stage: ResultStage 10 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:06:18 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:06:18 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:06:18 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
24/06/26 15:06:18 INFO DAGScheduler: Missing parents: List()
24/06/26 15:06:18 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[45] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:06:18 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 15:06:18 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 15:06:18 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.2.15:43025 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:06:18 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
24/06/26 15:06:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[45] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:06:18 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
24/06/26 15:06:18 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:06:18 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
24/06/26 15:06:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=292 untilOffset=293, for query queryId=ecad1c35-6842-4a93-acfd-d8c042165d5d batchId=5 taskId=9 partitionId=0
24/06/26 15:06:18 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=292 untilOffset=293, for query queryId=2e38d082-5107-42c5-a4ff-1dc6f6390043 batchId=4 taskId=10 partitionId=0
24/06/26 15:06:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to offset 292 for partition logs-0
24/06/26 15:06:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:06:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to offset 292 for partition logs-0
24/06/26 15:06:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:06:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:06:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Seeking to latest offset of partition logs-0
24/06/26 15:06:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=293, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:06:19 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:06:19 INFO DataWritingSparkTask: Committed partition 0 (task 10, attempt 0, stage 10.0)
24/06/26 15:06:19 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor read 1 records through 1 polls (polled  out 1 records), taking 530325354 nanos, during time span of 532461652 nanos.
24/06/26 15:06:19 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2242 bytes result sent to driver
24/06/26 15:06:19 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 574 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:06:19 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/06/26 15:06:19 INFO DAGScheduler: ResultStage 10 (start at NativeMethodAccessorImpl.java:0) finished in 0.587 s
24/06/26 15:06:19 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:06:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
24/06/26 15:06:19 INFO DAGScheduler: Job 10 finished: start at NativeMethodAccessorImpl.java:0, took 0.615136 s
24/06/26 15:06:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 4
-------------------------------------------
24/06/26 15:06:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:06:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Seeking to latest offset of partition logs-0
24/06/26 15:06:19 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=293, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:06:19 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:06:19 INFO DataWritingSparkTask: Committed partition 0 (task 9, attempt 0, stage 9.0)
24/06/26 15:06:19 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor read 1 records through 1 polls (polled  out 1 records), taking 545791993 nanos, during time span of 551868715 nanos.
24/06/26 15:06:19 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 3186 bytes result sent to driver
24/06/26 15:06:19 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 640 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:06:19 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/06/26 15:06:19 INFO DAGScheduler: ResultStage 9 (start at NativeMethodAccessorImpl.java:0) finished in 0.669 s
24/06/26 15:06:19 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:06:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
24/06/26 15:06:19 INFO DAGScheduler: Job 9 finished: start at NativeMethodAccessorImpl.java:0, took 0.689160 s
24/06/26 15:06:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@9ccd4a6] is committing.
24/06/26 15:06:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@9ccd4a6] committed.
+---------+---------+--------------------+
|timestamp|log_level|             message|
+---------+---------+--------------------+
|     NULL|     NULL|172.17.0.1 - - [2...|
+---------+---------+--------------------+

24/06/26 15:06:19 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:06:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/4 using temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.4.4b5ae981-12e5-4725-a87e-4c509597050e.tmp
24/06/26 15:06:19 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/5 using temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.5.591e793f-1c2a-4b3f-a858-7f64a2f63a69.tmp
24/06/26 15:06:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/.5.591e793f-1c2a-4b3f-a858-7f64a2f63a69.tmp to file:/tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3/commits/5
24/06/26 15:06:19 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/.4.4b5ae981-12e5-4725-a87e-4c509597050e.tmp to file:/tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd/commits/4
24/06/26 15:06:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ecad1c35-6842-4a93-acfd-d8c042165d5d",
  "runId" : "d486de34-5908-4a59-971f-8aeba1a52e9f",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:06:18.463Z",
  "batchId" : 5,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 52.631578947368425,
  "processedRowsPerSecond" : 1.070663811563169,
  "durationMs" : {
    "addBatch" : 735,
    "commitOffsets" : 110,
    "getBatch" : 0,
    "latestOffset" : 5,
    "queryPlanning" : 21,
    "triggerExecution" : 934,
    "walCommit" : 62
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 292
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 52.631578947368425,
    "processedRowsPerSecond" : 1.070663811563169,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 15:06:19 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "2e38d082-5107-42c5-a4ff-1dc6f6390043",
  "runId" : "0a3fd71d-9ed6-491e-b4ae-b3bb814a3930",
  "name" : null,
  "timestamp" : "2024-06-26T15:06:18.463Z",
  "batchId" : 4,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 50.0,
  "processedRowsPerSecond" : 1.070663811563169,
  "durationMs" : {
    "addBatch" : 740,
    "commitOffsets" : 102,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 18,
    "triggerExecution" : 934,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 292
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 50.0,
    "processedRowsPerSecond" : 1.070663811563169,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@660b95d2",
    "numOutputRows" : 1
  }
}
24/06/26 15:06:23 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.2.15:43025 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:06:23 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.2.15:43025 in memory (size: 10.2 KiB, free: 434.4 MiB)
127.0.0.1 - - [26/Jun/2024 15:06:24] "GET /logs HTTP/1.1" 200 -
24/06/26 15:06:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:06:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 15:07:50] "GET /logs HTTP/1.1" 200 -
24/06/26 15:07:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:07:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:08:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:09:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:10:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:54 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:11:54 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3, groupId=spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:11:54 INFO Metrics: Metrics scheduler closed
24/06/26 15:11:54 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:11:54 INFO Metrics: Metrics reporters closed
24/06/26 15:11:54 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-ae887e15-9909-44c5-9004-10099dcb1790-1970299199-executor-3 unregistered
24/06/26 15:11:54 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:11:54 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4, groupId=spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:11:54 INFO Metrics: Metrics scheduler closed
24/06/26 15:11:54 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:11:54 INFO Metrics: Metrics reporters closed
24/06/26 15:11:54 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-3d5c8479-c555-418e-953a-e78549e8e0b2--477746295-executor-4 unregistered
24/06/26 15:11:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:11:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:12:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:13:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:14:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:15:45 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 15:15:45 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 15:15:45 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 15:15:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 15:15:45 INFO MemoryStore: MemoryStore cleared
24/06/26 15:15:45 INFO BlockManager: BlockManager stopped
24/06/26 15:15:45 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 15:15:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 15:15:45 INFO SparkContext: Successfully stopped SparkContext
24/06/26 15:15:45 INFO ShutdownHookManager: Shutdown hook called
24/06/26 15:15:45 INFO ShutdownHookManager: Deleting directory /tmp/temporary-6c3d8fac-407b-4f02-bf67-f527bfa2e5cd
24/06/26 15:15:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-8e0c591a-4f00-4f96-a86f-d5f52b9ca4d7
24/06/26 15:15:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c/pyspark-c6578417-57f6-4403-9529-bcbb20d815b0
24/06/26 15:15:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-802f131f-9e4c-4197-82ae-a50dd2093f4c
24/06/26 15:15:45 INFO ShutdownHookManager: Deleting directory /tmp/temporary-3b618445-bf5b-4831-9bd9-b0780d98d2c3
Starting Spark at Wed Jun 26 03:16:43 PM UTC 2024
24/06/26 15:16:44 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 15:16:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-39ba0267-94ae-446b-8646-72e7e104f089;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 477ms :: artifacts dl 50ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-39ba0267-94ae-446b-8646-72e7e104f089
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/16ms)
24/06/26 15:16:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 15:16:47 INFO SparkContext: Running Spark version 3.5.1
24/06/26 15:16:47 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:16:47 INFO SparkContext: Java version 11.0.20.1
24/06/26 15:16:47 INFO ResourceUtils: ==============================================================
24/06/26 15:16:47 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 15:16:47 INFO ResourceUtils: ==============================================================
24/06/26 15:16:47 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 15:16:47 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 15:16:47 INFO ResourceProfile: Limiting resource is cpu
24/06/26 15:16:47 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 15:16:47 INFO SecurityManager: Changing view acls to: root
24/06/26 15:16:47 INFO SecurityManager: Changing modify acls to: root
24/06/26 15:16:47 INFO SecurityManager: Changing view acls groups to: 
24/06/26 15:16:47 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 15:16:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 15:16:47 INFO Utils: Successfully started service 'sparkDriver' on port 37817.
24/06/26 15:16:48 INFO SparkEnv: Registering MapOutputTracker
24/06/26 15:16:48 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 15:16:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 15:16:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 15:16:48 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 15:16:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a89e8a28-94dc-4f86-8d1c-a6b3d811658f
24/06/26 15:16:48 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 15:16:48 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 15:16:48 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 15:16:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:37817/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:37817/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:37817/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:37817/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:37817/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:37817/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:37817/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:37817/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:37817/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:37817/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:37817/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:16:48 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415007359
24/06/26 15:16:48 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:16:48 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 15:16:48 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:16:48 INFO Executor: Java version 11.0.20.1
24/06/26 15:16:48 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 15:16:48 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@21f25072 for default.
24/06/26 15:16:48 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:16:49 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:37817 after 38 ms (0 ms spent in bootstraps)
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp11398170848256229602.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp11398170848256229602.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp9260103179803766683.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp9260103179803766683.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp13277854875505550098.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp13277854875505550098.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp4796272075388443205.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp4796272075388443205.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp17010659867563505332.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp17010659867563505332.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp3131599120156651874.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp3131599120156651874.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp12770572207487651305.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp12770572207487651305.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp8032918010486313278.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp8032918010486313278.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp7941642439635608416.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp7941642439635608416.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp16926967094876409128.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp16926967094876409128.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 15:16:49 INFO Executor: Fetching spark://10.0.2.15:37817/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415007359
24/06/26 15:16:49 INFO Utils: Fetching spark://10.0.2.15:37817/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp14933624557123447247.tmp
24/06/26 15:16:49 INFO Utils: /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/fetchFileTemp14933624557123447247.tmp has been previously copied to /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:16:49 INFO Executor: Adding file:/tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/userFiles-d897f19b-8e71-4e28-9f66-64771fa81759/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:16:49 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43677.
24/06/26 15:16:49 INFO NettyBlockTransferService: Server created on 10.0.2.15:43677
24/06/26 15:16:49 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 15:16:49 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 43677, None)
24/06/26 15:16:49 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:43677 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 43677, None)
24/06/26 15:16:49 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 43677, None)
24/06/26 15:16:49 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 43677, None)
24/06/26 15:16:50 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 15:16:50 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 15:16:52 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 15:16:52 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:16:52 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98 resolved to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98.
24/06/26 15:16:52 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:16:52 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/metadata using temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/.metadata.429b4d19-c828-429b-af74-f47ee49689bc.tmp
24/06/26 15:16:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/.metadata.429b4d19-c828-429b-af74-f47ee49689bc.tmp to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/metadata
24/06/26 15:16:53 INFO MicroBatchExecution: Starting logs_table [id = ed763e51-24ed-405d-a4e9-72bc0516d607, runId = d9084f1b-83d2-4b74-b55e-5ee89c8009bf]. Use file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98 to store the query checkpoint.
24/06/26 15:16:53 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@35543203] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5364d6f4]
24/06/26 15:16:53 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:16:53 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:16:53 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:16:53 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:16:53 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:16:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:16:53 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d resolved to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d.
24/06/26 15:16:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:16:53 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/metadata using temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/.metadata.dbdcb367-2578-4764-868d-792e1ef095b0.tmp
24/06/26 15:16:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:16:53 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:16:53 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:16:53 INFO AppInfoParser: Kafka startTimeMs: 1719415013769
24/06/26 15:16:53 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/.metadata.dbdcb367-2578-4764-868d-792e1ef095b0.tmp to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/metadata
24/06/26 15:16:53 INFO MicroBatchExecution: Starting [id = dedee5ae-8e8b-4259-80bf-fb016535b923, runId = 347040c4-b831-4f5f-8951-e0de3e3481b3]. Use file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d to store the query checkpoint.
24/06/26 15:16:53 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@35543203] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@5364d6f4]
24/06/26 15:16:53 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:16:53 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:16:53 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:16:53 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:16:53 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:16:53 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:16:53 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:16:53 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:16:53 INFO AppInfoParser: Kafka startTimeMs: 1719415013861
 * Serving Flask app 'spark_app'
 * Debug mode: off
24/06/26 15:16:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/sources/0/0 using temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/sources/0/.0.b4a4ed49-d404-43e6-8b81-46d2921457b2.tmp
24/06/26 15:16:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/sources/0/0 using temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/sources/0/.0.109cc3b9-39e6-41fc-8ad9-8f3aa2e3db95.tmp
24/06/26 15:16:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/sources/0/.0.b4a4ed49-d404-43e6-8b81-46d2921457b2.tmp to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/sources/0/0
24/06/26 15:16:54 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":293}}
24/06/26 15:16:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/sources/0/.0.109cc3b9-39e6-41fc-8ad9-8f3aa2e3db95.tmp to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/sources/0/0
24/06/26 15:16:54 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":293}}
24/06/26 15:16:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/0 using temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/.0.6a92a1e3-995d-4b85-a114-3da6b6a19eba.tmp
24/06/26 15:16:54 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/0 using temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/.0.930ee119-ef91-49af-a2da-9c3548d323f2.tmp
24/06/26 15:16:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/.0.930ee119-ef91-49af-a2da-9c3548d323f2.tmp to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/0
24/06/26 15:16:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719415014452,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:16:54 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/.0.6a92a1e3-995d-4b85-a114-3da6b6a19eba.tmp to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/0
24/06/26 15:16:54 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719415014452,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:16:54 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 15:16:55 INFO CodeGenerator: Code generated in 329.215723 ms
24/06/26 15:16:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:16:55 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@78619903]. The input RDD has 1 partitions.
24/06/26 15:16:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:16:55 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:16:55 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:16:55 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:16:55 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:16:55 INFO DAGScheduler: Missing parents: List()
24/06/26 15:16:55 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:16:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.1 KiB, free 434.4 MiB)
24/06/26 15:16:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.4 KiB, free 434.4 MiB)
24/06/26 15:16:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:43677 (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 15:16:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:16:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 15:16:55 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:16:55 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:16:55 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:16:55 INFO DAGScheduler: Missing parents: List()
24/06/26 15:16:55 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:16:55 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 15:16:55 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 15:16:55 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:43677 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:16:55 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 15:16:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:16:55 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:16:55 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:16:55 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 15:16:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 15:16:55 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:16:55 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 15:16:56 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:16:56 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 15:16:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/26 15:16:56 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1252 bytes result sent to driver
24/06/26 15:16:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 212 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:16:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 15:16:56 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 158 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:16:56 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 15:16:56 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.412 s
24/06/26 15:16:56 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:16:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 15:16:56 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.471856 s
24/06/26 15:16:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@78619903] is committing.
24/06/26 15:16:56 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.227 s
24/06/26 15:16:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:16:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@78619903] committed.
24/06/26 15:16:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 15:16:56 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.485220 s
24/06/26 15:16:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 15:16:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/0 using temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/.0.6ee37e6c-e90c-45af-99db-3cd4ef07acd5.tmp
+---+------+----+
|log|stream|time|
+---+------+----+
+---+------+----+

24/06/26 15:16:56 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:16:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/0 using temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/.0.c634acd8-549d-46a8-8bff-f5e207503149.tmp
24/06/26 15:16:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/.0.6ee37e6c-e90c-45af-99db-3cd4ef07acd5.tmp to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/0
24/06/26 15:16:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/.0.c634acd8-549d-46a8-8bff-f5e207503149.tmp to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/0
24/06/26 15:16:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ed763e51-24ed-405d-a4e9-72bc0516d607",
  "runId" : "d9084f1b-83d2-4b74-b55e-5ee89c8009bf",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:16:53.157Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1261,
    "commitOffsets" : 114,
    "getBatch" : 20,
    "latestOffset" : 1269,
    "queryPlanning" : 225,
    "triggerExecution" : 3040,
    "walCommit" : 108
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 15:16:56 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dedee5ae-8e8b-4259-80bf-fb016535b923",
  "runId" : "347040c4-b831-4f5f-8951-e0de3e3481b3",
  "name" : null,
  "timestamp" : "2024-06-26T15:16:53.835Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1361,
    "commitOffsets" : 76,
    "getBatch" : 28,
    "latestOffset" : 612,
    "queryPlanning" : 227,
    "triggerExecution" : 2424,
    "walCommit" : 99
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@172131ef",
    "numOutputRows" : 0
  }
}
24/06/26 15:17:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:43677 in memory (size: 2.4 KiB, free: 434.4 MiB)
24/06/26 15:17:00 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:43677 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:17:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:06 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:16 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:26 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/1 using temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/.1.2a776cbc-7ee5-4c18-86ef-5d586ea6b8bb.tmp
24/06/26 15:17:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/1 using temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/.1.9fefdf56-bddc-43ea-acc9-e2a75ddae66a.tmp
24/06/26 15:17:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/.1.2a776cbc-7ee5-4c18-86ef-5d586ea6b8bb.tmp to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/offsets/1
24/06/26 15:17:29 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719415049812,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:17:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/.1.9fefdf56-bddc-43ea-acc9-e2a75ddae66a.tmp to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/offsets/1
24/06/26 15:17:29 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719415049817,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:17:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7deb5f61]. The input RDD has 1 partitions.
24/06/26 15:17:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:17:29 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:17:29 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:17:29 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:17:29 INFO DAGScheduler: Missing parents: List()
24/06/26 15:17:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:17:29 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:17:29 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:17:30 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 22.5 KiB, free 434.4 MiB)
24/06/26 15:17:30 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 434.4 MiB)
24/06/26 15:17:30 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:43677 (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:17:30 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 15:17:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:17:30 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 15:17:30 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:17:30 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:17:30 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:17:30 INFO DAGScheduler: Missing parents: List()
24/06/26 15:17:30 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:17:30 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:17:30 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 15:17:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 22.4 KiB, free 434.3 MiB)
24/06/26 15:17:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 434.3 MiB)
24/06/26 15:17:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:43677 (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:17:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 15:17:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:17:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 15:17:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/26 15:17:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 15:17:30 INFO CodeGenerator: Code generated in 29.685279 ms
24/06/26 15:17:30 INFO CodeGenerator: Code generated in 10.662846 ms
24/06/26 15:17:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=293 untilOffset=294, for query queryId=dedee5ae-8e8b-4259-80bf-fb016535b923 batchId=1 taskId=3 partitionId=0
24/06/26 15:17:30 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=293 untilOffset=294, for query queryId=ed763e51-24ed-405d-a4e9-72bc0516d607 batchId=1 taskId=2 partitionId=0
24/06/26 15:17:30 INFO CodeGenerator: Code generated in 17.305149 ms
24/06/26 15:17:30 INFO CodeGenerator: Code generated in 26.16358 ms
24/06/26 15:17:30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:17:30 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:17:30 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:17:30 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:17:30 INFO AppInfoParser: Kafka startTimeMs: 1719415050404
24/06/26 15:17:30 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:17:30 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:17:30 INFO AppInfoParser: Kafka startTimeMs: 1719415050404
24/06/26 15:17:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Assigned to partition(s): logs-0
24/06/26 15:17:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Assigned to partition(s): logs-0
24/06/26 15:17:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Seeking to offset 293 for partition logs-0
24/06/26 15:17:30 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Seeking to offset 293 for partition logs-0
24/06/26 15:17:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:17:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:17:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:17:30 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:17:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:17:30 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:17:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:17:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:17:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Seeking to latest offset of partition logs-0
24/06/26 15:17:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Seeking to latest offset of partition logs-0
24/06/26 15:17:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=294, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:17:31 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=294, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:17:31 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:17:31 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 15:17:31 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor read 1 records through 1 polls (polled  out 1 records), taking 589429007 nanos, during time span of 739592603 nanos.
24/06/26 15:17:31 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2311 bytes result sent to driver
24/06/26 15:17:31 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1078 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:17:31 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 15:17:31 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 1.117 s
24/06/26 15:17:31 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:17:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 15:17:31 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 1.183178 s
24/06/26 15:17:31 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 15:17:31 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:43677 in memory (size: 10.2 KiB, free: 434.4 MiB)
24/06/26 15:17:31 INFO CodeGenerator: Code generated in 92.304388 ms
24/06/26 15:17:32 INFO CodeGenerator: Code generated in 16.607867 ms
24/06/26 15:17:32 INFO CodeGenerator: Code generated in 23.151091 ms
+--------------------+------+----+
|                 log|stream|time|
+--------------------+------+----+
|{"offset":3033,"f...|stdout|NULL|
+--------------------+------+----+

24/06/26 15:17:32 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:17:32 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 15:17:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:17:32 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor read 1 records through 1 polls (polled  out 1 records), taking 588461114 nanos, during time span of 1744443514 nanos.
24/06/26 15:17:32 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3334 bytes result sent to driver
24/06/26 15:17:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/1 using temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/.1.5db0a941-0ad3-4f60-ae57-df6f36f9eb59.tmp
24/06/26 15:17:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2136 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:17:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 15:17:32 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 2.187 s
24/06/26 15:17:32 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:17:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 15:17:32 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 2.226657 s
24/06/26 15:17:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7deb5f61] is committing.
24/06/26 15:17:32 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@7deb5f61] committed.
24/06/26 15:17:32 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/1 using temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/.1.e597a55a-1109-48fb-9b32-bfa6066ebe4f.tmp
24/06/26 15:17:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/.1.5db0a941-0ad3-4f60-ae57-df6f36f9eb59.tmp to file:/tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d/commits/1
24/06/26 15:17:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "dedee5ae-8e8b-4259-80bf-fb016535b923",
  "runId" : "347040c4-b831-4f5f-8951-e0de3e3481b3",
  "name" : null,
  "timestamp" : "2024-06-26T15:17:29.808Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 0.41271151465125877,
  "durationMs" : {
    "addBatch" : 2247,
    "commitOffsets" : 70,
    "getBatch" : 0,
    "latestOffset" : 9,
    "queryPlanning" : 30,
    "triggerExecution" : 2423,
    "walCommit" : 66
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 0.41271151465125877,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@172131ef",
    "numOutputRows" : 1
  }
}
24/06/26 15:17:32 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/.1.e597a55a-1109-48fb-9b32-bfa6066ebe4f.tmp to file:/tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98/commits/1
24/06/26 15:17:32 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "ed763e51-24ed-405d-a4e9-72bc0516d607",
  "runId" : "d9084f1b-83d2-4b74-b55e-5ee89c8009bf",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:17:29.805Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 66.66666666666667,
  "processedRowsPerSecond" : 0.407000407000407,
  "durationMs" : {
    "addBatch" : 2280,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 7,
    "queryPlanning" : 32,
    "triggerExecution" : 2457,
    "walCommit" : 59
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 293
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 66.66666666666667,
    "processedRowsPerSecond" : 0.407000407000407,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 15:17:33 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:43677 in memory (size: 10.3 KiB, free: 434.4 MiB)
24/06/26 15:17:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:17:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:43 INFO CodeGenerator: Code generated in 50.452254 ms
24/06/26 15:18:44 INFO CodeGenerator: Code generated in 12.746752 ms
24/06/26 15:18:44 INFO CodeGenerator: Code generated in 10.794099 ms
24/06/26 15:18:44 INFO DAGScheduler: Registering RDD 20 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
24/06/26 15:18:44 INFO DAGScheduler: Got map stage job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:18:44 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:18:44 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:18:44 INFO DAGScheduler: Missing parents: List()
24/06/26 15:18:44 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:18:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.1 KiB, free 434.4 MiB)
24/06/26 15:18:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.4 MiB)
24/06/26 15:18:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:43677 (size: 5.7 KiB, free: 434.4 MiB)
24/06/26 15:18:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 15:18:44 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:18:44 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 15:18:44 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10016 bytes) 
24/06/26 15:18:44 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 15:18:44 INFO CodeGenerator: Code generated in 10.25515 ms
24/06/26 15:18:44 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1882 bytes result sent to driver
24/06/26 15:18:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 123 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:18:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 15:18:44 INFO DAGScheduler: ShuffleMapStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.158 s
24/06/26 15:18:44 INFO DAGScheduler: looking for newly runnable stages
24/06/26 15:18:44 INFO DAGScheduler: running: Set()
24/06/26 15:18:44 INFO DAGScheduler: waiting: Set()
24/06/26 15:18:44 INFO DAGScheduler: failed: Set()
24/06/26 15:18:44 INFO CodeGenerator: Code generated in 10.00627 ms
24/06/26 15:18:44 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
24/06/26 15:18:44 INFO DAGScheduler: Got job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:18:44 INFO DAGScheduler: Final stage: ResultStage 6 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:18:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
24/06/26 15:18:44 INFO DAGScheduler: Missing parents: List()
24/06/26 15:18:44 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:18:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)
24/06/26 15:18:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.4 MiB)
24/06/26 15:18:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:43677 (size: 5.9 KiB, free: 434.4 MiB)
24/06/26 15:18:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 15:18:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:18:44 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/26 15:18:44 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 15:18:44 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
24/06/26 15:18:44 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 15:18:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
24/06/26 15:18:44 INFO CodeGenerator: Code generated in 10.171506 ms
24/06/26 15:18:44 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 4038 bytes result sent to driver
24/06/26 15:18:44 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 102 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:18:44 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/26 15:18:44 INFO DAGScheduler: ResultStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.122 s
24/06/26 15:18:44 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:18:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/26 15:18:44 INFO DAGScheduler: Job 5 finished: count at NativeMethodAccessorImpl.java:0, took 0.140409 s
127.0.0.1 - - [26/Jun/2024 15:18:44] "GET /logs HTTP/1.1" 200 -
24/06/26 15:18:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:43677 in memory (size: 5.9 KiB, free: 434.4 MiB)
24/06/26 15:18:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:43677 in memory (size: 5.7 KiB, free: 434.4 MiB)
24/06/26 15:18:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:18:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:19:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:24 INFO CodeGenerator: Code generated in 10.491326 ms
24/06/26 15:20:24 INFO DAGScheduler: Registering RDD 26 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
24/06/26 15:20:24 INFO DAGScheduler: Got map stage job 6 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:20:24 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:20:24 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:20:24 INFO DAGScheduler: Missing parents: List()
24/06/26 15:20:24 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:20:24 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 11.8 KiB, free 434.4 MiB)
24/06/26 15:20:24 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.4 MiB)
24/06/26 15:20:24 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:43677 (size: 6.0 KiB, free: 434.4 MiB)
24/06/26 15:20:24 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
24/06/26 15:20:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[26] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:20:24 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/06/26 15:20:24 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10016 bytes) 
24/06/26 15:20:24 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
24/06/26 15:20:25 INFO CodeGenerator: Code generated in 14.250484 ms
24/06/26 15:20:25 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 1895 bytes result sent to driver
24/06/26 15:20:25 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 35 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:20:25 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/06/26 15:20:25 INFO DAGScheduler: ShuffleMapStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.048 s
24/06/26 15:20:25 INFO DAGScheduler: looking for newly runnable stages
24/06/26 15:20:25 INFO DAGScheduler: running: Set()
24/06/26 15:20:25 INFO DAGScheduler: waiting: Set()
24/06/26 15:20:25 INFO DAGScheduler: failed: Set()
24/06/26 15:20:25 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
24/06/26 15:20:25 INFO DAGScheduler: Got job 7 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:20:25 INFO DAGScheduler: Final stage: ResultStage 9 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:20:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
24/06/26 15:20:25 INFO DAGScheduler: Missing parents: List()
24/06/26 15:20:25 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:20:25 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)
24/06/26 15:20:25 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.4 MiB)
24/06/26 15:20:25 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:43677 (size: 5.9 KiB, free: 434.4 MiB)
24/06/26 15:20:25 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
24/06/26 15:20:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[29] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:20:25 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
24/06/26 15:20:25 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 15:20:25 INFO Executor: Running task 0.0 in stage 9.0 (TID 7)
24/06/26 15:20:25 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 15:20:25 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/06/26 15:20:25 INFO Executor: Finished task 0.0 in stage 9.0 (TID 7). 3995 bytes result sent to driver
24/06/26 15:20:25 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 19 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:20:25 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
24/06/26 15:20:25 INFO DAGScheduler: ResultStage 9 (count at NativeMethodAccessorImpl.java:0) finished in 0.032 s
24/06/26 15:20:25 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:20:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
24/06/26 15:20:25 INFO DAGScheduler: Job 7 finished: count at NativeMethodAccessorImpl.java:0, took 0.041094 s
24/06/26 15:20:25 INFO CodeGenerator: Code generated in 14.463882 ms
24/06/26 15:20:25 INFO SparkContext: Starting job: collect at /vagrant_data/spark_app.py:87
24/06/26 15:20:25 INFO DAGScheduler: Got job 8 (collect at /vagrant_data/spark_app.py:87) with 1 output partitions
24/06/26 15:20:25 INFO DAGScheduler: Final stage: ResultStage 10 (collect at /vagrant_data/spark_app.py:87)
24/06/26 15:20:25 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:20:25 INFO DAGScheduler: Missing parents: List()
24/06/26 15:20:25 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[32] at collect at /vagrant_data/spark_app.py:87), which has no missing parents
24/06/26 15:20:25 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 8.6 KiB, free 434.4 MiB)
24/06/26 15:20:25 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.4 MiB)
24/06/26 15:20:25 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.2.15:43677 (size: 4.2 KiB, free: 434.4 MiB)
24/06/26 15:20:25 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
24/06/26 15:20:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[32] at collect at /vagrant_data/spark_app.py:87) (first 15 tasks are for partitions Vector(0))
24/06/26 15:20:25 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
24/06/26 15:20:25 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10027 bytes) 
24/06/26 15:20:25 INFO Executor: Running task 0.0 in stage 10.0 (TID 8)
24/06/26 15:20:25 INFO CodeGenerator: Code generated in 20.234351 ms
24/06/26 15:20:25 INFO Executor: Finished task 0.0 in stage 10.0 (TID 8). 1643 bytes result sent to driver
24/06/26 15:20:25 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 35 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:20:25 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
24/06/26 15:20:25 INFO DAGScheduler: ResultStage 10 (collect at /vagrant_data/spark_app.py:87) finished in 0.083 s
24/06/26 15:20:25 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:20:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
24/06/26 15:20:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:43677 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/06/26 15:20:25 INFO DAGScheduler: Job 8 finished: collect at /vagrant_data/spark_app.py:87, took 0.093127 s
127.0.0.1 - - [26/Jun/2024 15:20:25] "GET /logs?stream=stdout HTTP/1.1" 200 -
24/06/26 15:20:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.2.15:43677 in memory (size: 5.9 KiB, free: 434.4 MiB)
24/06/26 15:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:38 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.2.15:43677 in memory (size: 4.2 KiB, free: 434.4 MiB)
24/06/26 15:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:20:59 INFO CodeGenerator: Code generated in 16.392153 ms
24/06/26 15:20:59 INFO DAGScheduler: Registering RDD 35 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 2
24/06/26 15:20:59 INFO DAGScheduler: Got map stage job 9 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:20:59 INFO DAGScheduler: Final stage: ShuffleMapStage 11 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:20:59 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:20:59 INFO DAGScheduler: Missing parents: List()
24/06/26 15:20:59 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[35] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:20:59 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 11.8 KiB, free 434.4 MiB)
24/06/26 15:20:59 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.4 MiB)
24/06/26 15:20:59 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.2.15:43677 (size: 6.0 KiB, free: 434.4 MiB)
24/06/26 15:20:59 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585
24/06/26 15:20:59 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[35] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:20:59 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
24/06/26 15:20:59 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10016 bytes) 
24/06/26 15:20:59 INFO Executor: Running task 0.0 in stage 11.0 (TID 9)
24/06/26 15:20:59 INFO CodeGenerator: Code generated in 10.741439 ms
24/06/26 15:20:59 INFO Executor: Finished task 0.0 in stage 11.0 (TID 9). 1895 bytes result sent to driver
24/06/26 15:20:59 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 26 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:20:59 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
24/06/26 15:20:59 INFO DAGScheduler: ShuffleMapStage 11 (count at NativeMethodAccessorImpl.java:0) finished in 0.045 s
24/06/26 15:20:59 INFO DAGScheduler: looking for newly runnable stages
24/06/26 15:20:59 INFO DAGScheduler: running: Set()
24/06/26 15:20:59 INFO DAGScheduler: waiting: Set()
24/06/26 15:20:59 INFO DAGScheduler: failed: Set()
24/06/26 15:20:59 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
24/06/26 15:20:59 INFO DAGScheduler: Got job 10 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:20:59 INFO DAGScheduler: Final stage: ResultStage 13 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:20:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
24/06/26 15:20:59 INFO DAGScheduler: Missing parents: List()
24/06/26 15:20:59 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[38] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:20:59 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)
24/06/26 15:20:59 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 434.4 MiB)
24/06/26 15:20:59 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.2.15:43677 (size: 5.9 KiB, free: 434.4 MiB)
24/06/26 15:20:59 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
24/06/26 15:20:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[38] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:20:59 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
24/06/26 15:20:59 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 15:20:59 INFO Executor: Running task 0.0 in stage 13.0 (TID 10)
24/06/26 15:20:59 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 15:20:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/06/26 15:20:59 INFO Executor: Finished task 0.0 in stage 13.0 (TID 10). 3995 bytes result sent to driver
24/06/26 15:21:00 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 22 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:21:00 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
24/06/26 15:21:00 INFO DAGScheduler: ResultStage 13 (count at NativeMethodAccessorImpl.java:0) finished in 0.036 s
24/06/26 15:21:00 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:21:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
24/06/26 15:21:00 INFO DAGScheduler: Job 10 finished: count at NativeMethodAccessorImpl.java:0, took 0.046215 s
24/06/26 15:21:00 INFO CodeGenerator: Code generated in 9.65431 ms
24/06/26 15:21:00 INFO SparkContext: Starting job: collect at /vagrant_data/spark_app.py:87
24/06/26 15:21:00 INFO DAGScheduler: Got job 11 (collect at /vagrant_data/spark_app.py:87) with 1 output partitions
24/06/26 15:21:00 INFO DAGScheduler: Final stage: ResultStage 14 (collect at /vagrant_data/spark_app.py:87)
24/06/26 15:21:00 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:21:00 INFO DAGScheduler: Missing parents: List()
24/06/26 15:21:00 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[41] at collect at /vagrant_data/spark_app.py:87), which has no missing parents
24/06/26 15:21:00 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 8.6 KiB, free 434.4 MiB)
24/06/26 15:21:00 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 434.4 MiB)
24/06/26 15:21:00 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.2.15:43677 (size: 4.2 KiB, free: 434.4 MiB)
24/06/26 15:21:00 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585
24/06/26 15:21:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[41] at collect at /vagrant_data/spark_app.py:87) (first 15 tasks are for partitions Vector(0))
24/06/26 15:21:00 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0
24/06/26 15:21:00 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10027 bytes) 
24/06/26 15:21:00 INFO Executor: Running task 0.0 in stage 14.0 (TID 11)
24/06/26 15:21:00 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.2.15:43677 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/06/26 15:21:00 INFO CodeGenerator: Code generated in 10.981595 ms
24/06/26 15:21:00 INFO Executor: Finished task 0.0 in stage 14.0 (TID 11). 1643 bytes result sent to driver
24/06/26 15:21:00 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 28 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:21:00 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
24/06/26 15:21:00 INFO DAGScheduler: ResultStage 14 (collect at /vagrant_data/spark_app.py:87) finished in 0.057 s
24/06/26 15:21:00 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:21:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished
24/06/26 15:21:00 INFO DAGScheduler: Job 11 finished: collect at /vagrant_data/spark_app.py:87, took 0.069222 s
127.0.0.1 - - [26/Jun/2024 15:21:00] "GET /logs?keyword=offset HTTP/1.1" 200 -
24/06/26 15:21:00 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.2.15:43677 in memory (size: 5.9 KiB, free: 434.4 MiB)
24/06/26 15:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:12 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.2.15:43677 in memory (size: 4.2 KiB, free: 434.4 MiB)
24/06/26 15:21:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:21:54 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 15:21:54 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 15:22:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:22:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:23:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1, groupId=spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:23:30 INFO Metrics: Metrics scheduler closed
24/06/26 15:23:30 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:23:30 INFO Metrics: Metrics reporters closed
24/06/26 15:23:30 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-07a22196-e319-4144-b1b2-907c4c4c5eac-1525061263-executor-1 unregistered
24/06/26 15:23:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:23:30 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2, groupId=spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:23:30 INFO Metrics: Metrics scheduler closed
24/06/26 15:23:30 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:23:30 INFO Metrics: Metrics reporters closed
24/06/26 15:23:30 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-e56ba4dc-24f0-46e3-a12f-9d5c46c65534--1384729766-executor-2 unregistered
24/06/26 15:23:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:32 INFO DAGScheduler: Registering RDD 44 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 3
24/06/26 15:23:32 INFO DAGScheduler: Got map stage job 12 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:23:32 INFO DAGScheduler: Final stage: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:23:32 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:23:32 INFO DAGScheduler: Missing parents: List()
24/06/26 15:23:32 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:23:32 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 11.1 KiB, free 434.4 MiB)
24/06/26 15:23:32 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 434.4 MiB)
24/06/26 15:23:32 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.2.15:43677 (size: 5.7 KiB, free: 434.4 MiB)
24/06/26 15:23:32 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585
24/06/26 15:23:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[44] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:23:32 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0
24/06/26 15:23:32 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10016 bytes) 
24/06/26 15:23:32 INFO Executor: Running task 0.0 in stage 15.0 (TID 12)
24/06/26 15:23:32 INFO Executor: Finished task 0.0 in stage 15.0 (TID 12). 1839 bytes result sent to driver
24/06/26 15:23:32 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 29 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:23:32 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
24/06/26 15:23:32 INFO DAGScheduler: ShuffleMapStage 15 (count at NativeMethodAccessorImpl.java:0) finished in 0.046 s
24/06/26 15:23:32 INFO DAGScheduler: looking for newly runnable stages
24/06/26 15:23:32 INFO DAGScheduler: running: Set()
24/06/26 15:23:32 INFO DAGScheduler: waiting: Set()
24/06/26 15:23:32 INFO DAGScheduler: failed: Set()
24/06/26 15:23:32 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
24/06/26 15:23:32 INFO DAGScheduler: Got job 13 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:23:32 INFO DAGScheduler: Final stage: ResultStage 17 (count at NativeMethodAccessorImpl.java:0)
24/06/26 15:23:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
24/06/26 15:23:32 INFO DAGScheduler: Missing parents: List()
24/06/26 15:23:32 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[47] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:23:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 12.5 KiB, free 434.4 MiB)
24/06/26 15:23:32 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 434.4 MiB)
24/06/26 15:23:32 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.2.15:43677 (size: 6.0 KiB, free: 434.4 MiB)
24/06/26 15:23:32 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1585
24/06/26 15:23:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[47] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:23:32 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0
24/06/26 15:23:32 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 15:23:32 INFO Executor: Running task 0.0 in stage 17.0 (TID 13)
24/06/26 15:23:32 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 15:23:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
24/06/26 15:23:32 INFO Executor: Finished task 0.0 in stage 17.0 (TID 13). 4081 bytes result sent to driver
24/06/26 15:23:32 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 54 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:23:32 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
24/06/26 15:23:32 INFO DAGScheduler: ResultStage 17 (count at NativeMethodAccessorImpl.java:0) finished in 0.075 s
24/06/26 15:23:32 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.2.15:43677 in memory (size: 5.7 KiB, free: 434.4 MiB)
24/06/26 15:23:32 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:23:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
24/06/26 15:23:32 INFO DAGScheduler: Job 13 finished: count at NativeMethodAccessorImpl.java:0, took 0.098817 s
10.0.2.2 - - [26/Jun/2024 15:23:32] "GET /logs HTTP/1.1" 200 -
24/06/26 15:23:42 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.2.15:43677 in memory (size: 6.0 KiB, free: 434.4 MiB)
24/06/26 15:23:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:23:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:24:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:25:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:26:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:27:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:28:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:29:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:29:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:29:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:29:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:29:17 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 15:29:17 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 15:29:17 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 15:29:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 15:29:17 INFO MemoryStore: MemoryStore cleared
24/06/26 15:29:17 INFO BlockManager: BlockManager stopped
24/06/26 15:29:17 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 15:29:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 15:29:17 INFO SparkContext: Successfully stopped SparkContext
24/06/26 15:29:17 INFO ShutdownHookManager: Shutdown hook called
24/06/26 15:29:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787/pyspark-67e4eb2d-d5f8-4692-a1b4-f4a90ef03744
24/06/26 15:29:17 INFO ShutdownHookManager: Deleting directory /tmp/temporary-867032d4-5d08-4009-b6f6-513dffadde98
24/06/26 15:29:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-f606d2b6-7fe3-445b-b158-a1b436a18787
24/06/26 15:29:17 INFO ShutdownHookManager: Deleting directory /tmp/spark-6f19f8ef-55bc-4567-8162-f29bc3ce6037
24/06/26 15:29:17 INFO ShutdownHookManager: Deleting directory /tmp/temporary-0ca10875-c5f2-4057-accd-bb9b1f47d40d
Starting Spark at Wed Jun 26 03:29:47 PM UTC 2024
24/06/26 15:29:48 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 15:29:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-92b49ce9-bcd2-4e3e-96b8-ce5a365f4e05;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 474ms :: artifacts dl 21ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-92b49ce9-bcd2-4e3e-96b8-ce5a365f4e05
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/8ms)
24/06/26 15:29:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 15:29:51 INFO SparkContext: Running Spark version 3.5.1
24/06/26 15:29:51 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:29:51 INFO SparkContext: Java version 11.0.20.1
24/06/26 15:29:51 INFO ResourceUtils: ==============================================================
24/06/26 15:29:51 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 15:29:51 INFO ResourceUtils: ==============================================================
24/06/26 15:29:51 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 15:29:51 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 15:29:51 INFO ResourceProfile: Limiting resource is cpu
24/06/26 15:29:51 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 15:29:51 INFO SecurityManager: Changing view acls to: root
24/06/26 15:29:51 INFO SecurityManager: Changing modify acls to: root
24/06/26 15:29:51 INFO SecurityManager: Changing view acls groups to: 
24/06/26 15:29:51 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 15:29:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 15:29:51 INFO Utils: Successfully started service 'sparkDriver' on port 35879.
24/06/26 15:29:51 INFO SparkEnv: Registering MapOutputTracker
24/06/26 15:29:51 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 15:29:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 15:29:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 15:29:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 15:29:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-67046447-8acc-4abb-bb23-fa3d25025383
24/06/26 15:29:51 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 15:29:51 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 15:29:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 15:29:52 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:35879/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:35879/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:35879/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:35879/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:35879/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:35879/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:35879/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:35879/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:35879/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:35879/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:35879/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:29:52 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:29:52 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 15:29:52 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:29:52 INFO Executor: Java version 11.0.20.1
24/06/26 15:29:52 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 15:29:52 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5f314ded for default.
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:29:52 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:29:52 INFO Executor: Fetching spark://10.0.2.15:35879/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:35879 after 35 ms (0 ms spent in bootstraps)
24/06/26 15:29:52 INFO Utils: Fetching spark://10.0.2.15:35879/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp11932548935084425931.tmp
24/06/26 15:29:52 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp11932548935084425931.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:29:52 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 15:29:52 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp10388512453313699657.tmp
24/06/26 15:29:52 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp10388512453313699657.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:29:52 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 15:29:52 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp6638121287166180848.tmp
24/06/26 15:29:52 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp6638121287166180848.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:29:52 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:29:52 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp6283226857865776123.tmp
24/06/26 15:29:52 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp6283226857865776123.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:29:52 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 15:29:52 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719415791156
24/06/26 15:29:52 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp7240018734120187797.tmp
24/06/26 15:29:52 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp7240018734120187797.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 15:29:53 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719415791156
24/06/26 15:29:53 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp2077129014076055622.tmp
24/06/26 15:29:53 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp2077129014076055622.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 15:29:53 INFO Executor: Fetching spark://10.0.2.15:35879/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719415791156
24/06/26 15:29:53 INFO Utils: Fetching spark://10.0.2.15:35879/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp15394589578584993864.tmp
24/06/26 15:29:53 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp15394589578584993864.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 15:29:53 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719415791156
24/06/26 15:29:53 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp17215268575033632433.tmp
24/06/26 15:29:53 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp17215268575033632433.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:29:53 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719415791156
24/06/26 15:29:53 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp11951331183976742224.tmp
24/06/26 15:29:53 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp11951331183976742224.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 15:29:53 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719415791156
24/06/26 15:29:53 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp11919987181419463084.tmp
24/06/26 15:29:53 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp11919987181419463084.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 15:29:53 INFO Executor: Fetching spark://10.0.2.15:35879/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719415791156
24/06/26 15:29:53 INFO Utils: Fetching spark://10.0.2.15:35879/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp8058305880268026897.tmp
24/06/26 15:29:53 INFO Utils: /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/fetchFileTemp8058305880268026897.tmp has been previously copied to /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:29:53 INFO Executor: Adding file:/tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/userFiles-415f620b-d0d6-44c0-a6fa-0bbeaf355cca/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 15:29:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34733.
24/06/26 15:29:53 INFO NettyBlockTransferService: Server created on 10.0.2.15:34733
24/06/26 15:29:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 15:29:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 34733, None)
24/06/26 15:29:53 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:34733 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 34733, None)
24/06/26 15:29:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 34733, None)
24/06/26 15:29:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 34733, None)
24/06/26 15:29:53 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 15:29:53 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 15:29:55 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 15:29:55 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:29:55 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7 resolved to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7.
24/06/26 15:29:55 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:29:55 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/metadata using temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/.metadata.eac977ba-897e-44c9-ae4b-9353cfdf4213.tmp
24/06/26 15:29:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/.metadata.eac977ba-897e-44c9-ae4b-9353cfdf4213.tmp to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/metadata
24/06/26 15:29:56 INFO MicroBatchExecution: Starting logs_table [id = bdf64dfb-a02e-48f1-af18-667446263e2e, runId = 282e3339-27f5-496d-bab5-deb38a808737]. Use file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7 to store the query checkpoint.
24/06/26 15:29:56 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@530a9740] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@42891c8a]
24/06/26 15:29:56 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:29:56 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:29:56 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:29:56 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:29:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:29:56 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2 resolved to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2.
24/06/26 15:29:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:29:56 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/metadata using temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/.metadata.04a4334b-82ec-49e4-80a8-0f05f54b5d0d.tmp
24/06/26 15:29:56 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:29:56 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/.metadata.04a4334b-82ec-49e4-80a8-0f05f54b5d0d.tmp to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/metadata
24/06/26 15:29:56 INFO MicroBatchExecution: Starting [id = af1c932e-5247-4c9e-ae18-c8686c26dc54, runId = 511f3f6c-87f7-40ef-9f88-0feee82180b8]. Use file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2 to store the query checkpoint.
24/06/26 15:29:56 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@530a9740] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@42891c8a]
24/06/26 15:29:56 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:29:56 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:29:56 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:29:56 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:29:56 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:29:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:29:56 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:29:56 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:29:56 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:29:56 INFO AppInfoParser: Kafka startTimeMs: 1719415796838
24/06/26 15:29:56 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:29:56 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:29:56 INFO AppInfoParser: Kafka startTimeMs: 1719415796838
 * Serving Flask app 'spark_app2'
 * Debug mode: off
24/06/26 15:29:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/sources/0/0 using temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/sources/0/.0.fb6e9a31-8b88-49c6-8c44-f1403618573a.tmp
24/06/26 15:29:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/sources/0/0 using temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/sources/0/.0.cc413568-0fc7-423f-b3de-2c705b3c5efb.tmp
24/06/26 15:29:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/sources/0/.0.cc413568-0fc7-423f-b3de-2c705b3c5efb.tmp to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/sources/0/0
24/06/26 15:29:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/sources/0/.0.fb6e9a31-8b88-49c6-8c44-f1403618573a.tmp to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/sources/0/0
24/06/26 15:29:57 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":294}}
24/06/26 15:29:57 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":294}}
24/06/26 15:29:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/0 using temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/.0.1696df4e-0dc9-4a9e-ab3b-a4de6dbc3c6c.tmp
24/06/26 15:29:57 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/0 using temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/.0.dc985466-b4b1-4e46-89aa-d05326a7ddaa.tmp
24/06/26 15:29:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/.0.dc985466-b4b1-4e46-89aa-d05326a7ddaa.tmp to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/0
24/06/26 15:29:57 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/.0.1696df4e-0dc9-4a9e-ab3b-a4de6dbc3c6c.tmp to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/0
24/06/26 15:29:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719415797443,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:29:57 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719415797443,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:29:57 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 15:29:58 INFO CodeGenerator: Code generated in 290.677235 ms
24/06/26 15:29:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:29:58 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4151a617]. The input RDD has 1 partitions.
24/06/26 15:29:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:29:58 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:29:58 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:29:58 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:29:58 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:29:58 INFO DAGScheduler: Missing parents: List()
24/06/26 15:29:58 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:29:58 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 15:29:58 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 15:29:58 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:34733 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:29:58 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 15:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:29:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 15:29:58 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:29:58 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:29:58 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:29:58 INFO DAGScheduler: Missing parents: List()
24/06/26 15:29:58 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:29:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/26 15:29:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 434.4 MiB)
24/06/26 15:29:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:34733 (size: 2.5 KiB, free: 434.4 MiB)
24/06/26 15:29:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 15:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:29:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 15:29:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:29:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:29:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 15:29:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 15:29:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:29:59 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 15:29:59 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:29:59 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 15:29:59 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1353 bytes result sent to driver
24/06/26 15:29:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1252 bytes result sent to driver
24/06/26 15:29:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 250 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:29:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 15:29:59 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 200 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:29:59 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 15:29:59 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.493 s
24/06/26 15:29:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:29:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 15:29:59 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.569297 s
24/06/26 15:29:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 15:29:59 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.301 s
24/06/26 15:29:59 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:29:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 15:29:59 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.569335 s
24/06/26 15:29:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4151a617] is committing.
24/06/26 15:29:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@4151a617] committed.
24/06/26 15:29:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/0 using temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/.0.92b4baf3-efba-4ae3-a9e6-1e3237182b5d.tmp
+----------+---------+-------+---------+----+------+
|@timestamp|log_level|message|container|host|stream|
+----------+---------+-------+---------+----+------+
+----------+---------+-------+---------+----+------+

24/06/26 15:29:59 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:29:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/0 using temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/.0.9f241f48-a564-4221-9614-5f0eae1f029d.tmp
24/06/26 15:29:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/.0.92b4baf3-efba-4ae3-a9e6-1e3237182b5d.tmp to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/0
24/06/26 15:29:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/.0.9f241f48-a564-4221-9614-5f0eae1f029d.tmp to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/0
24/06/26 15:29:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "bdf64dfb-a02e-48f1-af18-667446263e2e",
  "runId" : "282e3339-27f5-496d-bab5-deb38a808737",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:29:56.186Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1353,
    "commitOffsets" : 108,
    "getBatch" : 23,
    "latestOffset" : 1226,
    "queryPlanning" : 227,
    "triggerExecution" : 3070,
    "walCommit" : 83
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 15:29:59 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "af1c932e-5247-4c9e-ae18-c8686c26dc54",
  "runId" : "511f3f6c-87f7-40ef-9f88-0feee82180b8",
  "name" : null,
  "timestamp" : "2024-06-26T15:29:56.788Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1433,
    "commitOffsets" : 68,
    "getBatch" : 28,
    "latestOffset" : 651,
    "queryPlanning" : 229,
    "triggerExecution" : 2508,
    "walCommit" : 86
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@473acc15",
    "numOutputRows" : 0
  }
}
24/06/26 15:30:01 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:34733 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:30:01 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:34733 in memory (size: 2.5 KiB, free: 434.4 MiB)
24/06/26 15:30:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:30:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:31:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/1 using temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/.1.2f177e12-493e-4678-9698-810a426a5709.tmp
24/06/26 15:31:59 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/1 using temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/.1.99e30e5e-1c98-4152-88cf-c903affb3f5e.tmp
24/06/26 15:31:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/.1.2f177e12-493e-4678-9698-810a426a5709.tmp to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/offsets/1
24/06/26 15:31:59 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719415919495,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:31:59 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/.1.99e30e5e-1c98-4152-88cf-c903affb3f5e.tmp to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/offsets/1
24/06/26 15:31:59 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719415919499,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:31:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:31:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:31:59 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:31:59 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:31:59 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:31:59 INFO DAGScheduler: Missing parents: List()
24/06/26 15:31:59 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:31:59 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6ce5e4a0]. The input RDD has 1 partitions.
24/06/26 15:31:59 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:31:59 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 26.7 KiB, free 434.4 MiB)
24/06/26 15:31:59 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 434.4 MiB)
24/06/26 15:31:59 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:34733 (size: 11.2 KiB, free: 434.4 MiB)
24/06/26 15:31:59 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 15:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:31:59 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 15:31:59 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:31:59 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:31:59 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:31:59 INFO DAGScheduler: Missing parents: List()
24/06/26 15:31:59 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:31:59 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:31:59 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.9 KiB, free 434.3 MiB)
24/06/26 15:31:59 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 15:31:59 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.3 KiB, free 434.3 MiB)
24/06/26 15:31:59 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:34733 (size: 11.3 KiB, free: 434.4 MiB)
24/06/26 15:31:59 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 15:31:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:31:59 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 15:31:59 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10571 bytes) 
24/06/26 15:31:59 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 15:31:59 INFO CodeGenerator: Code generated in 48.73476 ms
24/06/26 15:31:59 INFO CodeGenerator: Code generated in 20.795913 ms
24/06/26 15:31:59 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=294 untilOffset=295, for query queryId=af1c932e-5247-4c9e-ae18-c8686c26dc54 batchId=1 taskId=2 partitionId=0
24/06/26 15:31:59 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=294 untilOffset=295, for query queryId=bdf64dfb-a02e-48f1-af18-667446263e2e batchId=1 taskId=3 partitionId=0
24/06/26 15:32:00 INFO CodeGenerator: Code generated in 14.702578 ms
24/06/26 15:32:00 INFO CodeGenerator: Code generated in 20.604511 ms
24/06/26 15:32:00 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:32:00 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:32:00 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:32:00 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:32:00 INFO AppInfoParser: Kafka startTimeMs: 1719415920168
24/06/26 15:32:00 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:32:00 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:32:00 INFO AppInfoParser: Kafka startTimeMs: 1719415920169
24/06/26 15:32:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Assigned to partition(s): logs-0
24/06/26 15:32:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Assigned to partition(s): logs-0
24/06/26 15:32:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Seeking to offset 294 for partition logs-0
24/06/26 15:32:00 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Seeking to offset 294 for partition logs-0
24/06/26 15:32:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:32:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:32:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:32:00 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Seeking to latest offset of partition logs-0
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Seeking to latest offset of partition logs-0
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=295, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:32:00 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=295, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:32:00 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:32:00 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 15:32:00 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor read 1 records through 1 polls (polled  out 1 records), taking 588647828 nanos, during time span of 731164962 nanos.
24/06/26 15:32:00 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2469 bytes result sent to driver
24/06/26 15:32:00 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1201 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:32:00 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 15:32:00 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.269 s
24/06/26 15:32:00 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:32:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 15:32:00 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.287959 s
24/06/26 15:32:00 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 15:32:01 INFO CodeGenerator: Code generated in 15.566998 ms
24/06/26 15:32:01 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:34733 in memory (size: 11.2 KiB, free: 434.4 MiB)
24/06/26 15:32:01 INFO CodeGenerator: Code generated in 18.399988 ms
+--------------------+---------+--------------------+--------------------+------------------+------+
|          @timestamp|log_level|             message|           container|              host|stream|
+--------------------+---------+--------------------+--------------------+------------------+------+
|2024-06-26T15:31:...|     NULL|172.17.0.1 - - [2...|{nginx-container,...|{vgr-spark-base64}|stdout|
+--------------------+---------+--------------------+--------------------+------------------+------+

24/06/26 15:32:01 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:32:01 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/1 using temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/.1.cea8da36-110f-4718-bdd5-1a77ba006fd7.tmp
24/06/26 15:32:02 INFO CodeGenerator: Code generated in 58.166884 ms
24/06/26 15:32:02 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:32:02 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 15:32:02 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor read 1 records through 1 polls (polled  out 1 records), taking 586029263 nanos, during time span of 1845681499 nanos.
24/06/26 15:32:02 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3996 bytes result sent to driver
24/06/26 15:32:02 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2286 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:32:02 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 15:32:02 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.307 s
24/06/26 15:32:02 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:32:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 15:32:02 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.406011 s
24/06/26 15:32:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6ce5e4a0] is committing.
24/06/26 15:32:02 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@6ce5e4a0] committed.
24/06/26 15:32:02 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/1 using temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/.1.4130da4c-0cf5-4281-b1fa-0c6597bb4230.tmp
24/06/26 15:32:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/.1.cea8da36-110f-4718-bdd5-1a77ba006fd7.tmp to file:/tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2/commits/1
24/06/26 15:32:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "af1c932e-5247-4c9e-ae18-c8686c26dc54",
  "runId" : "511f3f6c-87f7-40ef-9f88-0feee82180b8",
  "name" : null,
  "timestamp" : "2024-06-26T15:31:59.491Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 62.5,
  "processedRowsPerSecond" : 0.38153376573826786,
  "durationMs" : {
    "addBatch" : 2387,
    "commitOffsets" : 129,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 25,
    "triggerExecution" : 2621,
    "walCommit" : 71
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 62.5,
    "processedRowsPerSecond" : 0.38153376573826786,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@473acc15",
    "numOutputRows" : 1
  }
}
24/06/26 15:32:02 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/.1.4130da4c-0cf5-4281-b1fa-0c6597bb4230.tmp to file:/tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7/commits/1
24/06/26 15:32:02 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "bdf64dfb-a02e-48f1-af18-667446263e2e",
  "runId" : "282e3339-27f5-496d-bab5-deb38a808737",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:31:59.491Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 62.5,
  "processedRowsPerSecond" : 0.37453183520599254,
  "durationMs" : {
    "addBatch" : 2489,
    "commitOffsets" : 76,
    "getBatch" : 0,
    "latestOffset" : 4,
    "queryPlanning" : 32,
    "triggerExecution" : 2670,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 294
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 62.5,
    "processedRowsPerSecond" : 0.37453183520599254,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 15:32:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:28 INFO CodeGenerator: Code generated in 105.404264 ms
24/06/26 15:32:28 INFO CodeGenerator: Code generated in 20.28927 ms
127.0.0.1 - - [26/Jun/2024 15:32:28] "GET /logs HTTP/1.1" 200 -
24/06/26 15:32:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:32:54 INFO CodeGenerator: Code generated in 21.172074 ms
24/06/26 15:32:54 INFO SparkContext: Starting job: collect at /vagrant_data/spark_app2.py:100
24/06/26 15:32:54 INFO DAGScheduler: Got job 4 (collect at /vagrant_data/spark_app2.py:100) with 1 output partitions
24/06/26 15:32:54 INFO DAGScheduler: Final stage: ResultStage 4 (collect at /vagrant_data/spark_app2.py:100)
24/06/26 15:32:54 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:32:54 INFO DAGScheduler: Missing parents: List()
24/06/26 15:32:54 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at collect at /vagrant_data/spark_app2.py:100), which has no missing parents
24/06/26 15:32:54 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.8 KiB, free 434.4 MiB)
24/06/26 15:32:54 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 434.3 MiB)
24/06/26 15:32:54 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:34733 (size: 5.2 KiB, free: 434.4 MiB)
24/06/26 15:32:54 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 15:32:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at collect at /vagrant_data/spark_app2.py:100) (first 15 tasks are for partitions Vector(0))
24/06/26 15:32:54 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 15:32:54 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10142 bytes) 
24/06/26 15:32:54 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 15:32:54 INFO CodeGenerator: Code generated in 92.700255 ms
24/06/26 15:32:54 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:34733 in memory (size: 11.3 KiB, free: 434.4 MiB)
24/06/26 15:32:54 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1828 bytes result sent to driver
24/06/26 15:32:54 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 198 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:32:54 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 15:32:54 INFO DAGScheduler: ResultStage 4 (collect at /vagrant_data/spark_app2.py:100) finished in 0.249 s
24/06/26 15:32:54 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:32:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
24/06/26 15:32:54 INFO DAGScheduler: Job 4 finished: collect at /vagrant_data/spark_app2.py:100, took 0.266093 s
127.0.0.1 - - [26/Jun/2024 15:32:54] "GET /logs?container_name=nginx-container HTTP/1.1" 200 -
24/06/26 15:33:01 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:34733 in memory (size: 5.2 KiB, free: 434.4 MiB)
24/06/26 15:33:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 15:33:27] "GET /logs?limit=10 HTTP/1.1" 200 -
24/06/26 15:33:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:33:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
127.0.0.1 - - [26/Jun/2024 15:33:52] "GET /logs??start_time=2024-06-26T15:00:00 HTTP/1.1" 200 -
24/06/26 15:34:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:30 INFO CodeGenerator: Code generated in 106.645049 ms
24/06/26 15:34:30 INFO DAGScheduler: Registering RDD 23 (collect at /vagrant_data/spark_app2.py:142) as input to shuffle 0
24/06/26 15:34:30 INFO DAGScheduler: Got map stage job 5 (collect at /vagrant_data/spark_app2.py:142) with 1 output partitions
24/06/26 15:34:30 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at /vagrant_data/spark_app2.py:142)
24/06/26 15:34:30 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:34:30 INFO DAGScheduler: Missing parents: List()
24/06/26 15:34:30 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[23] at collect at /vagrant_data/spark_app2.py:142), which has no missing parents
24/06/26 15:34:30 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 35.1 KiB, free 434.4 MiB)
24/06/26 15:34:30 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 434.4 MiB)
24/06/26 15:34:30 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:34733 (size: 16.0 KiB, free: 434.4 MiB)
24/06/26 15:34:30 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 15:34:30 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[23] at collect at /vagrant_data/spark_app2.py:142) (first 15 tasks are for partitions Vector(0))
24/06/26 15:34:30 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
24/06/26 15:34:30 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10131 bytes) 
24/06/26 15:34:30 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
24/06/26 15:34:30 INFO CodeGenerator: Code generated in 122.600401 ms
24/06/26 15:34:30 INFO CodeGenerator: Code generated in 10.683652 ms
24/06/26 15:34:30 INFO CodeGenerator: Code generated in 8.132282 ms
24/06/26 15:34:31 INFO CodeGenerator: Code generated in 10.866063 ms
24/06/26 15:34:31 INFO CodeGenerator: Code generated in 23.379297 ms
24/06/26 15:34:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2488 bytes result sent to driver
24/06/26 15:34:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 416 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:34:31 INFO DAGScheduler: ShuffleMapStage 5 (collect at /vagrant_data/spark_app2.py:142) finished in 0.479 s
24/06/26 15:34:31 INFO DAGScheduler: looking for newly runnable stages
24/06/26 15:34:31 INFO DAGScheduler: running: Set()
24/06/26 15:34:31 INFO DAGScheduler: waiting: Set()
24/06/26 15:34:31 INFO DAGScheduler: failed: Set()
24/06/26 15:34:31 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
24/06/26 15:34:31 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/06/26 15:34:31 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/06/26 15:34:31 INFO CodeGenerator: Code generated in 20.075009 ms
24/06/26 15:34:31 INFO SparkContext: Starting job: collect at /vagrant_data/spark_app2.py:142
24/06/26 15:34:31 INFO DAGScheduler: Got job 6 (collect at /vagrant_data/spark_app2.py:142) with 1 output partitions
24/06/26 15:34:31 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /vagrant_data/spark_app2.py:142)
24/06/26 15:34:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
24/06/26 15:34:31 INFO DAGScheduler: Missing parents: List()
24/06/26 15:34:31 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[26] at collect at /vagrant_data/spark_app2.py:142), which has no missing parents
24/06/26 15:34:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 36.7 KiB, free 434.3 MiB)
24/06/26 15:34:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 434.3 MiB)
24/06/26 15:34:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:34733 (size: 16.8 KiB, free: 434.4 MiB)
24/06/26 15:34:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
24/06/26 15:34:31 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:34733 in memory (size: 16.0 KiB, free: 434.4 MiB)
24/06/26 15:34:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[26] at collect at /vagrant_data/spark_app2.py:142) (first 15 tasks are for partitions Vector(0))
24/06/26 15:34:31 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
24/06/26 15:34:31 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 15:34:31 INFO Executor: Running task 0.0 in stage 7.0 (TID 6)
24/06/26 15:34:31 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 15:34:31 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
24/06/26 15:34:31 INFO CodeGenerator: Code generated in 18.890446 ms
24/06/26 15:34:31 INFO Executor: Finished task 0.0 in stage 7.0 (TID 6). 4948 bytes result sent to driver
24/06/26 15:34:31 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 126 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:34:31 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
24/06/26 15:34:31 INFO DAGScheduler: ResultStage 7 (collect at /vagrant_data/spark_app2.py:142) finished in 0.167 s
24/06/26 15:34:31 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:34:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
24/06/26 15:34:31 INFO DAGScheduler: Job 6 finished: collect at /vagrant_data/spark_app2.py:142, took 0.190356 s
127.0.0.1 - - [26/Jun/2024 15:34:31] "GET /log-stats HTTP/1.1" 500 -
24/06/26 15:34:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:35 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.2.15:34733 in memory (size: 16.8 KiB, free: 434.4 MiB)
24/06/26 15:34:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:34:57 INFO NetworkClient: [AdminClient clientId=adminclient-2] Node -1 disconnected.
24/06/26 15:34:57 INFO NetworkClient: [AdminClient clientId=adminclient-1] Node -1 disconnected.
24/06/26 15:35:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:35:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:36:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:37:59 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:37:59 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2, groupId=spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:37:59 INFO Metrics: Metrics scheduler closed
24/06/26 15:37:59 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:37:59 INFO Metrics: Metrics reporters closed
24/06/26 15:37:59 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-fafde017-2d5e-4dab-856e-12db6382d4f7--1016489929-executor-2 unregistered
24/06/26 15:38:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:38:00 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1, groupId=spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:38:00 INFO Metrics: Metrics scheduler closed
24/06/26 15:38:00 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:38:00 INFO Metrics: Metrics reporters closed
24/06/26 15:38:00 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-b2e9bb47-aa77-4b8f-b2cf-3eb392ff8bbf-1064272032-executor-1 unregistered
24/06/26 15:38:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:04 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 15:38:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 15:38:04 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 15:38:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 15:38:04 INFO MemoryStore: MemoryStore cleared
24/06/26 15:38:04 INFO BlockManager: BlockManager stopped
24/06/26 15:38:04 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 15:38:04 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 15:38:04 INFO SparkContext: Successfully stopped SparkContext
24/06/26 15:38:04 INFO ShutdownHookManager: Shutdown hook called
24/06/26 15:38:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf/pyspark-c08df227-2d1a-40b9-9b6c-b8bf0b05c381
24/06/26 15:38:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-87d874a2-6f31-49a3-9f8a-1a76206dcc0a
24/06/26 15:38:04 INFO ShutdownHookManager: Deleting directory /tmp/temporary-fe51bc37-8d48-4728-91b3-be774c33e6b7
24/06/26 15:38:04 INFO ShutdownHookManager: Deleting directory /tmp/temporary-4366849e-643c-4bca-835b-00ede4a997b2
24/06/26 15:38:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-370395fb-26dc-445a-8f75-46880cf231cf
Starting Spark at Wed Jun 26 03:38:16 PM UTC 2024
24/06/26 15:38:18 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 15:38:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-52c0b1b2-e77e-4c37-95e5-3865da35579a;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 523ms :: artifacts dl 13ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-52c0b1b2-e77e-4c37-95e5-3865da35579a
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/8ms)
24/06/26 15:38:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 15:38:21 INFO SparkContext: Running Spark version 3.5.1
24/06/26 15:38:21 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:38:21 INFO SparkContext: Java version 11.0.20.1
24/06/26 15:38:21 INFO ResourceUtils: ==============================================================
24/06/26 15:38:21 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 15:38:21 INFO ResourceUtils: ==============================================================
24/06/26 15:38:21 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 15:38:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 15:38:21 INFO ResourceProfile: Limiting resource is cpu
24/06/26 15:38:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 15:38:21 INFO SecurityManager: Changing view acls to: root
24/06/26 15:38:21 INFO SecurityManager: Changing modify acls to: root
24/06/26 15:38:21 INFO SecurityManager: Changing view acls groups to: 
24/06/26 15:38:21 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 15:38:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 15:38:21 INFO Utils: Successfully started service 'sparkDriver' on port 37447.
24/06/26 15:38:21 INFO SparkEnv: Registering MapOutputTracker
24/06/26 15:38:21 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 15:38:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 15:38:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 15:38:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 15:38:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c9e21bcc-d6ca-4aa3-ae31-3c024b1bf31d
24/06/26 15:38:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 15:38:21 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 15:38:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 15:38:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:37447/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:37447/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:37447/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:37447/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:37447/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:37447/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:37447/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:37447/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:37447/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:37447/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:37447/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:38:22 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:38:22 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 15:38:22 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:38:22 INFO Executor: Java version 11.0.20.1
24/06/26 15:38:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 15:38:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2330441f for default.
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:38:22 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:38:22 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:37447 after 33 ms (0 ms spent in bootstraps)
24/06/26 15:38:22 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp13527025200056778972.tmp
24/06/26 15:38:22 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp13527025200056778972.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:38:22 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 15:38:22 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp1593649362058290522.tmp
24/06/26 15:38:22 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp1593649362058290522.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:38:22 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 15:38:22 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp12985736630234909456.tmp
24/06/26 15:38:22 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp12985736630234909456.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:38:22 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 15:38:22 INFO Executor: Fetching spark://10.0.2.15:37447/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Fetching spark://10.0.2.15:37447/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp6289996888006202905.tmp
24/06/26 15:38:22 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp6289996888006202905.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:38:22 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 15:38:22 INFO Executor: Fetching spark://10.0.2.15:37447/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Fetching spark://10.0.2.15:37447/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp16192531054261318553.tmp
24/06/26 15:38:22 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp16192531054261318553.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:38:22 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 15:38:22 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:22 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp7455322436877385460.tmp
24/06/26 15:38:23 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp7455322436877385460.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:38:23 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 15:38:23 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416301125
24/06/26 15:38:23 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp770561113248901151.tmp
24/06/26 15:38:23 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp770561113248901151.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:38:23 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 15:38:23 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:23 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp2364415629203133121.tmp
24/06/26 15:38:23 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp2364415629203133121.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:38:23 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:38:23 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416301125
24/06/26 15:38:23 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp8099217537091814230.tmp
24/06/26 15:38:23 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp8099217537091814230.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:38:23 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 15:38:23 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416301125
24/06/26 15:38:23 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp6669302657616181609.tmp
24/06/26 15:38:23 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp6669302657616181609.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:38:23 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:38:23 INFO Executor: Fetching spark://10.0.2.15:37447/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416301125
24/06/26 15:38:23 INFO Utils: Fetching spark://10.0.2.15:37447/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp10768639136784287415.tmp
24/06/26 15:38:23 INFO Utils: /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/fetchFileTemp10768639136784287415.tmp has been previously copied to /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:38:23 INFO Executor: Adding file:/tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/userFiles-5200efb2-6d1c-4a82-a97c-d5cb035bb7a6/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 15:38:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36899.
24/06/26 15:38:23 INFO NettyBlockTransferService: Server created on 10.0.2.15:36899
24/06/26 15:38:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 15:38:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 36899, None)
24/06/26 15:38:23 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:36899 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 36899, None)
24/06/26 15:38:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 36899, None)
24/06/26 15:38:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 36899, None)
24/06/26 15:38:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 15:38:23 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 15:38:25 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 15:38:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:38:25 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b resolved to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b.
24/06/26 15:38:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:38:25 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/metadata using temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/.metadata.c72ea032-fbea-4069-bb37-ac7840891a75.tmp
24/06/26 15:38:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/.metadata.c72ea032-fbea-4069-bb37-ac7840891a75.tmp to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/metadata
24/06/26 15:38:26 INFO MicroBatchExecution: Starting logs_table [id = fdc6e7a9-ce7b-45aa-96b8-0b391dbb0cfc, runId = e5fad2a3-1a47-4d84-a276-0cea1b0b71e9]. Use file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b to store the query checkpoint.
24/06/26 15:38:26 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@a42831b] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@598912a9]
24/06/26 15:38:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:38:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:38:26 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:38:26 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:38:26 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:38:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:38:26 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956 resolved to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956.
24/06/26 15:38:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:38:26 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/metadata using temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/.metadata.90f3b262-5942-4552-8a3f-312d79ce8c40.tmp
24/06/26 15:38:26 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/.metadata.90f3b262-5942-4552-8a3f-312d79ce8c40.tmp to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/metadata
24/06/26 15:38:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:38:26 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:38:26 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:38:26 INFO AppInfoParser: Kafka startTimeMs: 1719416306819
24/06/26 15:38:26 INFO MicroBatchExecution: Starting [id = e897e2c5-1ef6-44e2-9d8a-816d9a399e2c, runId = b22a9517-334f-426d-9b46-b678497f51f7]. Use file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956 to store the query checkpoint.
24/06/26 15:38:26 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@a42831b] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@598912a9]
24/06/26 15:38:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:38:26 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:38:26 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:38:26 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:38:26 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:38:26 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:38:26 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:38:26 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:38:26 INFO AppInfoParser: Kafka startTimeMs: 1719416306910
 * Serving Flask app 'spark_app2'
 * Debug mode: off
24/06/26 15:38:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/sources/0/0 using temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/sources/0/.0.6e761a31-e2dd-4e77-a78d-7902fdbe3e06.tmp
24/06/26 15:38:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/sources/0/0 using temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/sources/0/.0.17af2975-6f4a-4f54-b72f-09547eade2a6.tmp
24/06/26 15:38:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/sources/0/.0.6e761a31-e2dd-4e77-a78d-7902fdbe3e06.tmp to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/sources/0/0
24/06/26 15:38:27 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":295}}
24/06/26 15:38:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/sources/0/.0.17af2975-6f4a-4f54-b72f-09547eade2a6.tmp to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/sources/0/0
24/06/26 15:38:27 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":295}}
24/06/26 15:38:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/offsets/0 using temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/offsets/.0.e7e76358-5813-4a74-9ab9-ee13d2bea2f4.tmp
24/06/26 15:38:27 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/offsets/0 using temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/offsets/.0.28416ec8-9605-4c7e-96e9-9c6db0ac26fb.tmp
24/06/26 15:38:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/offsets/.0.e7e76358-5813-4a74-9ab9-ee13d2bea2f4.tmp to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/offsets/0
24/06/26 15:38:27 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719416307518,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:38:27 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/offsets/.0.28416ec8-9605-4c7e-96e9-9c6db0ac26fb.tmp to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/offsets/0
24/06/26 15:38:27 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719416307536,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:27 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:38:28 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 15:38:28 INFO CodeGenerator: Code generated in 319.043419 ms
24/06/26 15:38:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@45030cf0]. The input RDD has 1 partitions.
24/06/26 15:38:28 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:38:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:38:28 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:38:28 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:38:28 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:38:28 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:38:28 INFO DAGScheduler: Missing parents: List()
24/06/26 15:38:28 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:38:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/26 15:38:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 434.4 MiB)
24/06/26 15:38:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:36899 (size: 2.5 KiB, free: 434.4 MiB)
24/06/26 15:38:29 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 15:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:38:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 15:38:29 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:38:29 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:38:29 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:38:29 INFO DAGScheduler: Missing parents: List()
24/06/26 15:38:29 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:38:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 15:38:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 15:38:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:36899 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:38:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 15:38:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:38:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 15:38:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:38:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:38:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 15:38:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 15:38:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:38:29 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 15:38:29 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:38:29 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 15:38:29 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/26 15:38:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1252 bytes result sent to driver
24/06/26 15:38:29 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 243 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:38:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 15:38:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 181 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:38:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 15:38:29 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.540 s
24/06/26 15:38:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/06/26 15:38:29 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.618491 s
24/06/26 15:38:29 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.271 s
24/06/26 15:38:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@45030cf0] is committing.
24/06/26 15:38:29 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:38:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@45030cf0] committed.
24/06/26 15:38:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 15:38:29 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.676241 s
24/06/26 15:38:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 0
-------------------------------------------
24/06/26 15:38:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/commits/0 using temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/commits/.0.16aa123d-6d92-4cab-b1c1-393671af8161.tmp
24/06/26 15:38:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:36899 in memory (size: 2.5 KiB, free: 434.4 MiB)
24/06/26 15:38:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/commits/.0.16aa123d-6d92-4cab-b1c1-393671af8161.tmp to file:/tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b/commits/0
+----------+---------+-------+---------+----+------+
|@timestamp|log_level|message|container|host|stream|
+----------+---------+-------+---------+----+------+
+----------+---------+-------+---------+----+------+

24/06/26 15:38:29 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:38:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "fdc6e7a9-ce7b-45aa-96b8-0b391dbb0cfc",
  "runId" : "e5fad2a3-1a47-4d84-a276-0cea1b0b71e9",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:38:26.202Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1525,
    "commitOffsets" : 79,
    "getBatch" : 32,
    "latestOffset" : 1291,
    "queryPlanning" : 230,
    "triggerExecution" : 3284,
    "walCommit" : 90
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 15:38:29 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/commits/0 using temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/commits/.0.ca06ea8e-da19-46d8-bc47-dcf6cd7b2d73.tmp
24/06/26 15:38:29 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/commits/.0.ca06ea8e-da19-46d8-bc47-dcf6cd7b2d73.tmp to file:/tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956/commits/0
24/06/26 15:38:29 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "e897e2c5-1ef6-44e2-9d8a-816d9a399e2c",
  "runId" : "b22a9517-334f-426d-9b46-b678497f51f7",
  "name" : null,
  "timestamp" : "2024-06-26T15:38:26.883Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1651,
    "commitOffsets" : 97,
    "getBatch" : 11,
    "latestOffset" : 650,
    "queryPlanning" : 229,
    "triggerExecution" : 2745,
    "walCommit" : 96
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@54b7708a",
    "numOutputRows" : 0
  }
}
24/06/26 15:38:37 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:36899 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:38:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:38:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:39:59 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:09 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:19 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:29 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:39 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:49 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:40:57 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 15:40:57 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 15:40:57 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 15:40:57 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 15:40:57 INFO MemoryStore: MemoryStore cleared
24/06/26 15:40:57 INFO BlockManager: BlockManager stopped
24/06/26 15:40:57 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 15:40:57 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 15:40:57 INFO SparkContext: Successfully stopped SparkContext
24/06/26 15:40:57 INFO ShutdownHookManager: Shutdown hook called
24/06/26 15:40:57 INFO ShutdownHookManager: Deleting directory /tmp/temporary-41a12e54-2f95-4f48-91a3-0a9e0a16b14b
24/06/26 15:40:57 INFO ShutdownHookManager: Deleting directory /tmp/temporary-e65a4bff-0c91-4820-a7f6-2dfe02a24956
24/06/26 15:40:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-d2efa4d3-6134-47d4-9b6a-d8c927fb8be4
24/06/26 15:40:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f
24/06/26 15:40:57 INFO ShutdownHookManager: Deleting directory /tmp/spark-93952aed-71c2-4f03-ac29-37b250b2f26f/pyspark-923ab643-a230-46c1-bff0-dddddc91ed38
Starting Spark at Wed Jun 26 03:41:34 PM UTC 2024
24/06/26 15:41:36 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/26 15:41:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/usr/local/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-6b1855ad-ec89-483f-b90e-5b81e3303ca9;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
	found org.apache.kafka#kafka-clients;3.4.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.10.3 in central
	found org.slf4j#slf4j-api;2.0.7 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
	found org.apache.hadoop#hadoop-client-api;3.3.4 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 492ms :: artifacts dl 13ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-6b1855ad-ec89-483f-b90e-5b81e3303ca9
	confs: [default]
	0 artifacts copied, 11 already retrieved (0kB/9ms)
24/06/26 15:41:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/26 15:41:39 INFO SparkContext: Running Spark version 3.5.1
24/06/26 15:41:39 INFO SparkContext: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:41:39 INFO SparkContext: Java version 11.0.20.1
24/06/26 15:41:39 INFO ResourceUtils: ==============================================================
24/06/26 15:41:39 INFO ResourceUtils: No custom resources configured for spark.driver.
24/06/26 15:41:39 INFO ResourceUtils: ==============================================================
24/06/26 15:41:39 INFO SparkContext: Submitted application: KafkaConnectivityTest
24/06/26 15:41:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/06/26 15:41:39 INFO ResourceProfile: Limiting resource is cpu
24/06/26 15:41:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/06/26 15:41:39 INFO SecurityManager: Changing view acls to: root
24/06/26 15:41:39 INFO SecurityManager: Changing modify acls to: root
24/06/26 15:41:39 INFO SecurityManager: Changing view acls groups to: 
24/06/26 15:41:39 INFO SecurityManager: Changing modify acls groups to: 
24/06/26 15:41:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
24/06/26 15:41:39 INFO Utils: Successfully started service 'sparkDriver' on port 43891.
24/06/26 15:41:39 INFO SparkEnv: Registering MapOutputTracker
24/06/26 15:41:39 INFO SparkEnv: Registering BlockManagerMaster
24/06/26 15:41:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/06/26 15:41:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/06/26 15:41:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/06/26 15:41:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f0fe2c98-dd5b-4167-b438-5a4965923b3c
24/06/26 15:41:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/06/26 15:41:39 INFO SparkEnv: Registering OutputCommitCoordinator
24/06/26 15:41:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/06/26 15:41:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:43891/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://10.0.2.15:43891/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://10.0.2.15:43891/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://10.0.2.15:43891/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://10.0.2.15:43891/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://10.0.2.15:43891/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://10.0.2.15:43891/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://10.0.2.15:43891/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://10.0.2.15:43891/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://10.0.2.15:43891/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://10.0.2.15:43891/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:41:40 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:41:40 INFO Executor: Starting executor ID driver on host 10.0.2.15
24/06/26 15:41:40 INFO Executor: OS info Linux, 5.15.0-56-generic, amd64
24/06/26 15:41:40 INFO Executor: Java version 11.0.20.1
24/06/26 15:41:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
24/06/26 15:41:40 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7fe39fdf for default.
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:41:40 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO TransportClientFactory: Successfully created connection to /10.0.2.15:43891 after 34 ms (0 ms spent in bootstraps)
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp13376252314066352904.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp13376252314066352904.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.slf4j_slf4j-api-2.0.7.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.slf4j_slf4j-api-2.0.7.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp703219558077147052.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp703219558077147052.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.commons_commons-pool2-2.11.1.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp3149578177099314212.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp3149578177099314212.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/commons-logging_commons-logging-1.1.3.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp1970148142871878129.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp1970148142871878129.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-api-3.3.4.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp11432242578248101141.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp11432242578248101141.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.kafka_kafka-clients-3.4.1.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp12169989341967223224.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp12169989341967223224.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp15002344577339953226.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp15002344577339953226.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.lz4_lz4-java-1.8.0.jar
24/06/26 15:41:40 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.lz4_lz4-java-1.8.0.jar to class loader default
24/06/26 15:41:40 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1719416499138
24/06/26 15:41:40 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp983492630786632490.tmp
24/06/26 15:41:40 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp983492630786632490.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
24/06/26 15:41:41 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
24/06/26 15:41:41 INFO Executor: Fetching spark://10.0.2.15:43891/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719416499138
24/06/26 15:41:41 INFO Utils: Fetching spark://10.0.2.15:43891/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp9105354390669305643.tmp
24/06/26 15:41:41 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp9105354390669305643.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/com.google.code.findbugs_jsr305-3.0.0.jar
24/06/26 15:41:41 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/26 15:41:41 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719416499138
24/06/26 15:41:41 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp4928308945102274197.tmp
24/06/26 15:41:41 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp4928308945102274197.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
24/06/26 15:41:41 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
24/06/26 15:41:41 INFO Executor: Fetching spark://10.0.2.15:43891/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719416499138
24/06/26 15:41:41 INFO Utils: Fetching spark://10.0.2.15:43891/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp13094901607240462004.tmp
24/06/26 15:41:41 INFO Utils: /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/fetchFileTemp13094901607240462004.tmp has been previously copied to /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.xerial.snappy_snappy-java-1.1.10.3.jar
24/06/26 15:41:41 INFO Executor: Adding file:/tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/userFiles-295f8956-2f95-4793-bd88-7985cff7b68f/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/26 15:41:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37971.
24/06/26 15:41:41 INFO NettyBlockTransferService: Server created on 10.0.2.15:37971
24/06/26 15:41:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/06/26 15:41:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 37971, None)
24/06/26 15:41:41 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:37971 with 434.4 MiB RAM, BlockManagerId(driver, 10.0.2.15, 37971, None)
24/06/26 15:41:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 37971, None)
24/06/26 15:41:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 37971, None)
24/06/26 15:41:41 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/06/26 15:41:41 INFO SharedState: Warehouse path is 'file:/vagrant_data/spark-warehouse'.
24/06/26 15:41:43 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/06/26 15:41:43 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:41:43 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067 resolved to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067.
24/06/26 15:41:43 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:41:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/metadata using temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/.metadata.fe495039-d6fa-457a-b07b-6dda63d55253.tmp
24/06/26 15:41:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/.metadata.fe495039-d6fa-457a-b07b-6dda63d55253.tmp to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/metadata
24/06/26 15:41:44 INFO MicroBatchExecution: Starting logs_table [id = 4781df0e-a809-44c2-a479-efde9692f6ac, runId = 92e4e14f-8e0f-4b9c-9196-231c9aaa3bec]. Use file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067 to store the query checkpoint.
24/06/26 15:41:44 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@9a74caa] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2a5c0e83]
24/06/26 15:41:44 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:41:44 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:41:44 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:41:44 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:41:44 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
24/06/26 15:41:44 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66 resolved to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66.
24/06/26 15:41:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/06/26 15:41:44 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:41:44 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/metadata using temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/.metadata.0f80ccd3-5697-459a-a72e-b406ee3766a0.tmp
24/06/26 15:41:44 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/.metadata.0f80ccd3-5697-459a-a72e-b406ee3766a0.tmp to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/metadata
24/06/26 15:41:44 INFO MicroBatchExecution: Starting [id = 0c9b2e32-fe18-479b-9cdd-d185b30234c2, runId = 328f669b-2f14-4615-a803-005163a36d2a]. Use file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66 to store the query checkpoint.
24/06/26 15:41:44 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@9a74caa] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2a5c0e83]
24/06/26 15:41:44 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:41:44 INFO OffsetSeqLog: BatchIds found from listing: 
24/06/26 15:41:44 INFO MicroBatchExecution: Starting new streaming query.
24/06/26 15:41:44 INFO MicroBatchExecution: Stream started from {}
24/06/26 15:41:44 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [192.168.33.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/06/26 15:41:44 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:41:44 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
24/06/26 15:41:44 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:41:44 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:41:44 INFO AppInfoParser: Kafka startTimeMs: 1719416504824
24/06/26 15:41:44 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:41:44 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:41:44 INFO AppInfoParser: Kafka startTimeMs: 1719416504831
 * Serving Flask app 'spark_app2'
 * Debug mode: off
24/06/26 15:41:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/sources/0/0 using temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/sources/0/.0.41a77cb5-cd39-4412-b789-86657bf98e0b.tmp
24/06/26 15:41:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/sources/0/0 using temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/sources/0/.0.c2887904-9c8e-44ff-b7a7-4b98579e7b5d.tmp
24/06/26 15:41:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/sources/0/.0.41a77cb5-cd39-4412-b789-86657bf98e0b.tmp to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/sources/0/0
24/06/26 15:41:45 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":295}}
24/06/26 15:41:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/sources/0/.0.c2887904-9c8e-44ff-b7a7-4b98579e7b5d.tmp to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/sources/0/0
24/06/26 15:41:45 INFO KafkaMicroBatchStream: Initial offsets: {"logs":{"0":295}}
24/06/26 15:41:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/0 using temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/.0.34fb5281-8111-44b5-88b0-7f058d4b98de.tmp
24/06/26 15:41:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/0 using temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/.0.e71766d5-4a79-42c0-b0a1-716139153dd3.tmp
24/06/26 15:41:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/.0.34fb5281-8111-44b5-88b0-7f058d4b98de.tmp to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/0
24/06/26 15:41:45 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719416505403,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:41:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/.0.e71766d5-4a79-42c0-b0a1-716139153dd3.tmp to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/0
24/06/26 15:41:45 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719416505420,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:41:45 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
24/06/26 15:41:46 INFO CodeGenerator: Code generated in 297.832233 ms
24/06/26 15:41:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@12303558]. The input RDD has 1 partitions.
24/06/26 15:41:46 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:41:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:41:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:41:46 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:41:46 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:41:46 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:41:46 INFO DAGScheduler: Missing parents: List()
24/06/26 15:41:46 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:41:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.5 KiB, free 434.4 MiB)
24/06/26 15:41:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.5 KiB, free 434.4 MiB)
24/06/26 15:41:46 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:37971 (size: 2.5 KiB, free: 434.4 MiB)
24/06/26 15:41:46 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/06/26 15:41:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[8] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:41:46 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/06/26 15:41:46 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:41:46 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:41:46 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:41:46 INFO DAGScheduler: Missing parents: List()
24/06/26 15:41:46 INFO DAGScheduler: Submitting ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:41:46 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.4 KiB, free 434.4 MiB)
24/06/26 15:41:46 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2035.0 B, free 434.4 MiB)
24/06/26 15:41:46 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:37971 (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:41:46 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/06/26 15:41:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (ParallelCollectionRDD[9] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:41:46 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/06/26 15:41:46 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:41:46 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 9652 bytes) 
24/06/26 15:41:46 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/26 15:41:46 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
24/06/26 15:41:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:41:46 INFO DataWritingSparkTask: Committed partition 0 (task 1, attempt 0, stage 1.0)
24/06/26 15:41:46 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1252 bytes result sent to driver
24/06/26 15:41:46 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 133 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:41:46 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:41:46 INFO DataWritingSparkTask: Committed partition 0 (task 0, attempt 0, stage 0.0)
24/06/26 15:41:46 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/06/26 15:41:46 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.214 s
24/06/26 15:41:46 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1353 bytes result sent to driver
24/06/26 15:41:46 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 225 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:41:46 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/06/26 15:41:46 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:41:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/06/26 15:41:46 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.449 s
24/06/26 15:41:46 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.501343 s
24/06/26 15:41:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
24/06/26 15:41:46 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:41:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
-------------------------------------------
Batch: 024/06/26 15:41:46 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 0.506991 s

-------------------------------------------
24/06/26 15:41:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@12303558] is committing.
24/06/26 15:41:46 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@12303558] committed.
24/06/26 15:41:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/0 using temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/.0.fdf06cab-890d-43be-bb22-61f0096bfeb4.tmp
+----------+---------+-------+---------+----+------+
|@timestamp|log_level|message|container|host|stream|
+----------+---------+-------+---------+----+------+
+----------+---------+-------+---------+----+------+

24/06/26 15:41:47 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 0, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:41:47 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/0 using temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/.0.4eb69d29-73bc-4ce6-a300-91f12691b17a.tmp
24/06/26 15:41:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/.0.fdf06cab-890d-43be-bb22-61f0096bfeb4.tmp to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/0
24/06/26 15:41:47 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/.0.4eb69d29-73bc-4ce6-a300-91f12691b17a.tmp to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/0
24/06/26 15:41:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "0c9b2e32-fe18-479b-9cdd-d185b30234c2",
  "runId" : "328f669b-2f14-4615-a803-005163a36d2a",
  "name" : null,
  "timestamp" : "2024-06-26T15:41:44.784Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1343,
    "commitOffsets" : 61,
    "getBatch" : 28,
    "latestOffset" : 633,
    "queryPlanning" : 202,
    "triggerExecution" : 2356,
    "walCommit" : 78
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@6e5f2965",
    "numOutputRows" : 0
  }
}
24/06/26 15:41:47 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4781df0e-a809-44c2-a479-efde9692f6ac",
  "runId" : "92e4e14f-8e0f-4b9c-9196-231c9aaa3bec",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:41:44.234Z",
  "batchId" : 0,
  "numInputRows" : 0,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.0,
  "durationMs" : {
    "addBatch" : 1259,
    "commitOffsets" : 115,
    "getBatch" : 31,
    "latestOffset" : 1149,
    "queryPlanning" : 194,
    "triggerExecution" : 2868,
    "walCommit" : 87
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : null,
    "endOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "numInputRows" : 0,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.0,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 0
  }
}
24/06/26 15:41:47 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.2.15:37971 in memory (size: 2035.0 B, free: 434.4 MiB)
24/06/26 15:41:47 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.2.15:37971 in memory (size: 2.5 KiB, free: 434.4 MiB)
24/06/26 15:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:41:57 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:07 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:17 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/1 using temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/.1.30bcd092-b5ab-480c-b8e6-b5b0418121b0.tmp
24/06/26 15:42:21 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/1 using temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/.1.b74655c4-0c6d-433e-bcbc-dd3bf8e6be32.tmp
24/06/26 15:42:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/.1.30bcd092-b5ab-480c-b8e6-b5b0418121b0.tmp to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/offsets/1
24/06/26 15:42:21 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719416541062,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:42:21 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/.1.b74655c4-0c6d-433e-bcbc-dd3bf8e6be32.tmp to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/offsets/1
24/06/26 15:42:21 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1719416541063,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/06/26 15:42:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]]. The input RDD has 1 partitions.
24/06/26 15:42:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:42:21 INFO WriteToDataSourceV2Exec: Start processing data source write support: MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@56b95c5b]. The input RDD has 1 partitions.
24/06/26 15:42:21 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/06/26 15:42:21 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:42:21 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:42:21 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:42:21 INFO DAGScheduler: Missing parents: List()
24/06/26 15:42:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:42:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 26.7 KiB, free 434.4 MiB)
24/06/26 15:42:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 11.2 KiB, free 434.4 MiB)
24/06/26 15:42:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:37971 (size: 11.2 KiB, free: 434.4 MiB)
24/06/26 15:42:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
24/06/26 15:42:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:42:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
24/06/26 15:42:21 INFO DAGScheduler: Got job 3 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/06/26 15:42:21 INFO DAGScheduler: Final stage: ResultStage 3 (start at NativeMethodAccessorImpl.java:0)
24/06/26 15:42:21 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:42:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:42:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
24/06/26 15:42:21 INFO DAGScheduler: Missing parents: List()
24/06/26 15:42:21 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/06/26 15:42:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 26.9 KiB, free 434.3 MiB)
24/06/26 15:42:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 11.3 KiB, free 434.3 MiB)
24/06/26 15:42:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:37971 (size: 11.3 KiB, free: 434.4 MiB)
24/06/26 15:42:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
24/06/26 15:42:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[17] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/26 15:42:21 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
24/06/26 15:42:21 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10570 bytes) 
24/06/26 15:42:21 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
24/06/26 15:42:21 INFO CodeGenerator: Code generated in 40.671672 ms
24/06/26 15:42:21 INFO CodeGenerator: Code generated in 26.528375 ms
24/06/26 15:42:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=295 untilOffset=296, for query queryId=0c9b2e32-fe18-479b-9cdd-d185b30234c2 batchId=1 taskId=2 partitionId=0
24/06/26 15:42:21 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=logs-0 fromOffset=295 untilOffset=296, for query queryId=4781df0e-a809-44c2-a479-efde9692f6ac batchId=1 taskId=3 partitionId=0
24/06/26 15:42:21 INFO CodeGenerator: Code generated in 15.263215 ms
24/06/26 15:42:21 INFO CodeGenerator: Code generated in 30.299795 ms
24/06/26 15:42:21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:42:21 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [192.168.33.1:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

24/06/26 15:42:21 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:42:21 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:42:21 INFO AppInfoParser: Kafka startTimeMs: 1719416541693
24/06/26 15:42:21 INFO AppInfoParser: Kafka version: 3.4.1
24/06/26 15:42:21 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/26 15:42:21 INFO AppInfoParser: Kafka startTimeMs: 1719416541694
24/06/26 15:42:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Assigned to partition(s): logs-0
24/06/26 15:42:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Assigned to partition(s): logs-0
24/06/26 15:42:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Seeking to offset 295 for partition logs-0
24/06/26 15:42:21 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Seeking to offset 295 for partition logs-0
24/06/26 15:42:21 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:42:21 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:42:21 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Resetting the last seen epoch of partition logs-0 to 0 since the associated topicId changed from null to a5PTWgvgTR-PvUbkbIm0Aw
24/06/26 15:42:21 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Cluster ID: PsZ4IdcHTjKzH6XnBGwNHw
24/06/26 15:42:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:42:21 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Seeking to earliest offset of partition logs-0
24/06/26 15:42:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:42:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:42:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Seeking to latest offset of partition logs-0
24/06/26 15:42:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Seeking to latest offset of partition logs-0
24/06/26 15:42:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=296, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:42:22 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Resetting offset for partition logs-0 to position FetchPosition{offset=296, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[DESKTOP-G8N3TE3.mshome.net:9092 (id: 0 rack: null)], epoch=0}}.
24/06/26 15:42:22 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:42:22 INFO DataWritingSparkTask: Committed partition 0 (task 2, attempt 0, stage 2.0)
24/06/26 15:42:22 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor read 1 records through 1 polls (polled  out 1 records), taking 583735469 nanos, during time span of 736330876 nanos.
24/06/26 15:42:22 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2426 bytes result sent to driver
24/06/26 15:42:22 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1163 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:42:22 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/26 15:42:22 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 1.221 s
24/06/26 15:42:22 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:42:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
24/06/26 15:42:22 INFO DAGScheduler: Job 2 finished: start at NativeMethodAccessorImpl.java:0, took 1.234463 s
24/06/26 15:42:22 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is committing.
-------------------------------------------
Batch: 1
-------------------------------------------
24/06/26 15:42:22 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.2.15:37971 in memory (size: 11.2 KiB, free: 434.4 MiB)
24/06/26 15:42:22 INFO CodeGenerator: Code generated in 7.450495 ms
24/06/26 15:42:23 INFO CodeGenerator: Code generated in 15.382298 ms
+--------------------+---------+--------------------+--------------------+------------------+------+
|          @timestamp|log_level|             message|           container|              host|stream|
+--------------------+---------+--------------------+--------------------+------------------+------+
|2024-06-26T15:42:...|     NULL|172.17.0.1 - - [2...|{nginx-container,...|{vgr-spark-base64}|stdout|
+--------------------+---------+--------------------+--------------------+------------------+------+

24/06/26 15:42:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] committed.
24/06/26 15:42:23 INFO CodeGenerator: Code generated in 45.789949 ms
24/06/26 15:42:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/1 using temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/.1.391acb47-c9d1-4784-9a9e-680c9b767b3a.tmp
24/06/26 15:42:23 INFO DataWritingSparkTask: Writer for partition 0 is committing.
24/06/26 15:42:23 INFO DataWritingSparkTask: Committed partition 0 (task 3, attempt 0, stage 3.0)
24/06/26 15:42:23 INFO KafkaDataConsumer: From Kafka topicPartition=logs-0 groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor read 1 records through 1 polls (polled  out 1 records), taking 584180171 nanos, during time span of 1933595683 nanos.
24/06/26 15:42:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 3996 bytes result sent to driver
24/06/26 15:42:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2333 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:42:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
24/06/26 15:42:23 INFO DAGScheduler: ResultStage 3 (start at NativeMethodAccessorImpl.java:0) finished in 2.349 s
24/06/26 15:42:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:42:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
24/06/26 15:42:23 INFO DAGScheduler: Job 3 finished: start at NativeMethodAccessorImpl.java:0, took 2.439189 s
24/06/26 15:42:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@56b95c5b] is committing.
24/06/26 15:42:23 INFO WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: org.apache.spark.sql.execution.streaming.sources.MemoryStreamingWrite@56b95c5b] committed.
24/06/26 15:42:23 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/1 using temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/.1.2de419ca-38d5-4823-8f80-04a70c5a7004.tmp
24/06/26 15:42:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/.1.391acb47-c9d1-4784-9a9e-680c9b767b3a.tmp to file:/tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66/commits/1
24/06/26 15:42:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "0c9b2e32-fe18-479b-9cdd-d185b30234c2",
  "runId" : "328f669b-2f14-4615-a803-005163a36d2a",
  "name" : null,
  "timestamp" : "2024-06-26T15:42:21.053Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 62.5,
  "processedRowsPerSecond" : 0.37523452157598497,
  "durationMs" : {
    "addBatch" : 2425,
    "commitOffsets" : 136,
    "getBatch" : 0,
    "latestOffset" : 8,
    "queryPlanning" : 26,
    "triggerExecution" : 2665,
    "walCommit" : 68
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 296
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 296
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 62.5,
    "processedRowsPerSecond" : 0.37523452157598497,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "org.apache.spark.sql.execution.streaming.ConsoleTable$@6e5f2965",
    "numOutputRows" : 1
  }
}
24/06/26 15:42:23 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/.1.2de419ca-38d5-4823-8f80-04a70c5a7004.tmp to file:/tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067/commits/1
24/06/26 15:42:23 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4781df0e-a809-44c2-a479-efde9692f6ac",
  "runId" : "92e4e14f-8e0f-4b9c-9196-231c9aaa3bec",
  "name" : "logs_table",
  "timestamp" : "2024-06-26T15:42:21.054Z",
  "batchId" : 1,
  "numInputRows" : 1,
  "inputRowsPerSecond" : 58.8235294117647,
  "processedRowsPerSecond" : 0.37119524870081666,
  "durationMs" : {
    "addBatch" : 2513,
    "commitOffsets" : 77,
    "getBatch" : 0,
    "latestOffset" : 9,
    "queryPlanning" : 27,
    "triggerExecution" : 2694,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[logs]]",
    "startOffset" : {
      "logs" : {
        "0" : 295
      }
    },
    "endOffset" : {
      "logs" : {
        "0" : 296
      }
    },
    "latestOffset" : {
      "logs" : {
        "0" : 296
      }
    },
    "numInputRows" : 1,
    "inputRowsPerSecond" : 58.8235294117647,
    "processedRowsPerSecond" : 0.37119524870081666,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "MemorySink",
    "numOutputRows" : 1
  }
}
24/06/26 15:42:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:42:56 INFO CodeGenerator: Code generated in 88.40087 ms
24/06/26 15:42:56 INFO CodeGenerator: Code generated in 23.010464 ms
10.0.2.2 - - [26/Jun/2024 15:42:56] "GET /logs HTTP/1.1" 200 -
24/06/26 15:43:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:08 INFO CodeGenerator: Code generated in 89.779506 ms
24/06/26 15:43:08 INFO DAGScheduler: Registering RDD 20 (collect at /vagrant_data/spark_app2.py:142) as input to shuffle 0
24/06/26 15:43:08 INFO DAGScheduler: Got map stage job 4 (collect at /vagrant_data/spark_app2.py:142) with 1 output partitions
24/06/26 15:43:08 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (collect at /vagrant_data/spark_app2.py:142)
24/06/26 15:43:08 INFO DAGScheduler: Parents of final stage: List()
24/06/26 15:43:08 INFO DAGScheduler: Missing parents: List()
24/06/26 15:43:08 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[20] at collect at /vagrant_data/spark_app2.py:142), which has no missing parents
24/06/26 15:43:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 35.1 KiB, free 434.3 MiB)
24/06/26 15:43:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.0 KiB, free 434.3 MiB)
24/06/26 15:43:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:37971 (size: 16.0 KiB, free: 434.4 MiB)
24/06/26 15:43:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
24/06/26 15:43:08 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[20] at collect at /vagrant_data/spark_app2.py:142) (first 15 tasks are for partitions Vector(0))
24/06/26 15:43:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
24/06/26 15:43:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 10131 bytes) 
24/06/26 15:43:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
24/06/26 15:43:08 INFO CodeGenerator: Code generated in 89.542833 ms
24/06/26 15:43:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.2.15:37971 in memory (size: 11.3 KiB, free: 434.4 MiB)
24/06/26 15:43:08 INFO CodeGenerator: Code generated in 34.338004 ms
24/06/26 15:43:08 INFO CodeGenerator: Code generated in 12.385685 ms
24/06/26 15:43:08 INFO CodeGenerator: Code generated in 11.072855 ms
24/06/26 15:43:08 INFO CodeGenerator: Code generated in 10.473376 ms
24/06/26 15:43:09 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 2488 bytes result sent to driver
24/06/26 15:43:09 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 461 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:43:09 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
24/06/26 15:43:09 INFO DAGScheduler: ShuffleMapStage 4 (collect at /vagrant_data/spark_app2.py:142) finished in 0.527 s
24/06/26 15:43:09 INFO DAGScheduler: looking for newly runnable stages
24/06/26 15:43:09 INFO DAGScheduler: running: Set()
24/06/26 15:43:09 INFO DAGScheduler: waiting: Set()
24/06/26 15:43:09 INFO DAGScheduler: failed: Set()
24/06/26 15:43:09 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
24/06/26 15:43:09 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
24/06/26 15:43:09 INFO CodeGenerator: Code generated in 16.260949 ms
24/06/26 15:43:09 INFO SparkContext: Starting job: collect at /vagrant_data/spark_app2.py:142
24/06/26 15:43:09 INFO DAGScheduler: Got job 5 (collect at /vagrant_data/spark_app2.py:142) with 1 output partitions
24/06/26 15:43:09 INFO DAGScheduler: Final stage: ResultStage 6 (collect at /vagrant_data/spark_app2.py:142)
24/06/26 15:43:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
24/06/26 15:43:09 INFO DAGScheduler: Missing parents: List()
24/06/26 15:43:09 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at collect at /vagrant_data/spark_app2.py:142), which has no missing parents
24/06/26 15:43:09 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.7 KiB, free 434.3 MiB)
24/06/26 15:43:09 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 16.8 KiB, free 434.3 MiB)
24/06/26 15:43:09 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:37971 (size: 16.8 KiB, free: 434.4 MiB)
24/06/26 15:43:09 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
24/06/26 15:43:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at collect at /vagrant_data/spark_app2.py:142) (first 15 tasks are for partitions Vector(0))
24/06/26 15:43:09 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
24/06/26 15:43:09 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.2.15:37971 in memory (size: 16.0 KiB, free: 434.4 MiB)
24/06/26 15:43:09 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (10.0.2.15, executor driver, partition 0, NODE_LOCAL, 9631 bytes) 
24/06/26 15:43:09 INFO Executor: Running task 0.0 in stage 6.0 (TID 5)
24/06/26 15:43:09 INFO ShuffleBlockFetcherIterator: Getting 1 (80.0 B) non-empty blocks including 1 (80.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/26 15:43:09 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 17 ms
24/06/26 15:43:09 INFO CodeGenerator: Code generated in 22.049315 ms
24/06/26 15:43:09 INFO Executor: Finished task 0.0 in stage 6.0 (TID 5). 4991 bytes result sent to driver
24/06/26 15:43:09 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 207 ms on 10.0.2.15 (executor driver) (1/1)
24/06/26 15:43:09 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
24/06/26 15:43:09 INFO DAGScheduler: ResultStage 6 (collect at /vagrant_data/spark_app2.py:142) finished in 0.303 s
24/06/26 15:43:09 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
24/06/26 15:43:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
24/06/26 15:43:09 INFO DAGScheduler: Job 5 finished: collect at /vagrant_data/spark_app2.py:142, took 0.339962 s
10.0.2.2 - - [26/Jun/2024 15:43:09] "GET /log-stats HTTP/1.1" 200 -
24/06/26 15:43:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:15 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.2.15:37971 in memory (size: 16.8 KiB, free: 434.4 MiB)
24/06/26 15:43:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:43:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/06/26 15:44:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:44:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1, groupId=spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:44:43 INFO Metrics: Metrics scheduler closed
24/06/26 15:44:43 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:44:43 INFO Metrics: Metrics reporters closed
24/06/26 15:44:43 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-8eaf9063-ab33-41ac-9272-b39b9f8c0704--336114308-executor-1 unregistered
24/06/26 15:44:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/26 15:44:43 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2, groupId=spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor] Request joining group due to: consumer pro-actively leaving the group
24/06/26 15:44:43 INFO Metrics: Metrics scheduler closed
24/06/26 15:44:43 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
24/06/26 15:44:43 INFO Metrics: Metrics reporters closed
24/06/26 15:44:43 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-a90ef7ed-1cd8-473a-ad8e-51c8d2d2a47e-1384300445-executor-2 unregistered
24/06/26 15:44:43 INFO SparkContext: Invoking stop() from shutdown hook
24/06/26 15:44:43 INFO SparkContext: SparkContext is stopping with exitCode 0.
24/06/26 15:44:43 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
24/06/26 15:44:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/06/26 15:44:43 INFO MemoryStore: MemoryStore cleared
24/06/26 15:44:43 INFO BlockManager: BlockManager stopped
24/06/26 15:44:43 INFO BlockManagerMaster: BlockManagerMaster stopped
24/06/26 15:44:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/06/26 15:44:43 INFO SparkContext: Successfully stopped SparkContext
24/06/26 15:44:43 INFO ShutdownHookManager: Shutdown hook called
24/06/26 15:44:43 INFO ShutdownHookManager: Deleting directory /tmp/temporary-98096799-164d-49b6-94c4-c808e6c78c66
24/06/26 15:44:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d
24/06/26 15:44:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-18e3f124-3eab-4d63-9e45-cd8f1bff307d/pyspark-44b1281b-a2b2-46b5-a4d3-3bdd8d7ec73e
24/06/26 15:44:43 INFO ShutdownHookManager: Deleting directory /tmp/temporary-98ff227a-38f2-4b61-b630-ccfc3e0e8067
24/06/26 15:44:43 INFO ShutdownHookManager: Deleting directory /tmp/spark-02992243-1be2-4e9c-b5db-1f8630a4cb9b
24/06/26 15:44:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
