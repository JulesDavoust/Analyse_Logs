Starting Flask at Tue Jun 25 10:04:30 PM UTC 2024
Traceback (most recent call last):
  File "/vagrant_data/app.py", line 2, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
Starting Flask at Tue Jun 25 10:04:40 PM UTC 2024
Traceback (most recent call last):
  File "/vagrant_data/app.py", line 2, in <module>
    from pyspark.sql import SparkSession
ModuleNotFoundError: No module named 'pyspark'
Starting Flask at Tue Jun 25 10:08:14 PM UTC 2024
24/06/25 22:08:17 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:08:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/25 22:08:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:08:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
 * Serving Flask app 'app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
10.0.2.2 - - [25/Jun/2024 22:09:51] "GET / HTTP/1.1" 404 -
10.0.2.2 - - [25/Jun/2024 22:09:51] "GET /favicon.ico HTTP/1.1" 404 -
[2024-06-25 22:10:06,844] ERROR in app: Exception on /logs [GET]
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/vagrant_data/app.py", line 14, in get_logs
    logs_df = spark.sql("SELECT * FROM logs_table")
  File "/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py", line 1631, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
  File "/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `logs_table` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [logs_table], [], false

10.0.2.2 - - [25/Jun/2024 22:10:06] "GET /logs HTTP/1.1" 500 -
[2024-06-25 22:11:30,447] ERROR in app: Exception on /logs [GET]
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/vagrant_data/app.py", line 14, in get_logs
    logs_df = spark.sql("SELECT * FROM logs_table")
  File "/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py", line 1631, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
  File "/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `logs_table` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [logs_table], [], false

10.0.2.2 - - [25/Jun/2024 22:11:30] "GET /logs HTTP/1.1" 500 -
[2024-06-25 22:15:48,540] ERROR in app: Exception on /logs [GET]
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/vagrant_data/app.py", line 14, in get_logs
    logs_df = spark.sql("SELECT * FROM logs_table")
  File "/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py", line 1631, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
  File "/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `logs_table` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [logs_table], [], false

10.0.2.2 - - [25/Jun/2024 22:15:48] "GET /logs HTTP/1.1" 500 -
[2024-06-25 22:16:27,282] ERROR in app: Exception on /logs [GET]
Traceback (most recent call last):
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 1473, in wsgi_app
    response = self.full_dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 882, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 880, in full_dispatch_request
    rv = self.dispatch_request()
  File "/usr/local/lib/python3.10/dist-packages/flask/app.py", line 865, in dispatch_request
    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)  # type: ignore[no-any-return]
  File "/vagrant_data/app.py", line 14, in get_logs
    logs_df = spark.sql("SELECT * FROM logs_table")
  File "/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py", line 1631, in sql
    return DataFrame(self._jsparkSession.sql(sqlQuery, litArgs), self)
  File "/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `logs_table` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;
'Project [*]
+- 'UnresolvedRelation [logs_table], [], false

10.0.2.2 - - [25/Jun/2024 22:16:27] "GET /logs HTTP/1.1" 500 -
Starting Flask at Tue Jun 25 10:26:07 PM UTC 2024
24/06/25 22:26:11 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:26:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/25 22:26:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:26:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
 * Serving Flask app 'app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
127.0.0.1 - - [25/Jun/2024 22:26:56] "GET / HTTP/1.1" 404 -
127.0.0.1 - - [25/Jun/2024 22:27:09] "GET /logs HTTP/1.1" 404 -
10.0.2.2 - - [25/Jun/2024 22:27:19] "GET /logs HTTP/1.1" 404 -
10.0.2.2 - - [25/Jun/2024 22:30:25] "GET /logs HTTP/1.1" 404 -
10.0.2.2 - - [25/Jun/2024 22:39:43] "GET /logs HTTP/1.1" 404 -
Starting Flask at Tue Jun 25 10:40:05 PM UTC 2024
24/06/25 22:40:08 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:40:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/25 22:40:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:40:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
 * Serving Flask app 'app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
10.0.2.2 - - [25/Jun/2024 22:40:44] "GET /logs HTTP/1.1" 404 -
127.0.0.1 - - [25/Jun/2024 22:41:05] "GET /logs HTTP/1.1" 404 -
Starting Flask at Tue Jun 25 10:42:07 PM UTC 2024
24/06/25 22:42:10 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:42:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/25 22:42:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:42:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
 * Serving Flask app 'app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
10.0.2.2 - - [25/Jun/2024 22:43:32] "GET /logs HTTP/1.1" 404 -
10.0.2.2 - - [25/Jun/2024 22:43:36] "GET /tables HTTP/1.1" 200 -
Starting Flask at Tue Jun 25 10:45:56 PM UTC 2024
24/06/25 22:45:59 WARN Utils: Your hostname, vgr-spark-base64 resolves to a loopback address: 127.0.2.1; using 10.0.2.15 instead (on interface eth0)
24/06/25 22:45:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/06/25 22:45:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/06/25 22:46:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
 * Serving Flask app 'app'
 * Debug mode: off
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://10.0.2.15:5000
Press CTRL+C to quit
127.0.0.1 - - [25/Jun/2024 22:46:31] "GET /tables HTTP/1.1" 200 -
10.0.2.2 - - [25/Jun/2024 22:46:38] "GET /tables HTTP/1.1" 200 -
